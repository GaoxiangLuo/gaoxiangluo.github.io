<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Acknowledgement: This course (CSCI 8980) is being offered by Prof. Ju Sun at the University of Minnesota in Fall 2020. Pictures of slides are from the course.  Training DNNs: Basic Methods and Tricks">
<meta property="og:type" content="article">
<meta property="og:title" content="Basic Methods of Training Deep Neural Networks (DNNs)">
<meta property="og:url" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/index.html">
<meta property="og:site_name" content="Leon">
<meta property="og:description" content="Acknowledgement: This course (CSCI 8980) is being offered by Prof. Ju Sun at the University of Minnesota in Fall 2020. Pictures of slides are from the course.  Training DNNs: Basic Methods and Tricks">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure1.png">
<meta property="og:image" content="https://math.now.sh?from=min_W%5Csum_i%20%5Cmathbb%20%5Cell%28y_i%2C%20DNN_W(x_i%29)%20%2B%20%5COmega(W)%0A">
<meta property="og:image" content="https://math.now.sh?inline=%5Cell">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure2.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Ccdot">
<meta property="og:image" content="https://math.now.sh?from=%5Cnabla_w%5Cell%28sign(w%5ETx%29%2Cy)%3D%5Cell">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure3.png">
<meta property="og:image" content="https://math.now.sh?inline=tanh%28x%29">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure4.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure5.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure6.png">
<meta property="og:image" content="https://math.now.sh?inline=tanh">
<meta property="og:image" content="https://math.now.sh?inline=%7C%5Ccdot%7C">
<meta property="og:image" content="https://math.now.sh?inline=%5Csigma%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D">
<meta property="og:image" content="https://math.now.sh?from=L_k%20%5Cimplies%20%5B%5Cunderbrace%7B0%2C%5Cdots%2C0%7D_%7Bk-1%5Cquad%200">
<meta property="og:image" content="https://math.now.sh?from=z%20%5Cmapsto%20%5Clbrack%20%5Cfrac%7Be%5Ez1%7D%7B%5Csum_je%5Ezj%7D%2C%5Ccdots%2C%5Cfrac%7Be%5Ezp%7D%7B%5Csum_je%5Ezj%7D%20%5Crbrack%5ET%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure7.png">
<meta property="og:image" content="https://math.now.sh?inline=1">
<meta property="og:image" content="https://math.now.sh?inline=0">
<meta property="og:image" content="https://math.now.sh?inline=0">
<meta property="og:image" content="https://math.now.sh?inline=%5Cmathcal%7BE%7D">
<meta property="og:image" content="https://math.now.sh?inline=0">
<meta property="og:image" content="https://math.now.sh?from=min_W%5Csum_i%20%5Cmathbb%20%5Cell%28y_i%2C%20DNN_W(x_i%29)%20%2B%20%5COmega(W)%0A">
<meta property="og:image" content="https://math.now.sh?inline=m">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure8.png">
<meta property="og:image" content="https://math.now.sh?inline=m">
<meta property="og:image" content="https://math.now.sh?from=%5Ctext%7Bstochastic%20optimization%20%28stochastic%20%3D%20random%29%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%5Cnabla%5E2_W%5Cell%28y_i%2CDNN_W(x_i%29)%20%5Crightarrow%20%5Cmathbb%20E_%7Bx%2Cy%7D%5Cnabla_W%5Cell(y%2CDNN_W(x))">
<meta property="og:image" content="https://math.now.sh?from=%5Cfrac%7B1%7D%7B%7CJ%7C%7D%5Csum_%7Bj%20%5Cin%20J%7D%20%5Cnabla_W%5Cell%28y_i%EF%BC%8CDNN_w(x_j%29)%0A">
<meta property="og:image" content="https://math.now.sh?inline=J%20%5Csubset%20%5C%7B1%2C%5Cdots%2Cm%20%5C%7D">
<meta property="og:image" content="https://math.now.sh?inline=%7CJ%7C%20%5Cll%20m">
<meta property="og:image" content="https://math.now.sh?from=min_wF%28w%29%5Cdoteq%20%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Emf(m%3B%5Cxi_i)%20%5Cquad%20%5Cxi_i%20%5Ctext%7B">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure12.png">
<meta property="og:image" content="https://math.now.sh?inline=J_k">
<meta property="og:image" content="https://math.now.sh?inline=%7CJ_k%7C%3D1">
<meta property="og:image" content="https://math.now.sh?inline=J_k%20%5Csubset%20%5C%7B1%2C%5Cdots%2Cm%5C%7D">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure9.png">
<meta property="og:image" content="https://math.now.sh?inline=min_w%20%5C%7Cy-Xw%5C%7C_2%5E2">
<meta property="og:image" content="https://math.now.sh?inline=X%5Cin%20%5Cmathbb%20R%5E%7B10000%5Ctimes500%7D">
<meta property="og:image" content="https://math.now.sh?inline=y%5Cin%20R%5E%7B10000%7D">
<meta property="og:image" content="https://math.now.sh?inline=w%5Cin%20R%5E%7B500%7D">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure10.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Capprox">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure13.png">
<meta property="og:image" content="https://math.now.sh?from=%5Csum_%7Bk%7Dt_k%3D%5Cinfty%2C%20%5Cquad%20%5Csum_%7Bk%7Dt_k%5E2%3C%5Cinfty%0A">
<meta property="og:image" content="https://math.now.sh?inline=%5Cfrac%7Bstep%20size%7D%7BLR%20%5Ctext%7B%28learning%20rate%29%7D%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cfrac%7B1%7D%7Bt%7D">
<meta property="og:image" content="https://math.now.sh?inline=t_k%3D%5Cfrac%7B%5Calpha%7D%7B1%2B%5Cbeta%20k%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%2C%5Cbeta">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=t_k%20%3D%20%5Calpha%20e%5E%7B-%5Cbeta%20k%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha%2C%5Cbeta">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=t_0">
<meta property="og:image" content="https://math.now.sh?inline=L">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure14.png">
<meta property="og:image" content="https://math.now.sh?inline=g_i">
<meta property="og:image" content="https://math.now.sh?inline=i%5E%7Bth%7D">
<meta property="og:image" content="https://math.now.sh?inline=g_i">
<meta property="og:image" content="https://math.now.sh?inline=%28k%2B1%29%5E%7Bth%7D">
<meta property="og:image" content="https://math.now.sh?inline=i">
<meta property="og:image" content="https://math.now.sh?from=x_%7Bi%2Ck%2B1%7D%3Dx_%7Bi%2Ck%7D%20-%20t_k%20%5Cfrac%7Bg_%7Bi%2Ck%7D%7D%7B%5Csqrt%7B%5Csum_%7Bj%3D1%7D%5Ekg_%7Bi%2Cj%7D%5E2%7D%2B%5Cepsilon%7D%0A">
<meta property="og:image" content="https://math.now.sh?from=x_%7Bk%2B1%7D%3Dx_%7Bk%7D%20-%20t_k%20%5Cfrac%7Bg_%7Bk%7D%7D%7B%5Csqrt%7B%5Csum_%7Bj%3D1%7D%5Ekg_%7Bj%7D%5E2%7D%2B%5Cepsilon%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=s_k%5Cdoteq%5Csum_%7Bj%3D1%7D%5Ekg_j%5E2">
<meta property="og:image" content="https://math.now.sh?inline=s_k%3Ds_%7Bk-1%7D%2Bg_k%5E2">
<meta property="og:image" content="https://math.now.sh?inline=s_k">
<meta property="og:image" content="https://math.now.sh?inline=%5Cbeta%20%5Cin%20%280%2C1%29">
<meta property="og:image" content="https://math.now.sh?inline=s_k%3D%5Cbeta%20s_%7Bk-1%7D%2B%281-%5Cbeta%29g_k%5E2%20%5CLeftrightarrow%20s_k%3D(1-%5Cbeta)(g_k%5E2%2B%5Cbeta%20g_%7Bk-1%7D%5E2%2B%5Cbeta%5E2%20g_%7Bk-2%7D%5E2%20%2B%20%5Cdots)">
<meta property="og:image" content="https://math.now.sh?inline=%5Cbeta">
<meta property="og:image" content="https://math.now.sh?inline=m_k%3D%5Cbeta_1m_%7Bk-1%7D%2B%281-%5Cbeta_1%29g_k">
<meta property="og:image" content="https://math.now.sh?inline=s_k%3D%5Cbeta_2%20s_%7Bk-1%7D%2B%281-%5Cbeta_2%29g_k%5E2">
<meta property="og:image" content="https://math.now.sh?inline=x_%7Bk%2B1%7D%20%3D%20x_k-t_k%5Cfrac%7Bm_k%7D%7B%5Csqrt%7Bs_k%2B%5Cepsilon%7D%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cbeta_1%3D0.9%2C%20%5Cbeta_2%3D0.999%2C%20%5Cepsilon%3D1e-8">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure15.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure16.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure17.png">
<meta property="og:image" content="https://math.now.sh?inline=W%3D0">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla_%7BW_1%7DF%3D0">
<meta property="og:image" content="https://math.now.sh?inline=W_1">
<meta property="og:image" content="https://math.now.sh?inline=W">
<meta property="og:image" content="https://math.now.sh?inline=%5C%7C%5Cnabla%20f%28w%29%5C%7C%20%5Cleq%20%5Cepsilon">
<meta property="og:image" content="https://math.now.sh?inline=%5Cepsilon">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla%20f%28w%29">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure18.png">
<meta property="article:published_time" content="2020-10-27T15:05:02.000Z">
<meta property="article:modified_time" content="2020-11-15T23:41:56.334Z">
<meta property="article:author" content="Gaoxiang Luo">
<meta property="article:tag" content="Deep Neural Network">
<meta property="article:tag" content="Stochastic Gradient Descent">
<meta property="article:tag" content="Adagrad">
<meta property="article:tag" content="RMSprop">
<meta property="article:tag" content="Adam">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/figure1.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Basic Methods of Training Deep Neural Networks (DNNs)</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Leon" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/11/01/Association-Rules-Advanced-Concept/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/10/25/Numerical-Optimization-Computing-Derivatives/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&text=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&is_video=false&description=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Basic Methods of Training Deep Neural Networks (DNNs)&body=Check out this article: https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&name=Basic Methods of Training Deep Neural Networks (DNNs)&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&t=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Training DNNs: Basic Methods and Tricks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Three-Design-Choices"><span class="toc-number">1.1.</span> <span class="toc-text">Three Design Choices</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-activation-at-the-hidden-nodes"><span class="toc-number">1.1.1.</span> <span class="toc-text">Which activation at the hidden nodes?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-activation-at-the-output-nodes"><span class="toc-number">1.1.2.</span> <span class="toc-text">Which activation at the output nodes?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-loss"><span class="toc-number">1.1.3.</span> <span class="toc-text">Which loss?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-Algorithms"><span class="toc-number">1.2.</span> <span class="toc-text">Training Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-SGD"><span class="toc-number">1.2.1.</span> <span class="toc-text">Stochastic Gradient Descent (SGD)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What%E2%80%99s-an-epoch"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">What’s an epoch?</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GD-vs-SGD"><span class="toc-number">1.2.2.</span> <span class="toc-text">GD vs SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Step-Size-Learning-Rate-LR-for-SGD"><span class="toc-number">1.2.3.</span> <span class="toc-text">Step Size (Learning Rate (LR)) for SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-SGD-with-adaptive-learning-rate"><span class="toc-number">1.2.4.</span> <span class="toc-text">Why SGD with adaptive learning rate?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-1-Adagrad"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">Method 1: Adagrad</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-2-RMSprop"><span class="toc-number">1.2.4.2.</span> <span class="toc-text">Method 2: RMSprop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-3-Adam"><span class="toc-number">1.2.4.3.</span> <span class="toc-text">Method 3: Adam</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diagnosis-of-LR"><span class="toc-number">1.2.5.</span> <span class="toc-text">Diagnosis of LR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Where-to-initialize-for-DNNs"><span class="toc-number">1.2.6.</span> <span class="toc-text">Where to initialize for DNNs?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#When-to-stop"><span class="toc-number">1.2.7.</span> <span class="toc-text">When to stop?</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Basic Methods of Training Deep Neural Networks (DNNs)
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Leon</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-10-27T15:05:02.000Z" itemprop="datePublished">2020-10-27</time>
        
        (Updated: <time datetime="2020-11-15T23:41:56.334Z" itemprop="dateModified">2020-11-15</time>)
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Adagrad/" rel="tag">Adagrad</a>, <a class="tag-link-link" href="/tags/Adam/" rel="tag">Adam</a>, <a class="tag-link-link" href="/tags/Deep-Neural-Network/" rel="tag">Deep Neural Network</a>, <a class="tag-link-link" href="/tags/RMSprop/" rel="tag">RMSprop</a>, <a class="tag-link-link" href="/tags/Stochastic-Gradient-Descent/" rel="tag">Stochastic Gradient Descent</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>Acknowledgement: This course (CSCI 8980) is being offered by <a target="_blank" rel="noopener" href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p>
</blockquote>
<h1>Training DNNs: Basic Methods and Tricks</h1>
<h2 id="Three-Design-Choices">Three Design Choices</h2>
<p><img src="figure1.png" alt="figure1"></p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=min_W%5Csum_i%20%5Cmathbb%20%5Cell%28y_i%2C%20DNN_W(x_i%29)%20%2B%20%5COmega(W)%0A" /></p><ul>
<li>Which activation at the hidden nodes?</li>
<li>Which activation at the output node (Identity function is ok)?</li>
<li>Which <img src="https://math.now.sh?inline=%5Cell" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>?</li>
</ul>
<h3 id="Which-activation-at-the-hidden-nodes">Which activation at the hidden nodes?</h3>
<p><img src="figure2.png" alt="figure2"><br>
Is the sign(<img src="https://math.now.sh?inline=%5Ccdot" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) activation good for derivative-based optimization?<br>
No, for following two reasons:</p>
<ul>
<li>derivative of sign activation in anywhere is 0<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cnabla_w%5Cell%28sign(w%5ETx%29%2Cy)%3D%5Cell'(sign(w%5ETx)%2Cy)sign'(w%5ETx)x%3D0%0A" /></p></li>
<li>not differentiable at origin</li>
</ul>
<p>But why the classic Perceptron algorithm converges?<br>
Because perceptron algorithm is not a gradiaent descent algorithm.</p>
<p>What we want for activation:</p>
<ul>
<li>Differentiable or almost everywhere differentiabkle</li>
<li>Nonzero derivatives (almost everywhere)</li>
<li>Cheap to compute</li>
</ul>
<p>A positive example:<br>
<img src="figure3.png" alt="figure3"></p>
<p>What about <img src="https://math.now.sh?inline=tanh%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>?<br>
<img src="figure4.png" alt="figure4"></p>
<ul>
<li>It’s differentiable.</li>
<li>The gradient is similiar to sigmoid.</li>
</ul>
<p>How about ReLU and ReLU’s variation?<br>
<img src="figure5.png" alt="figure5"><br>
<img src="figure6.png" alt="figure6"></p>
<ul>
<li>ReLU and Leaky ReLU are the most popular</li>
<li><img src="https://math.now.sh?inline=tanh" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> less preferred but okay; hypertan and sigmoid should be avoided<br>
What do you think of <img src="https://math.now.sh?inline=%7C%5Ccdot%7C" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> as activation, i.e, absulute value function? Acceptable.</li>
</ul>
<h3 id="Which-activation-at-the-output-nodes">Which activation at the output nodes?</h3>
<p>Depending on the desired output:</p>
<ul>
<li>unbounded scalar/vector output (e.g., regression): <strong>identity activation</strong></li>
<li>binary classification with 0 or 1 output: e.g., sigmoid <img src="https://math.now.sh?inline=%5Csigma%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li>
<li>multiclass classification: labels into vectors via <strong>one-hot encoding</strong></li>
</ul>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=L_k%20%5Cimplies%20%5B%5Cunderbrace%7B0%2C%5Cdots%2C0%7D_%7Bk-1%5Cquad%200's%7D%2C1%2C%5Cunderbrace%7B0%2C%5Cdots%2C0%7D_%7Bn-k%5Cquad0's%7D%5D%5ET%0A" /></p><p>Softmax activation:</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=z%20%5Cmapsto%20%5Clbrack%20%5Cfrac%7Be%5Ez1%7D%7B%5Csum_je%5Ezj%7D%2C%5Ccdots%2C%5Cfrac%7Be%5Ezp%7D%7B%5Csum_je%5Ezj%7D%20%5Crbrack%5ET%0A" /></p><h3 id="Which-loss">Which loss?</h3>
<p><img src="figure7.png" alt="figure7"><br>
In multiclass classification label smoothing section and in one-hot encoding, we normally get the a vector with <img src="https://math.now.sh?inline=1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and all other entries <img src="https://math.now.sh?inline=0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, but if we want to further turn our parameters, <img src="https://math.now.sh?inline=0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is not likely to be helpful. So, we will use a small <img src="https://math.now.sh?inline=%5Cmathcal%7BE%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> instead of <img src="https://math.now.sh?inline=0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> so that information can be used later.</p>
<h2 id="Training-Algorithms">Training Algorithms</h2>
<p>Recall our optimization problem:</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=min_W%5Csum_i%20%5Cmathbb%20%5Cell%28y_i%2C%20DNN_W(x_i%29)%20%2B%20%5COmega(W)%0A" /></p><p>What happens when <img src="https://math.now.sh?inline=m" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is large, i.e., in the “big data” regime?<br>
<img src="figure8.png" alt="figure8"><br>
There is a intersting graph above. The GPU listed above is RTX 8000, which has 48G VRAM and around $5000 by the time of this blog. However, comparing to large dataset like MIMIC and ImageNet, it’s still really tiny little small.</p>
<p>How to get around the storage and computation bottleneck when <img src="https://math.now.sh?inline=m" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is large?</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Ctext%7Bstochastic%20optimization%20%28stochastic%20%3D%20random%29%7D%0A" /></p><p>Idea: use a small batch of data samples to approxiamate quantities of interest</p>
<ul>
<li>gradient: <img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%5Cnabla%5E2_W%5Cell%28y_i%2CDNN_W(x_i%29)%20%5Crightarrow%20%5Cmathbb%20E_%7Bx%2Cy%7D%5Cnabla_W%5Cell(y%2CDNN_W(x))" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br>
approximated by stochastic gradient:</li>
</ul>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7B1%7D%7B%7CJ%7C%7D%5Csum_%7Bj%20%5Cin%20J%7D%20%5Cnabla_W%5Cell%28y_i%EF%BC%8CDNN_w(x_j%29)%0A" /></p><p>for a random subset <img src="https://math.now.sh?inline=J%20%5Csubset%20%5C%7B1%2C%5Cdots%2Cm%20%5C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, where <img src="https://math.now.sh?inline=%7CJ%7C%20%5Cll%20m" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<h3 id="Stochastic-Gradient-Descent-SGD">Stochastic Gradient Descent (SGD)</h3>
<p>In general, suppose we want to solve</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=min_wF%28w%29%5Cdoteq%20%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Emf(m%3B%5Cxi_i)%20%5Cquad%20%5Cxi_i%20%5Ctext%7B's%20are%20data%20samples%7D%0A" /></p><p>Idea: replace gradient with a stochastic gradient in each step of GD<br>
<img src="figure12.png" alt="figure12"><br>
<img src="https://math.now.sh?inline=J_k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is a redrawn in each iteration. In traiditional SGD <img src="https://math.now.sh?inline=%7CJ_k%7C%3D1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and the version presented is also called <strong>mini-batch gradient descent</strong>.</p>
<h4 id="What’s-an-epoch">What’s an epoch?</h4>
<ul>
<li>Canonical SGD: sample a random subset <img src="https://math.now.sh?inline=J_k%20%5Csubset%20%5C%7B1%2C%5Cdots%2Cm%5C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> each iteration–sampling with replacement.</li>
<li>Practical SGD: shuffle the trainng set, and take a conscutive batch of size B (called batch size) each iteration–sampling without replacement one pass of the shuffled training set is called on epoch<br>
<img src="figure9.png" alt="figure9"></li>
</ul>
<h3 id="GD-vs-SGD">GD vs SGD</h3>
<p>Consider <img src="https://math.now.sh?inline=min_w%20%5C%7Cy-Xw%5C%7C_2%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, where <img src="https://math.now.sh?inline=X%5Cin%20%5Cmathbb%20R%5E%7B10000%5Ctimes500%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=y%5Cin%20R%5E%7B10000%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=w%5Cin%20R%5E%7B500%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br>
<img src="figure10.png" alt="figure10"><br>
Having observed that SGD converges to the same value as GD, but why it seems like left graph SGD is slower than GD? That’s just a illusion, we should look at the right graph, because the right graph is about each epoch. For SGD, going through the entire dateset is going through one epoch.</p>
<ul>
<li>By iteration: GD is faster</li>
<li>By iter(GD)/Epoch(SGD): SGD is faster</li>
<li>Remember, cost of one epoch of SGD <img src="https://math.now.sh?inline=%5Capprox" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> cost of one iteration of GD!</li>
</ul>
<p>Overall, SGD could be quicker to find a medium-accuracy solution with lower cost, which suffices for most purposes in machine. More on the reference below:<br>
<img src="figure13.png" alt="figure13"></p>
<h3 id="Step-Size-Learning-Rate-LR-for-SGD">Step Size (Learning Rate (LR)) for SGD</h3>
<p>Classical theory for SGD on convex problems requires</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Csum_%7Bk%7Dt_k%3D%5Cinfty%2C%20%5Cquad%20%5Csum_%7Bk%7Dt_k%5E2%3C%5Cinfty%0A" /></p><p>Practical implementation: diminishing <img src="https://math.now.sh?inline=%5Cfrac%7Bstep%20size%7D%7BLR%20%5Ctext%7B%28learning%20rate%29%7D%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, e.g.:</p>
<ul>
<li><img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7Bt%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> delay: <img src="https://math.now.sh?inline=t_k%3D%5Cfrac%7B%5Calpha%7D%7B1%2B%5Cbeta%20k%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=%5Calpha%2C%5Cbeta" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: tunable parameters, <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: iteration index</li>
<li>exponential delay: <img src="https://math.now.sh?inline=t_k%20%3D%20%5Calpha%20e%5E%7B-%5Cbeta%20k%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=%5Calpha%2C%5Cbeta" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: tunable parameters, <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: iteration index</li>
<li>staircase delay: start from <img src="https://math.now.sh?inline=t_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, divide it by a factor (e.g., 5 or 10) every <img src="https://math.now.sh?inline=L" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (say, 10) epochs–<strong>popular in practice</strong>. Some heuristic variants:
<ul>
<li>Watch the validation error and decrease the LR when it stagnates</li>
<li>Watch the objective and descrease of LR when it stagnates</li>
</ul>
</li>
</ul>
<p>There are around 10 ways of choosing the step size (learning rate) in Pytorch, and Pytorch call them scheduler. Check it out <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">torch.optim.lr_scheduler</a></p>
<h3 id="Why-SGD-with-adaptive-learning-rate">Why SGD with adaptive learning rate?</h3>
<p>One great complain of SGD is that if the conditioning of the function is not good, i.e., if you have a coutour plot that every time you take a step the direction is almost orthogonal with last step, SGD is sturggling a lot in moving.<br>
<img src="figure14.png" alt="figure14"><br>
Even if we can deal with it by Newton’s method, Quasi-Newton’s method and momentum methods, it’s still expensive. Here is a idea: if decompiosing your gradient direction to coordinate directions, e.g., x-direction and y-direction, if the magnitude of coordinate directions have very different values, it will have really bad effect on optimization. The solution is that if the gradient toward a certian coordinate direction is always large, I can divide the component of that direction by a large number, so that I can reduce the magnitude toward that direction. In other words, I want to amplify coordinate direction where gradient is always small, and I want to surpress cooprdinate direction where gradient is always large. How to do that?</p>
<h4 id="Method-1-Adagrad">Method 1: Adagrad</h4>
<p>Adagrad: divide <img src="https://math.now.sh?inline=g_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> by historic gradient magnitudes in the <img src="https://math.now.sh?inline=i%5E%7Bth%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> coordinate. If the historic gradient is small, then I divide <img src="https://math.now.sh?inline=g_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> by it I will get a larger value; vice versa.</p>
<p>At the <img src="https://math.now.sh?inline=%28k%2B1%29%5E%7Bth%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> iteration, for all <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>:</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=x_%7Bi%2Ck%2B1%7D%3Dx_%7Bi%2Ck%7D%20-%20t_k%20%5Cfrac%7Bg_%7Bi%2Ck%7D%7D%7B%5Csqrt%7B%5Csum_%7Bj%3D1%7D%5Ekg_%7Bi%2Cj%7D%5E2%7D%2B%5Cepsilon%7D%0A" /></p><p>or in elementwise notation</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=x_%7Bk%2B1%7D%3Dx_%7Bk%7D%20-%20t_k%20%5Cfrac%7Bg_%7Bk%7D%7D%7B%5Csqrt%7B%5Csum_%7Bj%3D1%7D%5Ekg_%7Bj%7D%5E2%7D%2B%5Cepsilon%7D%0A" /></p><p>Write <img src="https://math.now.sh?inline=s_k%5Cdoteq%5Csum_%7Bj%3D1%7D%5Ekg_j%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Note that <img src="https://math.now.sh?inline=s_k%3Ds_%7Bk-1%7D%2Bg_k%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. So only need to incrementally update the <img src="https://math.now.sh?inline=s_k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> sequence, which is cheap.</p>
<p>In PyTorch, check out <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad">torch.optim.Adagrad</a><br>
But Adagrad have a problem in the way of its accumulating, after a great number of iterations, the step will turn out to be super small. In fact, it stops doing the gradient descent, that’s also why we introduce Method 2 below.</p>
<h4 id="Method-2-RMSprop">Method 2: RMSprop</h4>
<p>The idea of RMSprop comparing to Adagrad is that we won’t accumulate all the historic gradients, but only using recent historic gradient. It will gradually phase out the histroy.<br>
For some <img src="https://math.now.sh?inline=%5Cbeta%20%5Cin%20%280%2C1%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=s_k%3D%5Cbeta%20s_%7Bk-1%7D%2B%281-%5Cbeta%29g_k%5E2%20%5CLeftrightarrow%20s_k%3D(1-%5Cbeta)(g_k%5E2%2B%5Cbeta%20g_%7Bk-1%7D%5E2%2B%5Cbeta%5E2%20g_%7Bk-2%7D%5E2%20%2B%20%5Cdots)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br>
Obvisouly, even though we’re summing up all historic gradient magnitude squares coordinatewise, but due to the value of <img src="https://math.now.sh?inline=%5Cbeta" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is between 0 and 1, the previsou history will become less and less important to the current gradient.</p>
<h4 id="Method-3-Adam">Method 3: Adam</h4>
<p>Two most popular SGD methods:</p>
<ul>
<li>SGD with momentum</li>
<li>Adam</li>
</ul>
<p>The idea of Adam is to combine RMSprop with momentum methods:</p>
<ul>
<li><img src="https://math.now.sh?inline=m_k%3D%5Cbeta_1m_%7Bk-1%7D%2B%281-%5Cbeta_1%29g_k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (combine mumentum and SGD)</li>
<li><img src="https://math.now.sh?inline=s_k%3D%5Cbeta_2%20s_%7Bk-1%7D%2B%281-%5Cbeta_2%29g_k%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (scaling factor update as in RMSprop)</li>
<li>Combination: <img src="https://math.now.sh?inline=x_%7Bk%2B1%7D%20%3D%20x_k-t_k%5Cfrac%7Bm_k%7D%7B%5Csqrt%7Bs_k%2B%5Cepsilon%7D%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li>
</ul>
<p>Good to know:<br>
Typical parameters: <img src="https://math.now.sh?inline=%5Cbeta_1%3D0.9%2C%20%5Cbeta_2%3D0.999%2C%20%5Cepsilon%3D1e-8" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br>
Check out <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">torch.optim.Adam</a></p>
<h3 id="Diagnosis-of-LR">Diagnosis of LR</h3>
<p><img src="figure15.png" alt="figure15"><br>
If you loss function blows up, it means you shall descress your learning rate. Low LR always leads to convergence, but takes forever. Permature flatten is a sign of large LR; permature sloping is a sign of early stopping–increase the number of epochs! Remember the starecase LR schedule.</p>
<p>Why adaptive methods relevant for DL?<br>
<img src="figure16.png" alt="figure16"><br>
The gradients have really different magniture across layers, and the trend is consistent. See more discussion adn explanation in <a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap5.html">http://neuralnetworksanddeeplearning.com/chap5.html</a></p>
<h3 id="Where-to-initialize-for-DNNs">Where to initialize for DNNs?</h3>
<p><img src="figure17.png" alt="figure17"></p>
<ul>
<li>What about <img src="https://math.now.sh?inline=W%3D0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>? <img src="https://math.now.sh?inline=%5Cnabla_%7BW_1%7DF%3D0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> – no movement on <img src="https://math.now.sh?inline=W_1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li>
<li>What about very large (small) <img src="https://math.now.sh?inline=W" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>? Large (small) value &amp; gradient – the problem becomes significant when ther are more layers.</li>
</ul>
<p>There are some principled ways of initialization</p>
<ol>
<li>torch.nn.init.xavier_uniform_</li>
<li>torch.nn.init.kaiming_uniform_</li>
<li>torch.nn.init.orthogonal_</li>
</ol>
<h3 id="When-to-stop">When to stop?</h3>
<p>Recall that a natrual stopping criterion for general GD is <img src="https://math.now.sh?inline=%5C%7C%5Cnabla%20f%28w%29%5C%7C%20%5Cleq%20%5Cepsilon" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> for a small <img src="https://math.now.sh?inline=%5Cepsilon" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Is this good when training DNNs?<br>
No, the gradient in DNNs is always SGD, so it doesn’t make sense to use norm of gradient or Hessian as a stopping value. Also, computing <img src="https://math.now.sh?inline=%5Cnabla%20f%28w%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> each iteration is expensive.</p>
<p>A practical\pragmatic stopping strategy for classification: early stopping<br>
<img src="figure18.png" alt="figure18"><br>
periodically check the validation error and stop when it doesn’t improve, because people believe that after this point it starts to be overfitting</p>
<!-- ### Why scaling matters?
Consider the loss function we've discussed and their Gradient/Hessian:
- Least-squares (LS): $\min_{\mb w} \; \frac{1}{m}\sum_{i=1}^m \norm{ y_i - \mb w^\T \mb x_i}{2}^2$
- Logistic regression: $\min_{\mb w}\; -\frac{1}{m}\sum_{i=1}^m \brac{y_i \mb w^\T \mb x_i - \log \paren{1+e^{\mb w^\T \mb x_i}}}$
- Shallow NN prediction: $\min_{\mb w} \; \frac{1}{m}\sum_{i=1}^m \norm{y_i - \sigma\paren{\mb w^\T \mb x_i}}_{2}^2$

Gradient: $\nabla_{\mb w} f = \frac{1}{m}\sum_{i=1}^m \ell'\paren{\mb w^\T \mb x_i; y_i}\mb x_i$
Hessian: $\nabla^2_{\mb w} f = \frac{1}{m}\sum_{i=1}^m \ell''\paren{\mb w^\T \mb x_i; y_i}\mb x_i \mb x_i^\T$

Consider the linear model $\mb W^T\mb x_{i} \leftrightarrow \mb y_{i}$ with coordinate $\mb x_{i}$. If you multiply $x_{i}$ by a scale value, it is equivalent to multipling $\mb W^T$ by the value one over the scale value. Hence, we can linearly scale $x_{i}$ without effects on model capacity. However, if we have very different values on coordinates, their weighted sum of derivative tend to have very large values, which is bad for optimization. Therefore, we should minimize this effect from data itself instead of relying on optimization algorithms. The Hessian also has this problem. Because Hessian depends on $\mathbb{E}({x_{i}x_{i}^T})$,  distribution of curvature along different coordinate direction will be very different with very large/small $x_{i}$. -->
<hr>
<div style="text-align: right"> To be continued... </div>
  </div>
</article>


  <!-- Gitalk start  -->

  <!-- Link Gitalk files  -->
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"/></script> 
  <div id="gitalk-container"></div>     
  <script type="text/javascript">
      var gitalk = new Gitalk({

        // gitalk's main params
        clientID: `3ce407f9c6ed6aaffdf8`,
        clientSecret: `2c6bc0465d806a6dc4ee20c9b1a0043d7a6101fe`,
        repo: `gaoxiangluo.github.io`,
        owner: 'GaoxiangLuo',
        admin: ['GaoxiangLuo'],
        id: location.pathname.slice(0, 50),
        distractionFreeMode: true,
        enableHotKey: true,
        language: `en`
      });
      gitalk.render('gitalk-container');
  </script> 
  <!-- Gitalk end -->




        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Training DNNs: Basic Methods and Tricks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Three-Design-Choices"><span class="toc-number">1.1.</span> <span class="toc-text">Three Design Choices</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-activation-at-the-hidden-nodes"><span class="toc-number">1.1.1.</span> <span class="toc-text">Which activation at the hidden nodes?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-activation-at-the-output-nodes"><span class="toc-number">1.1.2.</span> <span class="toc-text">Which activation at the output nodes?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Which-loss"><span class="toc-number">1.1.3.</span> <span class="toc-text">Which loss?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-Algorithms"><span class="toc-number">1.2.</span> <span class="toc-text">Training Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent-SGD"><span class="toc-number">1.2.1.</span> <span class="toc-text">Stochastic Gradient Descent (SGD)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#What%E2%80%99s-an-epoch"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">What’s an epoch?</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GD-vs-SGD"><span class="toc-number">1.2.2.</span> <span class="toc-text">GD vs SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Step-Size-Learning-Rate-LR-for-SGD"><span class="toc-number">1.2.3.</span> <span class="toc-text">Step Size (Learning Rate (LR)) for SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-SGD-with-adaptive-learning-rate"><span class="toc-number">1.2.4.</span> <span class="toc-text">Why SGD with adaptive learning rate?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-1-Adagrad"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">Method 1: Adagrad</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-2-RMSprop"><span class="toc-number">1.2.4.2.</span> <span class="toc-text">Method 2: RMSprop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Method-3-Adam"><span class="toc-number">1.2.4.3.</span> <span class="toc-text">Method 3: Adam</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diagnosis-of-LR"><span class="toc-number">1.2.5.</span> <span class="toc-text">Diagnosis of LR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Where-to-initialize-for-DNNs"><span class="toc-number">1.2.6.</span> <span class="toc-text">Where to initialize for DNNs?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#When-to-stop"><span class="toc-number">1.2.7.</span> <span class="toc-text">When to stop?</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&text=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&is_video=false&description=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Basic Methods of Training Deep Neural Networks (DNNs)&body=Check out this article: https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&title=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&name=Basic Methods of Training Deep Neural Networks (DNNs)&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2020/10/27/Basic-Methods-of-Training-Deep-Neural-Networks-DNNs/&t=Basic Methods of Training Deep Neural Networks (DNNs)"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2021
    Gaoxiang Luo
  </div>
  <!-- <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </nav>
  </div> -->
  <div class="busuanzi-count">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span class="site-pv">
      Page View <i class="fa fa-users"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
    <!-- <span class="site-uv">
      Number of Visitors <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuansi_value_site_uv"></span>
    </span> -->
  </div>
  <div class="Word-count">
    <i class="fas fa-chart-area"></i> Total count: </i><span class="post-count">25.7k</span>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-176745887-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":5,"vOffset":-38},"mobile":{"show":false,"scale":0.2},"react":{"opacityDefault":0.8,"opacityOnHover":0.2},"log":false});</script></body>
</html>
