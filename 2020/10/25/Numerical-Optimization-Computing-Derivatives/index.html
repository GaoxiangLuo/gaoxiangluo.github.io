<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Acknowledgement: This course (CSCI 8980) is being offered by Prof. Ju Sun at the University of Minnesota in Fall 2020. Pictures of slides are from the course.  Numerical Optimization: Computing Deriv">
<meta property="og:type" content="article">
<meta property="og:title" content="Numerical Optimization: Computing Derivatives">
<meta property="og:url" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/index.html">
<meta property="og:site_name" content="Leon">
<meta property="og:description" content="Acknowledgement: This course (CSCI 8980) is being offered by Prof. Ju Sun at the University of Minnesota in Fall 2020. Pictures of slides are from the course.  Numerical Optimization: Computing Deriv">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure1.png">
<meta property="og:image" content="https://math.now.sh?inline=f%3A%5Cmathbb%20R%5Em%20%5Crightarrow%20%5Cmathbb%20R%5En">
<meta property="og:image" content="https://math.now.sh?inline=h%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek">
<meta property="og:image" content="https://math.now.sh?inline=f">
<meta property="og:image" content="https://math.now.sh?inline=x">
<meta property="og:image" content="https://math.now.sh?inline=z%3Dh%28y%29">
<meta property="og:image" content="https://math.now.sh?inline=y%3Df%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=z%3Dh%20%5Ccirc%20f%28x%29%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek">
<meta property="og:image" content="https://math.now.sh?inline=x">
<meta property="og:image" content="https://math.now.sh?from=J_%7B%5Bh%20%5Ccirc%20f%5D%7D%28x%29%3DJ_h(f(x))J_f(x)%2C%20or%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=k%3D1">
<meta property="og:image" content="https://math.now.sh?from=%5Cnabla%20%5Bh%5Ccirc%20f%5D%28x%29%3DJ_f%5ET(x)%5Cnabla%20h(f(x))%0A">
<meta property="og:image" content="https://math.now.sh?from=f%28x%2B%5Cdelta%29%3Df(x)%2B%5Clangle%20%5Cnabla%20f(x)%2C%20%5Cdelta%20%5Crangle%20%2B%20o(%5C%7C%5Cdelta%5C%7C_2)%0A">
<meta property="og:image" content="https://math.now.sh?from=f%28x%2B%5Cdelta%29%3Df(x)%2B%5Clangle%20%5Cnabla%20f(x)%2C%20%5Cdelta%20%5Crangle%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Clangle%20%5Cdelta%2C%20%5Cnabla%5E2%20f(x)%20%5Cdelta%20%5Crangle%20%2B%20o(%5C%7C%5Cdelta%5C%7C_2%5E2)%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure2.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure3.png">
<meta property="og:image" content="https://math.now.sh?from=f%28W%29%3D%5Csum_i%5C%7Cy_i-%5Csigma(W_k%5Csigma(W_%7Bk-1%7D%5Cdots(W_1x_i)))%5C%7C_F%5E2%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure4.png">
<meta property="og:image" content="https://math.now.sh?inline=f%28x%29%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Em">
<meta property="og:image" content="https://math.now.sh?from=%5Cfrac%7B%5Cpartial%20f_j%7D%7B%5Cpartial%20x_i%7D%5Capprox%20%5Cfrac%7Bf_j%28x%2B%5Cdelta%20e_i%29-f_j(x)%7D%7B%5Cdelta%20%7D%20%5Cquad%20%5Ctext%7B(one%20element%20each%20time)%7D%0A">
<meta property="og:image" content="https://math.now.sh?from=%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x_i%7D%5Capprox%20%5Cfrac%7Bf%28x%2B%5Cdelta%20e_i%29-f_j(x)%7D%7B%5Cdelta%20%7D%20%5Cquad%20%5Ctext%7B(one%20column%20each%20time)%7D%0A">
<meta property="og:image" content="https://math.now.sh?from=J_f%28x%29p%5Capprox%20%5Cfrac%7Bf_j(x%2B%5Cdelta%20p)-f_j(x)%7D%7B%5Cdelta%20%7D%20%5Cquad%20%5Ctext%7B(directional)%7D%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure5.png">
<meta property="og:image" content="https://math.now.sh?inline=10%5E%7B-6%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cdelta">
<meta property="og:image" content="https://math.now.sh?inline=10%5E%7B-6%7D">
<meta property="og:image" content="https://math.now.sh?inline=%5Cdelta">
<meta property="og:image" content="https://math.now.sh?inline=10%5E%7B-3%7D">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure6.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla%5E2f%28x%29v">
<meta property="og:image" content="https://math.now.sh?inline=f_k%20%5Ccirc%20f_%7Bk-1%7D%20%5Ccirc%20%5Ccdots%20%5Ccirc%20f_2%20%5Ccirc%20f_1%28x%29%3A%20%5Cmathbb%20R%20%5Crightarrow%20%5Cmathbb%20R">
<meta property="og:image" content="https://math.now.sh?inline=y_0%3Dx%2C%20y_1%3Df_1%28x%29%2Cy_2%3Df(y_1)%2C%5Cdots%2Cy_k%3Df(y_%7Bk-1%7D)">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure7.png">
<meta property="og:image" content="https://math.now.sh?from=%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20y_k%7D%7B%5Cpartial%20y_0%7D%3D%5Cfrac%7B%5Cpartial%20y_k%7D%7B%5Cpartial%20y_%7Bk-1%7D%7D%5Cfrac%7B%5Cpartial%20y_%7Bk-1%7D%7D%7B%5Cpartial%20y_%7Bk-2%7D%7D%20%5Cdots%20%5Cfrac%7B%5Cpartial%20y_1%7D%7B%5Cpartial%20y_0%7D%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure8.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure9.png">
<meta property="og:image" content="https://math.now.sh?inline=y_i">
<meta property="og:image" content="https://math.now.sh?inline=%5Cfrac%7Bdy_i%7D%7Bdy_0%7D">
<meta property="og:image" content="https://math.now.sh?inline=y_i">
<meta property="og:image" content="https://math.now.sh?inline=%5Cfrac%7Bdy_k%7D%7Bdy_i%7D">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure10.png">
<meta property="og:image" content="https://math.now.sh?inline=f%3A%5Cmathbb%20R%5Em%20%5Crightarrow%20%5Cmathbb%20R%5En">
<meta property="og:image" content="https://math.now.sh?inline=h%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek">
<meta property="og:image" content="https://math.now.sh?inline=f">
<meta property="og:image" content="https://math.now.sh?inline=x">
<meta property="og:image" content="https://math.now.sh?inline=z%3Dh%28y%29">
<meta property="og:image" content="https://math.now.sh?inline=y%3Df%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=z%3Dh%20%5Ccirc%20f%28x%29%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek">
<meta property="og:image" content="https://math.now.sh?inline=x">
<meta property="og:image" content="https://math.now.sh?from=J_%7B%5Bh%20%5Ccirc%20f%5D%7D%28x%29%3DJ_h(f(x))J_f(x)%2C%20or%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%20%5CLeftrightarrow%20%5Cfrac%7B%5Cpartial%20z_j%7D%7B%5Cpartial%20x_i%7D%3D%5Csum_%7Bl%3D1%7D%5Em%5Cfrac%7B%5Cpartial%20z_j%7D%7B%5Cpartial%20y_l%7D%5Cfrac%7B%5Cpartial%20y_l%7D%7B%5Cpartial%20x_i%7D%20%5Cforall%20i%2Cj%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure11.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure12.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure13.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Cbar%20v_i">
<meta property="og:image" content="https://math.now.sh?inline=%5Cfrac%7B%5Ctext%7Bpartial%20of%20output%20%7D%7D%7B%5Ctext%7Bpartial%20of%20%7D%20v_i%7D">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure14.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure16.png">
<meta property="og:image" content="https://math.now.sh?inline=J_f%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=f%3A%20R%5En%20%5Crightarrow%20R%5Em">
<meta property="og:image" content="https://math.now.sh?inline=v%5ETJ_f%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=v%20%5Cin%20R%5Em">
<meta property="og:image" content="https://math.now.sh?inline=v">
<meta property="og:image" content="https://math.now.sh?inline=x">
<meta property="og:image" content="https://math.now.sh?inline=v">
<meta property="og:image" content="https://math.now.sh?inline=v_TJ_f%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=v%3De_i">
<meta property="og:image" content="https://math.now.sh?inline=i%3D1%2C%5Cdots%2Cm">
<meta property="og:image" content="https://math.now.sh?inline=J_f%28x%29">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla%20%28g%20%5Ccirc%20f%29%20%3D%20(%5Cnabla%20f%5ETJ_f)%5ET">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla%20f">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure17.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla%5E2f%28x%29v">
<meta property="article:published_time" content="2020-10-25T21:41:05.000Z">
<meta property="article:modified_time" content="2020-10-25T21:43:04.792Z">
<meta property="article:author" content="Gaoxiang Luo">
<meta property="article:tag" content="Numerical Optimization">
<meta property="article:tag" content="Backprop">
<meta property="article:tag" content="Auto Diff">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/figure1.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Numerical Optimization: Computing Derivatives</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Leon" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" href="/2020/10/17/Numerical-Optimization-Iterative-Methods/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&text=Numerical Optimization: Computing Derivatives"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&is_video=false&description=Numerical Optimization: Computing Derivatives"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Numerical Optimization: Computing Derivatives&body=Check out this article: https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&name=Numerical Optimization: Computing Derivatives&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&t=Numerical Optimization: Computing Derivatives"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Numerical Optimization: Computing Derivatives</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Analytic-Derivatives"><span class="toc-number">1.1.</span> <span class="toc-text">Analytic Derivatives</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Symbolic-Differentiation"><span class="toc-number">1.1.1.</span> <span class="toc-text">Symbolic Differentiation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limitation-of-analytic-diferentiation"><span class="toc-number">1.1.2.</span> <span class="toc-text">Limitation of analytic diferentiation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Approxiamte-the-gradient"><span class="toc-number">1.1.3.</span> <span class="toc-text">Approxiamte the gradient</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Auto-Differentiation"><span class="toc-number">1.2.</span> <span class="toc-text">Auto Differentiation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-Differentiation-in-1D"><span class="toc-number">1.2.1.</span> <span class="toc-text">Auto Differentiation in 1D</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-mode-in-1D"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Forward mode in 1D</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reverse-mode-in-1D"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Reverse mode in 1D</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-vs-Reverse"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">Forward vs Reverse</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-Differentiation-in-High-Dimension"><span class="toc-number">1.2.2.</span> <span class="toc-text">Auto Differentiation in High Dimension</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-multivariate-example-%E2%80%93-forward-mode"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">A multivariate example – forward mode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#A-multivatiate-example-%E2%80%93-reserve-mode"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">A multivatiate example – reserve mode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-vs-Reserve"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">Forward vs Reserve</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-Trick-%E2%80%93-Tensor-Abstration"><span class="toc-number">1.2.3.</span> <span class="toc-text">Implementation Trick – Tensor Abstration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-Trick-%E2%80%93-Vector-Jacobian-Product-VJP"><span class="toc-number">1.2.4.</span> <span class="toc-text">Implementation Trick – Vector Jacobian Product (VJP)</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Numerical Optimization: Computing Derivatives
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Leon</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-10-25T21:41:05.000Z" itemprop="datePublished">2020-10-25</time>
        
        (Updated: <time datetime="2020-10-25T21:43:04.792Z" itemprop="dateModified">2020-10-25</time>)
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Auto-Diff/" rel="tag">Auto Diff</a>, <a class="tag-link-link" href="/tags/Backprop/" rel="tag">Backprop</a>, <a class="tag-link-link" href="/tags/Numerical-Optimization/" rel="tag">Numerical Optimization</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>Acknowledgement: This course (CSCI 8980) is being offered by <a target="_blank" rel="noopener" href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p>
</blockquote>
<h1>Numerical Optimization: Computing Derivatives</h1>
<p>No matter which iterative methods you choose to use in order to do numerical optimization, you almost have to entail low-order derivatives (i.e., gradient and/or Hessian) to proceed. In other words, <strong>numerical derivatives</strong> (i.e., numbers) needed for the iterations.</p>
<p>In genreal, there are four kinds of computing techniques:<br>
<img src="figure1.png" alt="figure1"><br>
The main topic of this blog is about Auto Differentiation.</p>
<h2 id="Analytic-Derivatives">Analytic Derivatives</h2>
<p>The idea is to derive the analytic derivatives first, then make numerical substitution.</p>
<p>To derive the analytic derivatives by hand, we usually apply the chain rule (vector version) method.<br>
Let <img src="https://math.now.sh?inline=f%3A%5Cmathbb%20R%5Em%20%5Crightarrow%20%5Cmathbb%20R%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=h%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=z%3Dh%28y%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=y%3Df%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Then <img src="https://math.now.sh?inline=z%3Dh%20%5Ccirc%20f%28x%29%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=J_%7B%5Bh%20%5Ccirc%20f%5D%7D%28x%29%3DJ_h(f(x))J_f(x)%2C%20or%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%0A" /></p><p>when <img src="https://math.now.sh?inline=k%3D1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cnabla%20%5Bh%5Ccirc%20f%5D%28x%29%3DJ_f%5ET(x)%5Cnabla%20h(f(x))%0A" /></p><p>And another method we usually use is Taylor expansion method, it’s a bit uncommon but it’s convinient for certain functions and high-dimensional functions.</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=f%28x%2B%5Cdelta%29%3Df(x)%2B%5Clangle%20%5Cnabla%20f(x)%2C%20%5Cdelta%20%5Crangle%20%2B%20o(%5C%7C%5Cdelta%5C%7C_2)%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=f%28x%2B%5Cdelta%29%3Df(x)%2B%5Clangle%20%5Cnabla%20f(x)%2C%20%5Cdelta%20%5Crangle%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Clangle%20%5Cdelta%2C%20%5Cnabla%5E2%20f(x)%20%5Cdelta%20%5Crangle%20%2B%20o(%5C%7C%5Cdelta%5C%7C_2%5E2)%0A" /></p><h3 id="Symbolic-Differentiation">Symbolic Differentiation</h3>
<p>Idea: derive the analytic derivatives first, then make numerical substitution. Usually, software can derive the analytic derivatives for us, such as Matlab (Symbolic Math Toolbox, diff), Python (SymPy, diff), and Mathematica (D)<br>
<img src="figure2.png" alt="figure2"></p>
<h3 id="Limitation-of-analytic-diferentiation">Limitation of analytic diferentiation</h3>
<p><img src="figure3.png" alt="figure3"><br>
What is the gradient and/or Hessian of</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=f%28W%29%3D%5Csum_i%5C%7Cy_i-%5Csigma(W_k%5Csigma(W_%7Bk-1%7D%5Cdots(W_1x_i)))%5C%7C_F%5E2%0A" /></p><p>Applying the chain rule is boring and error-prone, and performing Taylor expansion is also tedious. The lession we learn from tech history is to leave boring jobs to computers.</p>
<h3 id="Approxiamte-the-gradient">Approxiamte the gradient</h3>
<p><img src="figure4.png" alt="figure4"><br>
Similiarly, to approximate the Jacobian for <img src="https://math.now.sh?inline=f%28x%29%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Em" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7B%5Cpartial%20f_j%7D%7B%5Cpartial%20x_i%7D%5Capprox%20%5Cfrac%7Bf_j%28x%2B%5Cdelta%20e_i%29-f_j(x)%7D%7B%5Cdelta%20%7D%20%5Cquad%20%5Ctext%7B(one%20element%20each%20time)%7D%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x_i%7D%5Capprox%20%5Cfrac%7Bf%28x%2B%5Cdelta%20e_i%29-f_j(x)%7D%7B%5Cdelta%20%7D%20%5Cquad%20%5Ctext%7B(one%20column%20each%20time)%7D%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=J_f%28x%29p%5Capprox%20%5Cfrac%7Bf_j(x%2B%5Cdelta%20p)-f_j(x)%7D%7B%5Cdelta%20%7D%20%5Cquad%20%5Ctext%7B(directional)%7D%0A" /></p><p>Why people prefer the <strong>central</strong> version?<br>
<img src="figure5.png" alt="figure5"><br>
If you want to get a error rate of <img src="https://math.now.sh?inline=10%5E%7B-6%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, in forward scheme you have to set <img src="https://math.now.sh?inline=%5Cdelta" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> to be <img src="https://math.now.sh?inline=10%5E%7B-6%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. But in central scheme, you only need to set <img src="https://math.now.sh?inline=%5Cdelta" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> to be <img src="https://math.now.sh?inline=10%5E%7B-3%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</p>
<p><img src="figure6.png" alt="figure6"><br>
Second-order methods are expensive storage-wise and computation-wise. In fact, what you need in all second-order methods are approximations of Hessian but not exact Hessian. Typically, what you need is <img src="https://math.now.sh?inline=%5Cnabla%5E2f%28x%29v" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and this is not Hessian itself but Hessian multiplying with a vector.</p>
<h2 id="Auto-Differentiation">Auto Differentiation</h2>
<h3 id="Auto-Differentiation-in-1D">Auto Differentiation in 1D</h3>
<p>Consider a univaiate function <img src="https://math.now.sh?inline=f_k%20%5Ccirc%20f_%7Bk-1%7D%20%5Ccirc%20%5Ccdots%20%5Ccirc%20f_2%20%5Ccirc%20f_1%28x%29%3A%20%5Cmathbb%20R%20%5Crightarrow%20%5Cmathbb%20R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Write <img src="https://math.now.sh?inline=y_0%3Dx%2C%20y_1%3Df_1%28x%29%2Cy_2%3Df(y_1)%2C%5Cdots%2Cy_k%3Df(y_%7Bk-1%7D)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, or in <strong>computational graph</strong> form:<br>
<img src="figure7.png" alt="figure7"><br>
Also, we represent chain rule in Leibniz form:</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20y_k%7D%7B%5Cpartial%20y_0%7D%3D%5Cfrac%7B%5Cpartial%20y_k%7D%7B%5Cpartial%20y_%7Bk-1%7D%7D%5Cfrac%7B%5Cpartial%20y_%7Bk-1%7D%7D%7B%5Cpartial%20y_%7Bk-2%7D%7D%20%5Cdots%20%5Cfrac%7B%5Cpartial%20y_1%7D%7B%5Cpartial%20y_0%7D%0A" /></p><p>How to evaluate the product?</p>
<ul>
<li>From left to right in the chain (follow the arrow): <strong>forward mode auto diff</strong></li>
<li>From right to left in the chain: <strong>backward/reverse mode auto diff</strong></li>
<li>Hybrid: mixed mode</li>
</ul>
<h4 id="Forward-mode-in-1D">Forward mode in 1D</h4>
<p><img src="figure8.png" alt="figure8"></p>
<h4 id="Reverse-mode-in-1D">Reverse mode in 1D</h4>
<p><img src="figure9.png" alt="figure9"><br>
The remarkable difference is firstly you need the forward part (traveling following the arrow) and calculate all the numerical values, and secondly move backward.</p>
<h4 id="Forward-vs-Reverse">Forward vs Reverse</h4>
<ul>
<li>Forward mode AD: one forward pass, compute <img src="https://math.now.sh?inline=y_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>’s and <img src="https://math.now.sh?inline=%5Cfrac%7Bdy_i%7D%7Bdy_0%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>'s together</li>
<li>Reverse mode AD: one forward pass to compute <img src="https://math.now.sh?inline=y_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>’s, one backward pass to compute <img src="https://math.now.sh?inline=%5Cfrac%7Bdy_k%7D%7Bdy_i%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>'s<br>
<img src="figure10.png" alt="figure10"></li>
</ul>
<h3 id="Auto-Differentiation-in-High-Dimension">Auto Differentiation in High Dimension</h3>
<p>Let <img src="https://math.now.sh?inline=f%3A%5Cmathbb%20R%5Em%20%5Crightarrow%20%5Cmathbb%20R%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=h%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=z%3Dh%28y%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=y%3Df%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Then <img src="https://math.now.sh?inline=z%3Dh%20%5Ccirc%20f%28x%29%3A%20%5Cmathbb%20R%5En%20%5Crightarrow%20%5Cmathbb%20R%5Ek" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=J_%7B%5Bh%20%5Ccirc%20f%5D%7D%28x%29%3DJ_h(f(x))J_f(x)%2C%20or%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20%3D%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20y%7D%5Cfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20x%7D%20%5CLeftrightarrow%20%5Cfrac%7B%5Cpartial%20z_j%7D%7B%5Cpartial%20x_i%7D%3D%5Csum_%7Bl%3D1%7D%5Em%5Cfrac%7B%5Cpartial%20z_j%7D%7B%5Cpartial%20y_l%7D%5Cfrac%7B%5Cpartial%20y_l%7D%7B%5Cpartial%20x_i%7D%20%5Cforall%20i%2Cj%0A" /></p><p><img src="figure11.png" alt="figure11"></p>
<h4 id="A-multivariate-example-–-forward-mode">A multivariate example – forward mode</h4>
<p><img src="figure12.png" alt="figure12"><br>
Forward mode auto differentiation depends on the input dimension.</p>
<h4 id="A-multivatiate-example-–-reserve-mode">A multivatiate example – reserve mode</h4>
<p><img src="figure13.png" alt="figure13"><br>
Note: <img src="https://math.now.sh?inline=%5Cbar%20v_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> means <img src="https://math.now.sh?inline=%5Cfrac%7B%5Ctext%7Bpartial%20of%20output%20%7D%7D%7B%5Ctext%7Bpartial%20of%20%7D%20v_i%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. There is a name for this variable in auto differentiation community, which is caleld adjoint variable.<br>
Reverse mode auto differentiation depends on the output dimension, which is exactly why nowadays netral networks prefer reverse mode because the ouput dimension is usually one dimension. This is also what people call <strong>backprop</strong> (back propagation) in deep learning.</p>
<h4 id="Forward-vs-Reserve">Forward vs Reserve</h4>
<p>Both of them cannot deal with loops in computational graph, i.e., acyclic graph.<br>
<img src="figure14.png" alt="figure14"><br>
The forward mode starts with roots, while reverse mode starts with leaves.</p>
<h3 id="Implementation-Trick-–-Tensor-Abstration">Implementation Trick – Tensor Abstration</h3>
<p>On previous images, those nodes in the computational graph are scalar, which is fine when we only have 1 or 2 variables. However, if the dimension grows to be very large but we still use scalar representation, it’s going to be really messy. But today we have a wonderful idea of tensor, and lots of useful packages already implemented. Now we’re able to represent each node in computational graph as a vector, matrix, 3-D tensor, …<br>
<img src="figure16.png" alt="figure16"><br>
Now the difference is that we have to apply chain rule in tensors rather than scalars.</p>
<h3 id="Implementation-Trick-–-Vector-Jacobian-Product-VJP">Implementation Trick – Vector Jacobian Product (VJP)</h3>
<p>Interested in <img src="https://math.now.sh?inline=J_f%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> for <img src="https://math.now.sh?inline=f%3A%20R%5En%20%5Crightarrow%20R%5Em" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Implement <img src="https://math.now.sh?inline=v%5ETJ_f%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> for any <img src="https://math.now.sh?inline=v%20%5Cin%20R%5Em" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<p>Why VJP is interesting? Why is it a stardard practice in lots of auto differentiation packages?<br>
The idea is that if you just give me the <img src="https://math.now.sh?inline=v" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> then I can retain the value. Even if you want the Jacobian at this point, if you just set the <img src="https://math.now.sh?inline=v" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> in <img src="https://math.now.sh?inline=v_TJ_f%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> to be <img src="https://math.now.sh?inline=v%3De_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (elementary vector/matrix) for <img src="https://math.now.sh?inline=i%3D1%2C%5Cdots%2Cm" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> then you recover rows of <img src="https://math.now.sh?inline=J_f%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Another benefit is that it’s often enough for application, e.g., calculate <img src="https://math.now.sh?inline=%5Cnabla%20%28g%20%5Ccirc%20f%29%20%3D%20(%5Cnabla%20f%5ETJ_f)%5ET" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> with known <img src="https://math.now.sh?inline=%5Cnabla%20f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<p>Why possible?<br>
<img src="figure17.png" alt="figure17"></p>
<p>Good to know:</p>
<ul>
<li>In practice, graph are built automatically by software</li>
<li>Higher-order derivatives can also be done, particularly Hessian-vector product <img src="https://math.now.sh?inline=%5Cnabla%5E2f%28x%29v" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (check out <a target="_blank" rel="noopener" href="https://github.com/google/jax">Jax</a>!)</li>
<li>Auto-diff in Tensorflow and <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html">Pytorch</a> are spacialized to DNNs, whereas Jax (in Python) is full fledged and more general</li>
<li>General resources for <a target="_blank" rel="noopener" href="http://www.autodiff.org">autodiff</a></li>
</ul>

  </div>
</article>


  <!-- Gitalk start  -->

  <!-- Link Gitalk files  -->
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"/></script> 
  <div id="gitalk-container"></div>     
  <script type="text/javascript">
      var gitalk = new Gitalk({

        // gitalk's main params
        clientID: `3ce407f9c6ed6aaffdf8`,
        clientSecret: `2c6bc0465d806a6dc4ee20c9b1a0043d7a6101fe`,
        repo: `gaoxiangluo.github.io`,
        owner: 'GaoxiangLuo',
        admin: ['GaoxiangLuo'],
        id: location.pathname.slice(0, 50),
        distractionFreeMode: true,
        enableHotKey: true,
        language: `en`
      });
      gitalk.render('gitalk-container');
  </script> 
  <!-- Gitalk end -->




        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Numerical Optimization: Computing Derivatives</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Analytic-Derivatives"><span class="toc-number">1.1.</span> <span class="toc-text">Analytic Derivatives</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Symbolic-Differentiation"><span class="toc-number">1.1.1.</span> <span class="toc-text">Symbolic Differentiation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limitation-of-analytic-diferentiation"><span class="toc-number">1.1.2.</span> <span class="toc-text">Limitation of analytic diferentiation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Approxiamte-the-gradient"><span class="toc-number">1.1.3.</span> <span class="toc-text">Approxiamte the gradient</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Auto-Differentiation"><span class="toc-number">1.2.</span> <span class="toc-text">Auto Differentiation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-Differentiation-in-1D"><span class="toc-number">1.2.1.</span> <span class="toc-text">Auto Differentiation in 1D</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-mode-in-1D"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Forward mode in 1D</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reverse-mode-in-1D"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Reverse mode in 1D</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-vs-Reverse"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">Forward vs Reverse</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Auto-Differentiation-in-High-Dimension"><span class="toc-number">1.2.2.</span> <span class="toc-text">Auto Differentiation in High Dimension</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-multivariate-example-%E2%80%93-forward-mode"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">A multivariate example – forward mode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#A-multivatiate-example-%E2%80%93-reserve-mode"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">A multivatiate example – reserve mode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-vs-Reserve"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">Forward vs Reserve</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-Trick-%E2%80%93-Tensor-Abstration"><span class="toc-number">1.2.3.</span> <span class="toc-text">Implementation Trick – Tensor Abstration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-Trick-%E2%80%93-Vector-Jacobian-Product-VJP"><span class="toc-number">1.2.4.</span> <span class="toc-text">Implementation Trick – Vector Jacobian Product (VJP)</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&text=Numerical Optimization: Computing Derivatives"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&is_video=false&description=Numerical Optimization: Computing Derivatives"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Numerical Optimization: Computing Derivatives&body=Check out this article: https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&title=Numerical Optimization: Computing Derivatives"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&name=Numerical Optimization: Computing Derivatives&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2020/10/25/Numerical-Optimization-Computing-Derivatives/&t=Numerical Optimization: Computing Derivatives"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020
    Gaoxiang Luo
  </div>
  <!-- <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </nav>
  </div> -->
  <div class="busuanzi-count">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span class="site-pv">
      Page View <i class="fa fa-users"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
    <!-- <span class="site-uv">
      Number of Visitors <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuansi_value_site_uv"></span>
    </span> -->
  </div>
  <div class="Word-count">
    <i class="fas fa-chart-area"></i> Total count: </i><span class="post-count">12.9k</span>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-176745887-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":5,"vOffset":-38},"mobile":{"show":false,"scale":0.2},"react":{"opacityDefault":0.8,"opacityOnHover":0.2},"log":false});</script></body>
</html>
