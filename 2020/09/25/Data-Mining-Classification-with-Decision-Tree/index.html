<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Acknowledgement: This course (CSCI 5523) is being offered by Prof. Vipin Kumar at the University of Minnesota in Fall 2020.  Base Classifiers  Decision Tree based Methods Rule-based Methods Nearest-n">
<meta property="og:type" content="article">
<meta property="og:title" content="Data Mining: Classification with Decision Tree">
<meta property="og:url" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/index.html">
<meta property="og:site_name" content="Leon">
<meta property="og:description" content="Acknowledgement: This course (CSCI 5523) is being offered by Prof. Vipin Kumar at the University of Minnesota in Fall 2020.  Base Classifiers  Decision Tree based Methods Rule-based Methods Nearest-n">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure1a.png">
<meta property="og:image" content="https://math.now.sh?from=Gini%20%5C%2C%20Index%3D1-%20%5Csum_%7Bi%3D0%7D%5E%7Bc-1%7Dp_i%28t%29%5E2%0A">
<meta property="og:image" content="https://math.now.sh?inline=p%5Ei%28t%29">
<meta property="og:image" content="https://math.now.sh?inline=i">
<meta property="og:image" content="https://math.now.sh?inline=t">
<meta property="og:image" content="https://math.now.sh?inline=c">
<meta property="og:image" content="https://math.now.sh?from=Entropy%3D%20-%20%5Csum_%7Bi%3D0%7D%5E%7Bc-1%7Dp_i%28t%29log_2p_i(t)%0A">
<meta property="og:image" content="https://math.now.sh?from=Classification%20%5C%2C%20Error%20%3D%201%20-%20max%5Bp_i%28t%29%5D%0A">
<meta property="og:image" content="https://math.now.sh?inline=P">
<meta property="og:image" content="https://math.now.sh?inline=M">
<meta property="og:image" content="https://math.now.sh?from=Gain%20%3D%20P%20-%20M%0A">
<meta property="og:image" content="https://math.now.sh?from=Information%20%5C%2C%20Gain%20%3D%20Entropy%28parent%29%20-%20%5Csum_%7Bi%3D1%7Dk%20%5Cfrac%7Bn_i%7D%7Bn%7DEntropy(i)%0A">
<meta property="og:image" content="https://math.now.sh?inline=p">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=n_i">
<meta property="og:image" content="https://math.now.sh?inline=i">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure2a.png">
<meta property="og:image" content="https://math.now.sh?from=Gain%20%5C%2C%20Ratio%20%3D%20%5Cfrac%7BGain_split%7D%7BSplit%20%5C%2C%20Info%7D%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%20Split%20%5C%2C%20Info%20%3D%20-%5Csum_%7Bi%3D1%7D%5Ek%20%5Cfrac%7Bn_i%7D%7Bn%7Dlog_2%5Cfrac%7Bn_i%7D%7Bn%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=p">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=n_i">
<meta property="og:image" content="https://math.now.sh?inline=i">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure3a.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure1.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure2.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure3.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure4.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=T">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?from=err_%7Bgen%7D%28T%29%20%3D%20err(T)%20%2B%20%5COmega%20%5Ctimes%20%5Cfrac%7Bk%7D%7BN_%7Btrain%7D%7D%0A">
<meta property="og:image" content="https://math.now.sh?inline=err%28T%29">
<meta property="og:image" content="https://math.now.sh?inline=%5COmega">
<meta property="og:image" content="https://math.now.sh?inline=%5Calpha">
<meta property="og:image" content="https://math.now.sh?inline=k">
<meta property="og:image" content="https://math.now.sh?inline=N_%7Btrain%7D">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure5.png">
<meta property="og:image" content="https://math.now.sh?inline=e%28T_L%29">
<meta property="og:image" content="https://math.now.sh?inline=x%5E2">
<meta property="og:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure6.png">
<meta property="article:published_time" content="2020-09-25T16:16:41.000Z">
<meta property="article:modified_time" content="2020-10-05T20:38:00.365Z">
<meta property="article:author" content="Gaoxiang Luo">
<meta property="article:tag" content="Data Mining">
<meta property="article:tag" content="Classifier">
<meta property="article:tag" content="Decision Tree">
<meta property="article:tag" content="Gini Index">
<meta property="article:tag" content="Entropy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/figure1a.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Data Mining: Classification with Decision Tree</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Leon" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/09/27/Visual-and-Rigorous-Proof-of-Universal-Approximation-Theorem-UAT/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/09/20/Deep-Neural-Networks-DNNs-Overview/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&text=Data Mining: Classification with Decision Tree"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&is_video=false&description=Data Mining: Classification with Decision Tree"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Data Mining: Classification with Decision Tree&body=Check out this article: https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&name=Data Mining: Classification with Decision Tree&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&t=Data Mining: Classification with Decision Tree"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Decision Tree</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Decision-Tree-Induction-Algorithms"><span class="toc-number">1.1.</span> <span class="toc-text">Decision Tree Induction Algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hunt%E2%80%99s-Algorithm"><span class="toc-number">1.2.</span> <span class="toc-text">Hunt’s Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Measures-of-Node-Impurity"><span class="toc-number">1.3.</span> <span class="toc-text">Measures of Node Impurity</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-find-the-best-split"><span class="toc-number">1.3.1.</span> <span class="toc-text">How to find the best split?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gain-Ratio-for-Entropy"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Gain Ratio (for Entropy)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Pros-and-Cons-of-Decision-Tree-Based-Classification"><span class="toc-number">1.4.</span> <span class="toc-text">The Pros and Cons of Decision Tree Based Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Classification-Errors"><span class="toc-number">1.5.</span> <span class="toc-text">Classification Errors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notes-on-overfitting"><span class="toc-number">1.5.1.</span> <span class="toc-text">Notes on overfitting</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Selection"><span class="toc-number">1.6.</span> <span class="toc-text">Model Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-validation-set"><span class="toc-number">1.6.1.</span> <span class="toc-text">Using validation set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incorporating-model-complexity"><span class="toc-number">1.6.2.</span> <span class="toc-text">Incorporating model complexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Estimating-the-Complexity-of-Decision-Trees"><span class="toc-number">1.6.3.</span> <span class="toc-text">Estimating the Complexity of Decision Trees</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-selection-for-decision-trees"><span class="toc-number">1.7.</span> <span class="toc-text">Model selection for decision trees</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-Pruning-Early-Stopping-Rule"><span class="toc-number">1.7.1.</span> <span class="toc-text">Pre-Pruning (Early Stopping Rule)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Post-pruning"><span class="toc-number">1.7.2.</span> <span class="toc-text">Post-pruning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Evaluation"><span class="toc-number">1.8.</span> <span class="toc-text">Model Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross-validation-Example"><span class="toc-number">1.8.1.</span> <span class="toc-text">Cross-validation Example</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Data Mining: Classification with Decision Tree
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Leon</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-09-25T16:16:41.000Z" itemprop="datePublished">2020-09-25</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Classifier/" rel="tag">Classifier</a>, <a class="tag-link-link" href="/tags/Data-Mining/" rel="tag">Data Mining</a>, <a class="tag-link-link" href="/tags/Decision-Tree/" rel="tag">Decision Tree</a>, <a class="tag-link-link" href="/tags/Entropy/" rel="tag">Entropy</a>, <a class="tag-link-link" href="/tags/Gini-Index/" rel="tag">Gini Index</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>Acknowledgement: This course (CSCI 5523) is being offered by <a target="_blank" rel="noopener" href="https://www-users.cs.umn.edu/~kumar001/">Prof. Vipin Kumar</a> at the University of Minnesota in Fall 2020.</p>
</blockquote>
<p>Base Classifiers</p>
<ul>
<li>Decision Tree based Methods</li>
<li>Rule-based Methods</li>
<li>Nearest-neighbor</li>
<li>Neural Networks, Deep Neural Nets</li>
<li>Naive Bayes and Bayesian Belief Networks</li>
<li>Support Vector Machines</li>
</ul>
<p>Ensemble Classifiers</p>
<ul>
<li>Boosting, Bagging, Random Forests</li>
</ul>
<h1>Decision Tree</h1>
<h2 id="Decision-Tree-Induction-Algorithms">Decision Tree Induction Algorithms</h2>
<ul>
<li>Hunt’s Algorithm (one of the earliest)</li>
<li>CART</li>
<li>ID3, C4.5</li>
<li>SLIQ,SPRINT</li>
</ul>
<h2 id="Hunt’s-Algorithm">Hunt’s Algorithm</h2>
<p><img src="figure1a.png" alt="figure1a"><br>
As you may see in the picture, a Hunt’s Algorithm in decision tree is to define a condition to split data into two or more branches when a node is not pure (involving both labeled classes), until the leaf node has all the data with the same class. Now with this model, we might ask how do we determine the best spiltting. Therefore, some evaluation metrics may come into place.</p>
<h2 id="Measures-of-Node-Impurity">Measures of Node Impurity</h2>
<p>First, we have to know what is node impurity. If a node has 10 data points in total, and 5 of them are labeled class 1 and 5 of them are labeled class 0, then it has a high degree of impurity. But if a node has 9 data points labeled class 1 and only 1 data point labeled class 0, then we say it has a low degree of impurity. People have designed methods to measure node impurity. There are three most popular ones:</p>
<ul>
<li>Gini Index</li>
</ul>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gini%20%5C%2C%20Index%3D1-%20%5Csum_%7Bi%3D0%7D%5E%7Bc-1%7Dp_i%28t%29%5E2%0A" /></p><p>where <img src="https://math.now.sh?inline=p%5Ei%28t%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the frequency of class <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> at node <img src="https://math.now.sh?inline=t" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and <img src="https://math.now.sh?inline=c" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the total number of classes</p>
<pre><code>- It has a minimum of 0 when all records belong to one class, which is the most beneficial situation for classification.
- It has a maximum of $1 - \frac&#123;1&#125;&#123;c&#125;$ when records are equally distributed among all classes, which is the least beneficial situation.
- It's used in decision tree algorithems such as CART, SLIQ, SPRINT 
</code></pre>
<ul>
<li>Entropy</li>
</ul>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Entropy%3D%20-%20%5Csum_%7Bi%3D0%7D%5E%7Bc-1%7Dp_i%28t%29log_2p_i(t)%0A" /></p><pre><code>- It has a minimum of 0 and a maximum of $log_2c$.
- Entropy based computations are quite similiar to the GINI index computations.
</code></pre>
<ul>
<li>Misclassification Error</li>
</ul>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Classification%20%5C%2C%20Error%20%3D%201%20-%20max%5Bp_i%28t%29%5D%0A" /></p><h3 id="How-to-find-the-best-split">How to find the best split?</h3>
<ol>
<li>Compute Impurity Measure (<img src="https://math.now.sh?inline=P" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) before splitting</li>
<li>Compute Impurity Measure (<img src="https://math.now.sh?inline=M" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) after splitting</li>
<li>Choose the attribute test condition that produces the highest gain</li>
</ol>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gain%20%3D%20P%20-%20M%0A" /></p><p>Particularlly, when we say information gain, it implies the Gain of Entropy method.</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Information%20%5C%2C%20Gain%20%3D%20Entropy%28parent%29%20-%20%5Csum_%7Bi%3D1%7Dk%20%5Cfrac%7Bn_i%7D%7Bn%7DEntropy(i)%0A" /></p><p>where Parent Node <img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is split into <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> partitions (children); <img src="https://math.now.sh?inline=n_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is number of records in child node <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</p>
<p>Let’s look at a simple example.<br>
<img src="figure2a.png" alt="figure2a"></p>
<p>I think this image is straighforward. At this point we may naturally think of the more partitions we have, the more likely we’ll end up with a lower degree of impurity, but does it mean the more partitions the better? No, it’s not always meaningful to do so. For instance, if we classify each customers by ID, it doesn’t do anything or give us any useful information, so we want to have a measure that will penalize too many successors.</p>
<h4 id="Gain-Ratio-for-Entropy">Gain Ratio (for Entropy)</h4>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gain%20%5C%2C%20Ratio%20%3D%20%5Cfrac%7BGain_split%7D%7BSplit%20%5C%2C%20Info%7D%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%20Split%20%5C%2C%20Info%20%3D%20-%5Csum_%7Bi%3D1%7D%5Ek%20%5Cfrac%7Bn_i%7D%7Bn%7Dlog_2%5Cfrac%7Bn_i%7D%7Bn%7D%0A" /></p><p>Where parent node <img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is split into <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> partitions, and <img src="https://math.now.sh?inline=n_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the number of records in child node <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<ul>
<li>Split Info happends to be larget if too many successors, this is a way to prefer small number of successors.</li>
<li>This is degisned to overcome the disadvantage of information gain</li>
<li>Used in C4.5 algorithm</li>
</ul>
<p><img src="figure3a.png" alt="figure3a"><br>
One of the senarios of applying split info may be the image above. We see that the left most split and the right most split has a similiar value of Gini Index, but the right most one has a much lower split info so that we would go with the lower successors split.</p>
<h2 id="The-Pros-and-Cons-of-Decision-Tree-Based-Classification">The Pros and Cons of Decision Tree Based Classification</h2>
<p>Advantages:</p>
<ul>
<li>Relativelty inexpensive to construct</li>
<li>Extremely fast at classifying unkown records</li>
<li>Easy to interpret for small-sized trees</li>
<li>Robust to noise (especially when methods to avoid overfitting are employed)</li>
<li>Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>Due to the greedy nature of splitting criterion, interacting attributes (that can distinguish between classes together but not individually) may be passed over in favor of other attribtues that are less discriminating</li>
<li>Each decision boundary involves only a single attribute</li>
</ul>
<h2 id="Classification-Errors">Classification Errors</h2>
<ul>
<li>Training errors</li>
<li>Test errors</li>
<li>Generalization errors: expected error of a model over random selection of records from same distribution</li>
</ul>
<p><img src="figure1.png" alt="figure1"></p>
<p>Let’s look at this example above. The positive class is from a gaussian centered at (10,10) with 400 noisy instances added.</p>
<p><img src="figure2.png" alt="figure2"></p>
<p>If we only use 10% of dataset to train a decision tree, it will end up with a tree with 4 nodes as the picture above. While we increase the size of training dataset, the training error continues to goes down slowly, and we might have a decision tree with more nodes. But doesn’t it really improve our model’s performance?</p>
<p><img src="figure3.png" alt="figure3"></p>
<p>Actually not! As the model becomes more and more complex with larger training set, the test error starts to go up. This is what we called <strong>overfitting</strong>.</p>
<ul>
<li>Underfitting: when model is too simple, both training and test errors are large.</li>
<li>Overfitting: when model is too complex, training error is small but test error is large.</li>
</ul>
<p><img src="figure4.png" alt="figure4"></p>
<p>What if we increase the size of the model while we have a overfitting model? The anwser is that both the training errors and test error would go down as the image above.</p>
<p>At this point, let’s summurize the possible reasons for model overfitting.</p>
<ul>
<li>Limited training size</li>
<li>High model complexity: have something to do with multiple comparision procedure</li>
</ul>
<h3 id="Notes-on-overfitting">Notes on overfitting</h3>
<ul>
<li>Overfitting results in decision trees that are more complex than necessary</li>
<li>Training errors does not provide a good estimate of how well the tree will perform on previously unseen records</li>
<li>Need ways for estimating generalization errors</li>
</ul>
<h2 id="Model-Selection">Model Selection</h2>
<p>Purpose: to ensure that model is not overly complex (to avoid overfitting)<br>
Goal: to estimate generalization error</p>
<ul>
<li>Using validation set</li>
<li>Incorporating model complexity</li>
</ul>
<h3 id="Using-validation-set">Using validation set</h3>
<ul>
<li>Diving training data into two parts
<ul>
<li>Training set: use for model building</li>
<li>Validation set: use for estimating generalization error</li>
</ul>
</li>
<li>Drawback:
<ul>
<li>Less data available for training</li>
</ul>
</li>
</ul>
<h3 id="Incorporating-model-complexity">Incorporating model complexity</h3>
<p>Purpose: use some model complexities while building a model with training set.<br>
This is to 1) reduce training set error and 2) add penalty to increasing complexity simultaneously.</p>
<p>Gen.Error(Model) = Train.Error(Model,Train.Data) + <img src="https://math.now.sh?inline=%5Calpha" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> x Complexity(Model)</p>
<h3 id="Estimating-the-Complexity-of-Decision-Trees">Estimating the Complexity of Decision Trees</h3>
<p><strong>Pessimistic Error Estimate</strong> of decision tree <img src="https://math.now.sh?inline=T" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> with <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> leaf nodes.</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=err_%7Bgen%7D%28T%29%20%3D%20err(T)%20%2B%20%5COmega%20%5Ctimes%20%5Cfrac%7Bk%7D%7BN_%7Btrain%7D%7D%0A" /></p><ul>
<li><img src="https://math.now.sh?inline=err%28T%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: error rate on all training records (number of error divived by the total training examples)</li>
<li><img src="https://math.now.sh?inline=%5COmega" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: trade-off hyper-parameter (similar to <img src="https://math.now.sh?inline=%5Calpha" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>)
<ul>
<li>relative cost of adding a leaf node</li>
</ul>
</li>
<li><img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of leaf nodes</li>
<li><img src="https://math.now.sh?inline=N_%7Btrain%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: total number of training records</li>
</ul>
<p>Let’s look at an example:<br>
<img src="figure5.png" alt="figure5"><br>
How to calculate <img src="https://math.now.sh?inline=e%28T_L%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>?<br>
For example, if a leaf node has 3 positive and 1 negative, then the negative case is an error case. Applying the same through to all the leaf nodes will give you the total number of error cases. The other parts of this formula is straightforward in the image above.</p>
<h2 id="Model-selection-for-decision-trees">Model selection for decision trees</h2>
<h3 id="Pre-Pruning-Early-Stopping-Rule">Pre-Pruning (Early Stopping Rule)</h3>
<ul>
<li>Stop the algorithm before it becomes a fully-grown tree</li>
<li>Typical stopping conditions for a node:
<ul>
<li>stop if all instances belong to the same class</li>
<li>stop if all the attribute values are the same</li>
</ul>
</li>
<li>More restrictive conditions:
<ul>
<li>stop if number of instances is less than some user-specified threshold</li>
<li>stop if class distribution of instances are independent of the available features (e.g., using <img src="https://math.now.sh?inline=x%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> test)</li>
<li>stop if expanding the current node does not improve impurity measures (e.g., Gini or information gain)</li>
<li>stop if estimated generalization error falls below certain threshold</li>
</ul>
</li>
</ul>
<h3 id="Post-pruning">Post-pruning</h3>
<ul>
<li>Grow decision tree to its entirety</li>
<li>Subtree replacement
<ul>
<li>Trim the nodes of the decision tree in a bottom-up fashion</li>
<li>If generalization error improves after trimming, replace sub-tree by a leaf node</li>
<li>Class label of leaf node is determined from majority class of instances in the sub-tree.</li>
</ul>
</li>
</ul>
<h2 id="Model-Evaluation">Model Evaluation</h2>
<p>Purpose:</p>
<ul>
<li>To estimate performance of classifier on previously unseen data (test set)</li>
</ul>
<p>Holdout</p>
<ul>
<li>Reserve k% for training and (100-k)% for testing</li>
<li>Random subsampling: repeated holdout</li>
</ul>
<p>Cross validation</p>
<ul>
<li>Partition data into k disjoint subsets</li>
<li>k-fold: train on k-1 partitions, test on the remaining one</li>
<li>Leave-one-out: k = n</li>
</ul>
<p>If I only have a small number of dataset, and use too much for test set, then I don’t have enough data for training. I don’t want my model with biases toward how I train my model. What should I do?</p>
<h3 id="Cross-validation-Example">Cross-validation Example</h3>
<p><img src="figure6.png" alt="figure6"></p>
<ul>
<li>Repeated cross-validation
<ul>
<li>perform cross-validation a number of times</li>
<li>gives an estimate of the variance of the generalization error</li>
</ul>
</li>
<li>Stratified cross-validation
<ul>
<li>Guarantee the same percentage of class labels in training and test</li>
<li>Important when classes are imbalanced and the sample is small</li>
</ul>
</li>
<li>Use nested cross-validation approach for model selection and evaluation</li>
</ul>

  </div>
</article>


  <!-- Gitalk start  -->

  <!-- Link Gitalk files  -->
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"/></script> 
  <div id="gitalk-container"></div>     
  <script type="text/javascript">
      var gitalk = new Gitalk({

        // gitalk's main params
        clientID: `3ce407f9c6ed6aaffdf8`,
        clientSecret: `2c6bc0465d806a6dc4ee20c9b1a0043d7a6101fe`,
        repo: `gaoxiangluo.github.io`,
        owner: 'GaoxiangLuo',
        admin: ['GaoxiangLuo'],
        id: location.pathname.slice(0, 50),
        distractionFreeMode: true,
        enableHotKey: true,
        language: `en`
      });
      gitalk.render('gitalk-container');
  </script> 
  <!-- Gitalk end -->




        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Decision Tree</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Decision-Tree-Induction-Algorithms"><span class="toc-number">1.1.</span> <span class="toc-text">Decision Tree Induction Algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hunt%E2%80%99s-Algorithm"><span class="toc-number">1.2.</span> <span class="toc-text">Hunt’s Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Measures-of-Node-Impurity"><span class="toc-number">1.3.</span> <span class="toc-text">Measures of Node Impurity</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-find-the-best-split"><span class="toc-number">1.3.1.</span> <span class="toc-text">How to find the best split?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gain-Ratio-for-Entropy"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Gain Ratio (for Entropy)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Pros-and-Cons-of-Decision-Tree-Based-Classification"><span class="toc-number">1.4.</span> <span class="toc-text">The Pros and Cons of Decision Tree Based Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Classification-Errors"><span class="toc-number">1.5.</span> <span class="toc-text">Classification Errors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Notes-on-overfitting"><span class="toc-number">1.5.1.</span> <span class="toc-text">Notes on overfitting</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Selection"><span class="toc-number">1.6.</span> <span class="toc-text">Model Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-validation-set"><span class="toc-number">1.6.1.</span> <span class="toc-text">Using validation set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incorporating-model-complexity"><span class="toc-number">1.6.2.</span> <span class="toc-text">Incorporating model complexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Estimating-the-Complexity-of-Decision-Trees"><span class="toc-number">1.6.3.</span> <span class="toc-text">Estimating the Complexity of Decision Trees</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-selection-for-decision-trees"><span class="toc-number">1.7.</span> <span class="toc-text">Model selection for decision trees</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-Pruning-Early-Stopping-Rule"><span class="toc-number">1.7.1.</span> <span class="toc-text">Pre-Pruning (Early Stopping Rule)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Post-pruning"><span class="toc-number">1.7.2.</span> <span class="toc-text">Post-pruning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Evaluation"><span class="toc-number">1.8.</span> <span class="toc-text">Model Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross-validation-Example"><span class="toc-number">1.8.1.</span> <span class="toc-text">Cross-validation Example</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&text=Data Mining: Classification with Decision Tree"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&is_video=false&description=Data Mining: Classification with Decision Tree"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Data Mining: Classification with Decision Tree&body=Check out this article: https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&title=Data Mining: Classification with Decision Tree"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&name=Data Mining: Classification with Decision Tree&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/&t=Data Mining: Classification with Decision Tree"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2021
    Gaoxiang Luo
  </div>
  <!-- <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </nav>
  </div> -->
  <div class="busuanzi-count">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span class="site-pv">
      Page View <i class="fa fa-users"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
    <!-- <span class="site-uv">
      Number of Visitors <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuansi_value_site_uv"></span>
    </span> -->
  </div>
  <div class="Word-count">
    <i class="fas fa-chart-area"></i> Total count: </i><span class="post-count">27.8k</span>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-176745887-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":5,"vOffset":-38},"mobile":{"show":false,"scale":0.2},"react":{"opacityDefault":0.8,"opacityOnHover":0.2},"log":false});</script></body>
</html>
