<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leon</title>
  
  
  <link href="https://gaoxiangluo.github.io/atom.xml" rel="self"/>
  
  <link href="https://gaoxiangluo.github.io/"/>
  <updated>2020-09-28T03:53:11.697Z</updated>
  <id>https://gaoxiangluo.github.io/</id>
  
  <author>
    <name>Gaoxiang Luo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Think DL: Universal Approximation Theorem (UAT)</title>
    <link href="https://gaoxiangluo.github.io/2020/09/27/Think-DL-Universal-Approximation-Theorem-UAT/"/>
    <id>https://gaoxiangluo.github.io/2020/09/27/Think-DL-Universal-Approximation-Theorem-UAT/</id>
    <published>2020-09-28T02:16:59.000Z</published>
    <updated>2020-09-28T03:53:11.697Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 8980) is being offered by <a href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p></blockquote><h1>Why should we trust Neural Networks (NNs)?</h1><p>We will start by looking at the supervised learning. Although today’s NNs are not only for the supervised learning, we will use this embedded illustration from machine learning to give you some ideas.</p><p><img src="general_view_and_nn_view.png" alt="general_view_and_nn_view"><br>As shown above, the only difference between these two views is that we choose a NN architecture instead of selecting a family of functions. The idea behind both views is function approximation, and we want to emphasize more in function approximation to give you a more accurate description of supervised learning.</p><p><img src="figure1.png" alt="figure1"><br>Basically you can think of supervised learning in this way. First of all, we have an underlying true function <img src="https://math.now.sh?inline=f_0" style="filter: opacity(75%);transform:scale(0.75);text-align:center;display:inline-block;margin: 0;"/>, and our data can be generated by <img src="https://math.now.sh?inline=f_0" style="filter: opacity(75%);transform:scale(0.75);text-align:center;display:inline-block;margin: 0;"/> with dense sampling, which are the \textcolor{MidnightBlue}{blue dots} in Figure 1. Second, we choose a family of functions <img src="https://math.now.sh?inline=H" style="filter: opacity(75%);transform:scale(0.75);text-align:center;display:inline-block;margin: 0;"/>. The purpose here is to learn from this family <img src="https://math.now.sh?inline=H" style="filter: opacity(75%);transform:scale(0.75);text-align:center;display:inline-block;margin: 0;"/> to find a function <img src="https://math.now.sh?inline=f%20%5Cin%20H" style="filter: opacity(75%);transform:scale(0.75);text-align:center;display:inline-block;margin: 0;"/> \textcolor{orange}{(orange curve)} that is close to the ground truth function <img src="https://math.now.sh?inline=f_0" style="filter: opacity(75%);transform:scale(0.75);text-align:center;display:inline-block;margin: 0;"/>. This is different with the views mentioned earlier. Those views are to fit the data, which is more about the training error. But here if you really find the ground truth function by learning, you will do perfectly well on eliminating test error.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 8980) is being offered by &lt;a href=&quot;https://sunju.org/&quot;&gt;Prof. Ju Sun&lt;/a&gt; at the University</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="https://gaoxiangluo.github.io/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://gaoxiangluo.github.io/tags/Neural-Network/"/>
    
    <category term="Theory" scheme="https://gaoxiangluo.github.io/tags/Theory/"/>
    
  </entry>
  
  <entry>
    <title>Think DL: old and new neural networks</title>
    <link href="https://gaoxiangluo.github.io/2020/09/20/Think-DL-old-and-new-neural-networks/"/>
    <id>https://gaoxiangluo.github.io/2020/09/20/Think-DL-old-and-new-neural-networks/</id>
    <published>2020-09-21T03:29:00.000Z</published>
    <updated>2020-09-28T02:37:07.362Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 8980) is being offered by <a href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p></blockquote><h1>Netruel Networks: Old and New</h1><h2 id="Start-from-Neurons">Start from Neurons</h2><h3 id="Model-of-biological-neurons">Model of biological neurons</h3><p><img src="slide4.png" alt="slide4"></p><h2 id="Shallow-to-DNNs">Shallow to DNNs</h2><h3 id="Artificial-Neurons">Artificial Neurons</h3><p><img src="slide7.png" alt="slide7"><br>ReLU is the most popular activation function nowadays.</p><p><img src="slide8.png" alt="slide8"><br>People play diverse forms of NN but only play with graph without cycles, because we cannot do auto differentiation once we have cycles.</p><h3 id="Supervised-Learning-in-ML">Supervised Learning in ML</h3><p><img src="slide9.png" alt="slide9"></p><h3 id="Supervised-Learning-in-DL">Supervised Learning in DL</h3><p><img src="slide10.png" alt="slide10"><br>Instead of finding a function from a family of functions, we try to find a group of weights (w) and offsets(b) to minimize the average loss.</p><h3 id="Perceptron">Perceptron</h3><p>Def: Perceptron is a single artificial neuron for binary classification.<br><img src="slide12.png" alt="slide12"><br><img src="slide13.png" alt="slide13"><br><img src="slide14.png" alt="slide14"><br><img src="slide16.png" alt="slide16"></p><h3 id="They-are-all-shallow-NNs">They are all (shallow) NNs</h3><ul><li>Linear Regression</li><li>Perceptron and Logistic Regression</li><li>Softmax Regression</li><li>Multilayer Perceptron (feedforward NNs)</li><li>Support Vector Machine (SVM)</li><li>PCA (autoencoder)</li><li>Matrix Factorization</li></ul><h2 id="Some-Background-Knowledge">Some Background Knowledge</h2><h3 id="Turing-Test">Turing Test</h3><p>How we measure whether we achieve human-level AI?<br><img src="slide20.png" alt="slide20"></p><h3 id="Key-ingredients-of-DL">Key ingredients of DL</h3><p><img src="slide26.png" alt="slide26"><br>Suprisingly, we’re still using those computational methods that were invented 25-30 years ago. Nowadays the progress of DL largely depends on improvement of hardware (GPUs), but seldom on new computational methods.</p><p>Question to myself: Are we approaching the third AI winter? Should we focus more on foundamental research instead of picking new applications?</p><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 8980) is being offered by &lt;a href=&quot;https://sunju.org/&quot;&gt;Prof. Ju Sun&lt;/a&gt; at the University</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="https://gaoxiangluo.github.io/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://gaoxiangluo.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Think DL: overview</title>
    <link href="https://gaoxiangluo.github.io/2020/09/20/Think-DL-overview/"/>
    <id>https://gaoxiangluo.github.io/2020/09/20/Think-DL-overview/</id>
    <published>2020-09-21T03:23:39.000Z</published>
    <updated>2020-09-28T02:37:04.746Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 8980) is being offered by <a href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p></blockquote><h1>Why Deep Learning</h1><h2 id="What-is-Deep-Learning-DL">What is Deep Learning (DL)?</h2><h3 id="DL-is-about…">DL is about…</h3><ul><li>Deep Neural Network (DNNs)</li><li><strong>Data</strong> or training DNNs (e.g. images, videos, text sequences)</li><li><strong>Methods</strong> for training DNNs (e.g. AdaGrad, ADAM, RMSProp, Dropout)</li><li><strong>Hardware</strong> platforms for training DNNs (e.g. GPUs, TPUs, FPGAs)</li><li><strong>Software</strong> platforms for training DNNs (e.g., Tensorflow, Pytorch, MXNet)</li><li><strong>Applications!</strong> (e.g. vision, speech, NLP, imaging, physics, mathematics finance)</li></ul><h3 id="DL-leads-to-many-things-…">DL leads to many things …</h3><blockquote><p>Oxford Dictionary </br><br>Revolution: a great change in conditions, ways of working, beliefs, etc. that affects large numbers of people.</p></blockquote><h3 id="Academic-breakthroughs">Academic breakthroughs</h3><ul><li>increasing successful rate in image classification (2012)</li><li>increasing successful rate in speech recognition (2012)</li><li>Go game (2017)</li><li>image generation</li></ul><p><img src="slide5.png" alt="slide5"></p><h2 id="Commercial-breakthroughs">Commercial breakthroughs</h2><ul><li>self-driving vehicles</li><li>smart-home devices</li><li>healthcare</li><li>robotics</li></ul><p>Paper are produced at an <strong>overwhelming</strong> rate. </br><br>Note: <a href="arvis.org">arvis</a> (where people post preprint papers)</p><h1>Why first principles?</h1><ul><li>Tuning and optimizing for a task require basic intuitions</li><li>Historial lesson: model structures in data</li><li>Current challenge: move toward trustworthiness</li><li>Future world: navigate uncertainties</li></ul><h2 id="Structures-are-crucial">Structures are crucial</h2><p><img src="slide14.png" alt="slide14"></p><h2 id="Toward-trustworthy-AI">Toward trustworthy AI</h2><ul><li>we need to know first principles in order to improve and understand</li><li>Trustworthiness:<ul><li>robustness</li><li>fairness (<em>what if data is primarily from certain population group</em>)</li><li>explainability (<em>what if we make human-level robot but we don’t understand how to make decision</em>)</li><li>transparency</li></ul></li></ul><h2 id="Future-uncertainties">Future uncertainties</h2><ul><li>New types of data (e.g. 6-D tensors)</li><li>New hardware (e.g. better GPU memory)</li><li>New model pipelines (e.g. network of networks, differential programming)</li><li>New applications</li><li>New techniques replacing DL</li></ul><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 8980) is being offered by &lt;a href=&quot;https://sunju.org/&quot;&gt;Prof. Ju Sun&lt;/a&gt; at the University</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="https://gaoxiangluo.github.io/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://gaoxiangluo.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>For UMN: How to setup remote enviroment with CSE lab machine via VSCode</title>
    <link href="https://gaoxiangluo.github.io/2020/09/12/For-UMN-How-to-setup-remote-enviroment-with-CSE-lab-machine-via-VSCode/"/>
    <id>https://gaoxiangluo.github.io/2020/09/12/For-UMN-How-to-setup-remote-enviroment-with-CSE-lab-machine-via-VSCode/</id>
    <published>2020-09-12T14:10:03.000Z</published>
    <updated>2020-09-12T14:59:07.317Z</updated>
    
    <content type="html"><![CDATA[<h1>VSCode Setup with CSE Lab Machines on Your Own Computer</h1><h2 id="Demo">Demo</h2><p><img src="Demo.png" alt="Demo"><br>(P.S. The OCaml code above is supposed to be wrong)</p><h3 id="Why-should-you-consider-this-setup">Why should you consider this setup?</h3><ul><li>feel like working on CSE Lab Machines literally</li><li>don’t have to install <code>OCaml</code> on your computer because you’re using lab <code>OCaml</code>;</li><li>graphical user interface with <code>OCaml</code> Syntax Highlighting</li><li>faster than <code>vole</code> since it talks to server directly rather than transmitting through web browser</li><li>can be used on <code>Linux</code>, <code>MacOS</code> and <code>Windows</code> because they all have <code>VSCode</code></li><li>file transfer: drag you local file into CSE lab machine</li></ul><h2 id="Tutorial">Tutorial</h2><p>This tutorial can help you to set up your remote enviroment graphically with CSE lab machines on your own computer.</p><hr><ol><li><p>Open <code>VSCode</code>, go to extension on the left nevigation bar and search <code>ssh</code><br><img src="step1.png" alt="step1"></p></li><li><p>Install <code>REMOTE - SSH</code>, click <code>SSH extension</code> icon on the left nevigation bar and click <code>connect</code><br><img src="step2.png" alt="step2"></p></li><li><p>On the pop-up command window, type in <code>ssh &lt;x500&gt;@&lt;cse lab machines&gt; -A</code>.<br>For instance, I will use:</p><ul><li><code>ssh luo00042@atlas.cselabs.umn.edu -A</code> ATLAS is a server machine which has 256G RAM! But sometimes if a student is running heavy-duty task on ATLAS, you may want to use another machine.</li><li><code>ssh luo00042@csel-kh4250-03.cselabs.umn.edu -A</code></li></ul></li></ol><p><img src="step3.png" alt="step3"></p><p><strong>Note:</strong> A list of CSE lab UNIX machines that you can remotely work on, you can find them <a href="https://cseit.umn.edu/computer-classrooms/cse-labs-unix-machine-listings">here</a>.</p><p><strong>Remember:</strong> Replace the <code>x500</code> with yours, not mine; and make sure to include the <code>-A</code> flag, otherwise you have to use VPN or be under university wifi to connect.</p><ol start="4"><li><p>Click to update ssh configuration so you don’t have to type the long ssh command every time.<br><img src="step4.png" alt="step4"></p></li><li><p>When it’s done, there will be a SSH target on the left. Click <code>connect</code><br><img src="step5.png" alt="step5"></p></li><li><p>A new window will pop up and there will be prompt that you need to enter your <code>x500</code> password.<br><img src="step6.png" alt="step6"></p></li><li><p>You are in CSE lab machine via SSH connection! Click <code>Open Folder</code>. Doesn’t it look familiar to you? That’s your CSE lab folder! Navigate to your own repo and click <code>OK</code><br><img src="step7.png" alt="step7"></p></li><li><p>That’s it! You can open a terminal within <code>VSCode</code>, and it will be under SSH connection as well!<br><img src="step8.png" alt="step8"></p></li></ol><p>Now you can edit your code <strong>on lab machine</strong>, save your code <strong>on lab machine</strong>, and test your code with terminal <strong>on lab machine</strong>!</p><p><strong>Editor Note:</strong> While it’s easier to edit your code with a nice-looking IDE, it’s still good to pick up tools like <code>emacs</code> and <code>vim</code>. I hope you find this tutorial helpful to you!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;VSCode Setup with CSE Lab Machines on Your Own Computer&lt;/h1&gt;
&lt;h2 id=&quot;Demo&quot;&gt;Demo&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;Demo.png&quot; alt=&quot;Demo&quot;&gt;&lt;br&gt;
(P.S. The OC</summary>
      
    
    
    
    
    <category term="How-to" scheme="https://gaoxiangluo.github.io/tags/How-to/"/>
    
  </entry>
  
  <entry>
    <title>Robotics: Perception study note week 1</title>
    <link href="https://gaoxiangluo.github.io/2020/09/06/Robotics-Perception-study-note-week-1/"/>
    <id>https://gaoxiangluo.github.io/2020/09/06/Robotics-Perception-study-note-week-1/</id>
    <published>2020-09-07T03:47:32.000Z</published>
    <updated>2020-09-28T03:34:12.512Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course is being offered on <a href="https://www.coursera.org/learn/robotics-perception/">Coursera</a> for free to audit.</p></blockquote><blockquote><p>Gaoxiang: Due to the characteristic of this course, there maybe lots of pictures.</p></blockquote><h1>Overview</h1><ul><li>Week 1: Geometry of Projection</li><li>Week 2: Augmented Reality &amp; Visual Metrology</li><li>Week 3 &amp; 4: Where am I?</li></ul><hr><h1>Week 1</h1><p><img src="camera_types.png" alt="camera_type"><br>There are panoromic cameras, stereos camera, laser scanner and Kinect.</p><h1>How does a thin lens work?</h1><p><img src="thin_lens.png" alt="thin_lens"></p><ul><li>Rays parallet to the optical axis meet the focus after leaving the lens.</li><li>Rays through center of the lens do not change direction.</li></ul><p><img src="thin_lens_equation.png" alt="equation"></p><h1>What happens when we move the image plane?</h1><p>moving image plane = focusing (practically)<br><img src="move_image_plane.png" alt="plane_move"><br>The read line segment is what makes image blur.</p><h1>Perspective projection: size of object image</h1><p><img src="perspective_projection.png" alt="perspective_projection"></p><ul><li>This is easily proved by similarity of triangle.</li><li>A point object of the same size coming closer results on a larger image.</li><li>A point moving on the same ray does not change its image.</li></ul><hr><h1>Single View Geometry</h1><h2 id="Two-facts">Two facts:</h2><ul><li>When we take a pictuer, the 3D location has become a 2D plane. We have lost the third dimension in this process.</li><li>It matters how are oritented to the world when we take a picture.</li></ul><h2 id="Ideas-I-Measurements-on-planes">Ideas I: Measurements on planes</h2><p>We can unwarp then measutre, which means to make parallel lines stay parallel and perpendicular angle stay perpendicular.<br><img src="measure_planes.png" alt="measure_planes"></p><h2 id="Ideas-II-Vanishing-points">Ideas II: Vanishing points</h2><p><img src="vanishing_point.png" alt="vanishing_point"><br>The roof and ground are parellel in real world (blue lines), and they converge to a point if we draw them. That’s the vanishing point.</p><p><img src="imaging_vanishing_point.png" alt="imaging_vanishing_point"><br>As the blue dot moves further away, the projection point on the image plane will be closer and closer to the vanishing point.</p><hr><h1>More on Perspective Projection</h1><h2 id="Bi-perspectograph">Bi-perspectograph</h2><p><img src="prove_bi_perspectograph.png" alt="prove_bi_perspectograph"><br>Take some time to understand this proof. As the point P* on image plane moves, we’re able to draw the projection curve on the ground plane with point P’.</p><h2 id="Two-parameters">Two parameters:</h2><ul><li>Height of camera (line O*S)<ul><li>If we project a circle from image plane, the lower the camera is, the more squeezed is the ellipse.</li></ul></li><li>Distance of projection center from image plane (distance between two parallel lines LM and OS); also known as focal length<ul><li>When it moves, it <strong>only change the size not the shape</strong>.</li></ul></li></ul><hr><h1>Glimpse on Vanishing Points</h1><p><img src="properties_vanishing_point.png" alt="properties_vanishing_point"><br>The three properties of vanishing points are on the image above.</p><h2 id="Vanishing-Lines">Vanishing Lines</h2><p><img src="vanishing_lines.png" alt="vanishing_lines"><br>Horizon is a set of all directions to the infinity.</p><p>In other words, vanishing lines (horizon) is the intersection of image plane and the ground plane that is lifted up.<br><img src="compute_vanishing_lines.png" alt="compute_vanishing_lines"></p><h2 id="How-to-measure-height">How to measure height?</h2><p>If we draw a line of a walking person’s feet (ground plane), and lift up this line to the person’s head, these two lines are parellel in the third-person perspective. But in my perspective, these two lines will intersect to a vanishing point on horizon, which is the same height of camera.<br><img src="comparing_heights.png" alt="comparsing_heights"><br>According to this rule, we need a referenc2e object with known length then we can measure the heights in the scene.<br><img src="measuring_height.png" alt="measuring_height"><br>Connect the bottom of the object and the reference on the groud plane, it will intersect with horizon on a vanishing point. Then connect the vanishing point and the head of the object back to the reference. Now you have a ratio between object and reference so that you can measure the height.</p><hr><h1>Homogeneous Coordinates</h1><p>Before we move on to next section, I think it’s neccesary to know what is homogeneous coordinates and why we use it. This part is from the course video but my understandings.</p><h2 id="What-is-homogeneous-coordinates">What is homogeneous coordinates?</h2><p>It represents coordinates in 2 dimensions with a 3-vector.</p><ul><li>From euclidean to homogeneous<ul><li>add third coordinate as 1<br>(2,3)’ – (2,3,1)’</li><li>add third coordinate as 0 to express infinity<br>(2,3)’ – (2,3,0)’</li></ul></li><li>From homogeneous to euclidean<ul><li>divived by value of third coordinate<br>(4,5,1)’ – (4,5)<br>(8,6,3)’ – (8/3,6/3)</li><li>divived by 0 also proves that the point is in infinity</li></ul></li></ul><h2 id="Why-we-use-homogeneous-coordinates">Why we use homogeneous coordinates?</h2><ul><li>x = cx (x!=0)<br>e.g: (2,4,1)’ == (4,8,2)</li><li>it can represent point at infinity by the form (x,y,0)</li></ul><hr><h1>Perspective Projection</h1><h2 id="How-to-represent-a-point">How to represent a point?</h2><p><img src="projective_plane.png" alt="projective_plane"><br>The point in the image plane can be considered as a ray pointing to infinity.<br><img src="point.png" alt="point"><br>Since we consider a point on place as a ray going through the point and penetrating to the space, we can represent it as a vector using homogeneous coordinates.</p><h2 id="How-to-represent-a-line">How to represent a line?</h2><p><img src="lines.png" alt="lines"><br><strong>Important:</strong> a line is a plane of rays through origin.<br>(a,b,c) is a normal vector to the plane.<br><img src="line_representation.png" alt="line_representation"><br>We can also represent the line with polar coordinate representation.</p><h2 id="Line-passing-through-two-points">Line passing through two points</h2><p><img src="line_two_points.png" alt="line_two_points"><br>For every two points on the plane, there are two rays passing through them respectively from origin. It’s known that two lines define a plane, and sometimes we describe this plane by using it’s normal line.</p><p><img src="calculate_line.png" alt="calculate_line"><br>This is a sample MATLAB code to calculate the line.</p><pre class=" language-language-matlab"><code class="language-language-matlab">function I = get_line_by_two_points(x,y)x1 = [x(1), y(1), 1]';x2 = [x(2), y(2), 1]';I = cross(x1,x2);I = I / sqrt(I(1)*I(1) + I(2)*I(2));</code></pre><h2 id="How-to-find-the-intersection-of-two-lines">How to find the intersection of two lines?</h2><p><img src="intersection_of_lines.png" alt="intersection_of_lines"></p><pre class=" language-language-matlab"><code class="language-language-matlab">function x0 = get_point_by_two_line(I, II)x0 = cross(I,II);x0 = [x0(1)/x0(3); x0(2)/x0(3)];</code></pre><hr><h1>Point-Line Duality</h1><p><img src="duality.png" alt="duality"><br>This slide is to realize the duality of point and line in projective space. If given any formula, we can switch the meanings of poitns and lines to get another formula.</p><p><strong>Good to think:</strong> Now we know that take the cross product of two lines we get a point(ray) on the image plane, but what if the point we get has the form of (x,y,0). If we convert from 3D to 2D, either x or y divived by 0 we will get a point in infinity.<br><img src="point_at_infinity.png" alt="point_at_infinity"></p><p>That’s the point at infinity, what is line at infinity. We have to find a line that every points with form (x,y,0) will pass through it. So, algebraically:<br><img src="line_at_infinity.png" alt="line_at_infinity"></p><h2 id="Ideal-Points-and-Lines">Ideal Points and Lines</h2><p><img src="ideal_point_and_line.png" alt="ideal_point_and_line"></p><hr><h1>Rotations and Translations</h1><h2 id="Camera-Coordinates-World-Coordinates">Camera Coordinates &amp; World Coordinates</h2><p><img src="rgb_xyz.png" alt="rgb_xyz"><br>Convention: rgb to xyz</p><h2 id="What-is-the-geometric-meaning-of-translation">What is the geometric meaning of translation?</h2><p><img src="translation.png" alt="translation"><br>Mathematically, if the world coordinates of point P is (0,0,0), then the camera coordinates of point P is the vector from camera origin to world origin as shown on picture above.</p><h2 id="What-is-the-geometric-meaning-of-rotation">What is the geometric meaning of rotation?</h2><p><img src="rotation.png" alt="rotation"></p><ul><li>r1 is the x-axis of the world with repect to the camera as red</li><li>r2 is the y-axis of the world with repect to the camera as green</li><li>r3 is the z-axis of the world with repect to the camera as blue</li></ul><h2 id="An-example-of-how-to-read-rotation-matrix-and-tranlation-vector">An example of how to read rotation matrix and tranlation vector</h2><p><img src="rotation_matrix.png" alt="rotation_matrix"></p><ul><li>x is parellel to r1 but opposite direction, so r1 is (-1,0,0)’</li><li>y is parellel to r3 but opposite direction, so r2 is (0,0,-1)’</li><li>z is parellel to r2 but opposite direction, so r3 is (0,-1,0)’</li></ul><p><img src="read_translation.png" alt="read_translation"><br>Reading translation is simply read the origin of world to origin of camera. In this case, origin goes toward y direction for 5, and z direction for 10.<br><strong>Important:</strong> the vectors of rotation matrix have to be orthogonal to each other and determinant of rotation matrix has to equal one.</p><h2 id="body-coordinate-system">body coordinate system</h2><p>What if we have one more more coordinate system which is the body coordinates?<br><img src="transform_coordinate.png" alt="transform_coordinate"><br>It will be the same idea of using rotation and translation, but we have a simpler expression which is to use a 4x4 matrix.</p><h2 id="inverse-transformation">inverse transformation</h2><p><img src="inverse_transformation.png" alt="inverse_transformation"><br>Since we know the translation is the vector from camera’s origin to world’s origin, then inverse transformation is from world’s origin back to camera’s origin when keeping the rotation unchanged.</p><h2 id="Alternative-way-to-find-coordinates-after-transformation">Alternative way to find coordinates after transformation</h2><p><img src="golden_fule.png" alt="golden_fule"><br>The approach previously is more intuition-based I think. If you prefer mathematical computation, the formula can be applied to find the coordinates after transformation. Remember to put translation in front of rotations.</p><p><img src="rotation_rule.png" alt="rotation_rule"><br>This is a picture I found from wikipedia if you’re not familiar with rotation with repect to a certain axis.</p><h1>Pinhole Camera Model</h1><p><img src="camera_model.png" alt="camera_model"></p><ul><li>Pink: camera body (camera oritentation and position in the world)</li><li>Blue: sensor (transform optical measurement into pixel)</li><li>Red: focal length</li></ul><p><img src="pinhole_camera.png" alt="pinhole_camera"><br>In daily life, the camera is not a canvas but a reverse canvas. The image plane is behind us rather than in front of us, but we can imagine there is a virtual image in front of us.</p><h2 id="1st-Person-Camera-World">1st Person Camera World</h2><p><img src="1st_person.png" alt="1st_person"></p><p>How to define is that x-axis is the horizontal line in front of you, and y-axis is a vertical line in front of you, then z-axis is pointing toward/from you due to right-hand rule.</p><p>The point c in the image in you, as the center of the universe (0,0,0), and there is a xy-plane in front of you. The item we see through the image plane can be shrinked to the image plane through the equations above.</p><p>Since the image plane is fixed-size. If I take image plane further out, away from object, then object image will be smaller.</p><p>At this point, it’s good to think about:</p><ul><li>Where is the center of projection?</li><li>What is the focal length?</li></ul><h2 id="Let’s-do-a-experiement">Let’s do a experiement</h2><ol><li><p>Draw a set of rediating line on a piece of paper.</p></li><li><p>Place your phone camera as the picture shown.<br><img src="locating_center.png" alt="locating_center"></p></li><li><p>Look at the image in your camera, and move your camera back and forth until you see every line is parellel to each other.<br><img src="parellel_lines.gif" alt="parellel_lines"><br>(from my iPhone)</p></li><li><p>Draw a line to record the camera position.</p></li></ol><p>This process can be illustrated in this diagram.<br><img src="locate_center.png" alt="locate_center"></p><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course is being offered on &lt;a href=&quot;https://www.coursera.org/learn/robotics-perception/&quot;&gt;Coursera&lt;/a&gt; </summary>
      
    
    
    
    
    <category term="Study Note" scheme="https://gaoxiangluo.github.io/tags/Study-Note/"/>
    
    <category term="Coursera" scheme="https://gaoxiangluo.github.io/tags/Coursera/"/>
    
    <category term="Robotics" scheme="https://gaoxiangluo.github.io/tags/Robotics/"/>
    
    <category term="Perception" scheme="https://gaoxiangluo.github.io/tags/Perception/"/>
    
  </entry>
  
  <entry>
    <title>Work on multiple computers for your schoolwork? Use git!</title>
    <link href="https://gaoxiangluo.github.io/2020/09/04/Work-on-multiple-computers-for-your-schoolwork-Use-git/"/>
    <id>https://gaoxiangluo.github.io/2020/09/04/Work-on-multiple-computers-for-your-schoolwork-Use-git/</id>
    <published>2020-09-05T02:16:57.000Z</published>
    <updated>2020-09-12T14:24:32.473Z</updated>
    
    <content type="html"><![CDATA[<h1>Sync Computers</h1><p>How do I use git to sync my Projects/HWs/Labs if I need to code them on laptop/desktop/lab machine?</p><pre class=" language-language-bash"><code class="language-language-bash"># On the first device$ git init$ git add .$ git commit -m "First release"$ git push </code></pre><pre class=" language-language-bash"><code class="language-language-bash"># On other devices$ git clone URL # end with .git</code></pre><pre class=" language-language-bash"><code class="language-language-bash"># From now on, every time one of devices is ready to release$ git commit -a -m "Next release"$ git push</code></pre><pre class=" language-language-bash"><code class="language-language-bash"># Any devices that want to sync to latest release$ git pull # a pull is simply a fetch then merge</code></pre><p>What if you want to continue edit on your file that wasn’t finished on other devices? In other words, without pushing.</p><pre class=" language-language-bash"><code class="language-language-bash">$ git pull other.computer:/path/to/files HEAD # HEAD tag is like a cursor that normally points at the latest commit</code></pre><p>If you’re using MacOS or Windows, there is a offcial GUI called <strong>Github Destop</strong> that will do the job for you. Also, <strong>Visual Studio Code</strong> has build-in plugin for git.</p><hr><h1>Branch is Your Friend</h1><p>What if you just want to try out some new ideas or maybe make a hotfix without interrupting the main repo from being used?</p><pre class=" language-language-bash"><code class="language-language-bash"># Try new ideas$ git commit -m "Initial commit" # always remember to commit before switching around$ git checkout -b new_branch # create a new branch and switch to it#### try some new ideas on this branch ####$ git commit -a -m "Another commit" # good habbit to commit$ git checkout master # switch back to master$ git branch -m new_branch # if you want to remove the branch that was created</code></pre><pre class=" language-language-bash"><code class="language-language-bash"># Quick Fixes$ git commit -a$ git checkout -b fixes###### fix goes here #######$ git commit -a -m "Bug fixed"$ git checkout master# You can even merge in the freshly baked bugfix$ git merge fixes</code></pre><hr><h1>Commit Message is Important</h1><p>Sometimes it’s good to be descriptive in your commit, so that other users will understand what changes you’ve made. If you use two <code>-m</code> flag, the first one will act like a title and the second will act like a descriptive body.</p><pre class=" language-language-bash"><code class="language-language-bash">$ git commit -m "title" -m "more detailed info"</code></pre><p>Did you just commit, but wish you had typed a different message? This can help you to change the last message.</p><pre class=" language-language-bash"><code class="language-language-bash">$ git commit --amend -m "an updated commit message"</code></pre><hr><h1>Acknowledgement</h1><p>Thanks <a href="http://www-cs-students.stanford.edu/~blynn/">Ben Lynn</a> for publishing the fun and informative git guide <a href="http://www-cs-students.stanford.edu/~blynn/gitmagic/">git magic</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Sync Computers&lt;/h1&gt;
&lt;p&gt;How do I use git to sync my Projects/HWs/Labs if I need to code them on laptop/desktop/lab machine?&lt;/p&gt;
&lt;pre clas</summary>
      
    
    
    
    
    <category term="How-to" scheme="https://gaoxiangluo.github.io/tags/How-to/"/>
    
    <category term="Git" scheme="https://gaoxiangluo.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Build a blog with Hexo</title>
    <link href="https://gaoxiangluo.github.io/2020/08/29/Build-a-blog-with-Hexo/"/>
    <id>https://gaoxiangluo.github.io/2020/08/29/Build-a-blog-with-Hexo/</id>
    <published>2020-08-30T01:15:25.000Z</published>
    <updated>2020-09-12T14:24:30.987Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Setup">Setup</h2><ol><li><p>Install <code>Node.js</code>.</p><ul><li>MacOS: Download <code>Node.js</code> from <code>https://nodejs.org/</code>. When you install <code>Node.js</code>, <code>npm</code> will be installed automatically.</li><li>Ubuntu 20.04: Download them from command line.<pre class=" language-language-bash"><code class="language-language-bash">$ sudo apt update$ sudo apt install nodejs$ sudo apt install npm</code></pre></li></ul></li><li><p>When you install <code>Node.js</code>, <code>npm</code> will be installed automatically.</p></li><li><p>Make sure you have <code>git</code> installed.</p></li><li><p>Download Hexo’s framework using <code>npm</code>.</p><pre class=" language-language-bash"><code class="language-language-bash">$ sudo npm install -g hexo-cli</code></pre></li></ol><hr><h2 id="Initialization">Initialization</h2><ol><li><p>Make a new folder and navigate to it through terminal.</p></li><li><p>Initialize a blog with Hexo command line under the folder.</p><pre class=" language-language-bash"><code class="language-language-bash">$ hexo init</code></pre></li><li><p>Start a local server to host your blog.</p><pre class=" language-language-bash"><code class="language-language-bash">$ hexo server</code></pre></li><li><p>Now open a browser, and type <code>localhost:4000</code> in address bar. You will see the blog that you just created!</p></li></ol><hr><h2 id="Host-the-blog-on-Github">Host the blog on Github</h2><ol><li><p>In order to have others see your blog, you have to find a place to host your blog. Here I suggest using GitHub page, because each Github account can have a free domain ending with <code>github.io</code>.<br>npm install hexo-deployer-git --save</p></li><li><p>Go to Github, create a new repository and name it <code>web-name.github.io</code> (<strong>replace <code>web-name</code> with the name that you want</strong>). Make sure the repository is <strong>public</strong>.<br><img src="init-github-io.png" alt="Make a new repo to host blogs"></p></li><li><p>Open <code>_config.yml</code> with your favorite editor, and configure it as follows:</p><pre class=" language-language-yml"><code class="language-language-yml">deploy:  type: git  repo: https://github.com/GaoxiangLuo/web-name.github.io.git  branch: master</code></pre></li><li><p>Deploy your blog onto Github, login with username and password if being prompted</p><pre class=" language-language-bash"><code class="language-language-bash">$ hexo deploy</code></pre></li><li><p>Congratulation! You blog has been published at <code>web-name.github.io</code>!</p></li></ol><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Setup&quot;&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;code&gt;Node.js&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MacOS: Download &lt;code&gt;Node.js&lt;/code&gt; from &lt;code&gt;https://no</summary>
      
    
    
    
    
    <category term="How-to" scheme="https://gaoxiangluo.github.io/tags/How-to/"/>
    
    <category term="Web Dev" scheme="https://gaoxiangluo.github.io/tags/Web-Dev/"/>
    
    <category term="Hexo" scheme="https://gaoxiangluo.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>I started my blog</title>
    <link href="https://gaoxiangluo.github.io/2020/08/29/I-started-my-blog/"/>
    <id>https://gaoxiangluo.github.io/2020/08/29/I-started-my-blog/</id>
    <published>2020-08-29T19:13:36.000Z</published>
    <updated>2020-09-07T03:49:56.219Z</updated>
    
    <content type="html"><![CDATA[<p>Hey all! I finally started my own blog, since I’ve been captured by how elegant the markdown language is. There are some reasons for me to start blogging.  Firstly, it motivates me to summarize the cool stuff that I’ve learned, which helps me to review and gain deeper insights of those knowledge. Secondly, I hope my blog will be a place where people can know me more; meanwhile, I would like to contribute my effort to the overall ComSci society, in publishing blogs that help people resolve technical issues. I hope it’s not too late to start as a rising junior undergrad, and I would like to see my blog grows as I grow!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hey all! I finally started my own blog, since I’ve been captured by how elegant the markdown language is. There are some reasons for me t</summary>
      
    
    
    
    
    <category term="Log" scheme="https://gaoxiangluo.github.io/tags/Log/"/>
    
  </entry>
  
</feed>
