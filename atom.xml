<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leon</title>
  
  
  <link href="https://gaoxiangluo.github.io/atom.xml" rel="self"/>
  
  <link href="https://gaoxiangluo.github.io/"/>
  <updated>2020-10-12T19:44:02.537Z</updated>
  <id>https://gaoxiangluo.github.io/</id>
  
  <author>
    <name>Gaoxiang Luo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Overview of Support Vector Machine (SVM) and Ensemble Learning</title>
    <link href="https://gaoxiangluo.github.io/2020/10/09/Overview-of-Support-Vector-Machine-SVM-and-Ensemble-Learning/"/>
    <id>https://gaoxiangluo.github.io/2020/10/09/Overview-of-Support-Vector-Machine-SVM-and-Ensemble-Learning/</id>
    <published>2020-10-09T19:31:40.000Z</published>
    <updated>2020-10-12T19:44:02.537Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 5523) is being offered by <a href="https://www-users.cs.umn.edu/~kumar001/">Prof. Vipin Kumar</a> at the University of Minnesota in Fall 2020.</p></blockquote><h1>Support Vector Machines</h1><p>We will start by looking at this image and think about which decision boundary is better? B1 or B2?<br><img src="figure1a.png" alt="figure1"><br>If we find the hyperplace maximizes the margin as the image does, then we can tell B1 is better than B2, which means if we have one more record B1 is more likely to correctly classify the new record.</p><h2 id="Linear-SVM">Linear SVM</h2><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=f%28%5Coverrightarrow%7Bx%7D%29%20%3D%20%5Cbegin%7Bcases%7D%201%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%20if%20%5Coverrightarrow%7Bw%7D%20%5Cbullet%20%5Coverrightarrow%7Bx%7D%20%5Cgeqslant%201%20%5C%5C%20-1%20%5C%2C%20%5C%2C%20%5C%2C%20if%20%5Coverrightarrow%7Bw%7D%20%5Cbullet%20%5Coverrightarrow%7Bx%7D%20%5Cleqslant%20-1%20%5Cend%7Bcases%7D%20%0A" /></p><p>Learning the model is equaivalent to determining the values of <img src="https://math.now.sh?inline=%5Coverrightarrow%7Bw%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=b" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and our objective is to maximize: <img src="https://math.now.sh?inline=Margin%20%3D%20%5Cfrac%7B2%7D%7B%5C%7C%5Coverrightarrow%7Bw%7D%5C%7C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, which is equivalent to minimizing: <img src="https://math.now.sh?inline=L%28%5Coverrightarrow%7Bw%7D%29%3D%5Cfrac%7B%5C%7C%5Coverrightarrow%7Bw%7D%5C%7C%5E2%7D%7B2%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. And this is a <strong>constrained optimization problem</strong>.</p><p>What if the problem is not linearly separable like the image following?<br><img src="figure2a.png" alt="figure2"><br>The notion is the same, we’re still utilizing the idea of margin, but we want to add some adjustment to some errors. Then we introduce slack variables.<br>We need to minimize: <img src="https://math.now.sh?inline=L%28%5Coverrightarrow%7Bw%7D%29%3D%5Cfrac%7B%5C%7C%5Coverrightarrow%7Bw%7D%5C%7C%5E2%7D%7B2%7D%20%2B%20C%5C%7B%5Csum_%7Bi%3D1%7D%5EN%20%5Cxi_i%5Ek%20%5C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br>subject to:</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=f%28%5Coverrightarrow%7Bx%7D%29%20%3D%20%5Cbegin%7Bcases%7D%201%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%20if%20%5Coverrightarrow%7Bw%7D%20%5Cbullet%20%5Coverrightarrow%7Bx%7D%20%5Cgeqslant%201%20-%20%5Cxi_i%20%5C%5C%20-1%20%5C%2C%20%5C%2C%20%5C%2C%20if%20%5Coverrightarrow%7Bw%7D%20%5Cbullet%20%5Coverrightarrow%7Bx%7D%20%5Cleqslant%20-1%20%2B%20%5Cxi_i%20%5Cend%7Bcases%7D%0A" /></p><p>It turns out that we can actually also use this version in some data set which is separable.<br><img src="figure3a.png" alt="figure3"><br>There is a trade-off here. B1 has a wider margin but also has a record misclassififed (error), while B2 has no errors but with a narrower margin. We can’t tell which one is performaning better because it’s relevant to the specific question, but using the validation set will always give us an answer.</p><h2 id="Nonlinear-SVM">Nonlinear SVM</h2><p><img src="figure4a.png" alt="figure4"><br>We can try to transform the data into higher dimensional space:<br><img src="figure5a.png" alt="figure5"><br>Since this is an intro class, we didn’t go into details of this nonliear example.</p><h2 id="Kernel-Trick">Kernel Trick</h2><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5CPhi%28x_i%29%20%5Cbullet%20%5CPhi(x_j)%20%3D%20K(x_i%2Cx_j)%0A" /></p><p><img src="https://math.now.sh?inline=K%28x_i%2Cx_j%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is a kernal function (expressed in terms of the coordinates in the original space)<br>Examples:</p><ul><li><img src="https://math.now.sh?inline=K%28x%2Cy%29%3D(x%20%5Ccdot%20y%20%2B%201)%5Ep" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li><img src="https://math.now.sh?inline=K%28x%2Cy%29%20%3D%20e%5E%7B%5Cfrac%7B-%5C%7Cx%20-%20y%20%5C%7C%5E2%7D%7B2%20%5Csigma%5E2%7D%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li><img src="https://math.now.sh?inline=K%28x%2Cy%29%20%3D%20tanh(kx%20%5Ccdot%20y%20-%20%5Cdelta)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul><p>Advantages of using kernel:</p><ul><li>Don’t have to know the mapping function <img src="https://math.now.sh?inline=%5CPhi" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li>Computing dot product <img src="https://math.now.sh?inline=%5CPhi%28x_i%29%20%5Cbullet%20%5CPhi(x_j)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> in the original space avoids curse of dimensionality</li></ul><p>Not all functions can be kernels:</p><ul><li>Must make sure there is a corresponding <img src="https://math.now.sh?inline=%5CPhi" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> in some high-dimensional space</li><li>Mercer’s theorem</li></ul><h2 id="Characteristics-of-SVM">Characteristics of SVM</h2><ul><li><p>The learning problem is formulated as a convex optimization problem</p><ul><li>Efficient algorithms are avaiable to find the global minima</li><li>Many of the other methods use greedy approaches and find locally optimal solutions</li><li>High computational complexity for building the model</li></ul></li><li><p>Robust to noise</p></li><li><p>Overfitting is handled by maximizing the margin of the decision boundary</p></li><li><p>SVM can handle irrelevant and redundant better than many other techniques</p></li><li><p>The user needs to provide the type of kernel function and cost function</p></li><li><p>Difficult to handle missing values</p></li></ul><h1>Ensemble Methods</h1><ul><li>Constrct a set of base classifiers leanred from the training data</li><li>Predict class label of test records by combining the predictions made by multiple classifiers (e.g., by taking majority vote)</li></ul><h2 id="Why-does-ensemble-methods-work">Why does ensemble methods work?</h2><p>Suppose there are 25 base classifiers</p><ul><li>Each classifier has error rate, <img src="https://math.now.sh?inline=%5Cepsilon%20%3D%200.35" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li>Majority vote of classifiers used for classification</li><li>If all classifiers are identical, then error rate of ensemble = <img src="https://math.now.sh?inline=%5Cepsilon%280.35%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li>If all classifiers are independent (errors are uncorrelated): Error rate of ensemble = probability of having more than half of base classifiers being wrong</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=e_ensemble%20%3D%20%5Csum_%7Bi%3D13%7D%5E25%20%7B25%20%5Cchoose%20i%7D%20%5Cepsilon%5Ei%20%281-%5Cepsilon%29%5E%7B25-i%7D%3D0.06%0A" /></p><h2 id="Necessary-conditions-for-ensemble-methods">Necessary conditions for ensemble methods</h2><p>Ensemble methods work better than a single base classifier if:</p><ol><li>All base classifiers are independent of each other</li><li>All base classifiers perform better than random guessing (error rate &lt; 0.5 for binary classification)</li></ol><p><img src="figure1.png" alt="figure1"></p><h2 id="Rationale-for-ensemble-learning">Rationale for ensemble learning</h2><p>Ensemble methods work best with unstable base classifiers</p><ul><li>Classifiers that are sensitive to minor perturbations in training set, due to high model complexity (e.g., a little change of records may end up with a different model)</li><li>Examples: Unpruned decision trees, ANNs, …</li><li>Low Bias in finding optimal decision boundary</li><li>High variance for minor changes in training set or model selection procedure</li></ul><h2 id="Bias-Variance-Decomposition">Bias-Variance Decomposition</h2><p>Analogous problem of reaching a target y by firing projectiles from x (regression problem):<br><img src="figure2.png" alt="figure2"><br>On the right side of the image, those stars are actual values of y, while those dots on the left are predict values of y, and the difference between of their averages is the Bias.</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=gen.error%28m%29%3Dc_1%20%5Ctimes%20noise%20%2B%20bias(m)%20%2B%20c_2%20%5Ctimes%20variance(m)%0A" /></p><p><img src="figure3.png" alt="figure3"><br>Both of the situation above are bad. Ensemble methods try to reduce the variance of complex models (with low bias) by aggregating responses of multiple base classfiers.</p><p>A visual for ensemble learning:<br><img src="figure4.png" alt="figure4"></p><h2 id="Constructing-ensemble-classifiers">Constructing ensemble classifiers</h2><ul><li>By manipulating training set (e.g., bagging, boosting)</li><li>By manipulating input features (e.g., random forests)</li><li>By manipulating class labels (e.g., error-correcting output coding)</li><li>By manipulating learning algorithm (e.g., injecting randomness in ANN or decision tree)</li></ul><h2 id="Bagging-Bootstrap-AGGregatING">Bagging (Bootstrap AGGregatING)</h2><ul><li>Bootstrap sampling: sampling with replacement<br><img src="figure5.png" alt="figure5"><br>You can do different round of bagging, and each data is equally probability to be selected.</li><li>Build classifier on each bootstrap sample</li><li>Probability of a training instance being selected in a bootstrap sample is:<ul><li><img src="https://math.now.sh?inline=1%20-%20%281%20-%20%5Cfrac%7B1%7D%7Bn%7D%29%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (n: number of training instances)</li><li>~0.632 when n is large</li></ul></li></ul><p><img src="figure6.png" alt="figure6"></p><h2 id="Boosting">Boosting</h2><p>An iterative procedure to adaptively change distribution of training data by focusing more on previously misclassified records</p><ul><li>Initially, all N records are assigned equal weights (for being selected for training)</li><li>Records that are wrongly classified will have their weights increased in the next round</li><li>Records that are classified correctly wil have their weights decreased in the next round</li><li>Unlike bagging, weights may change at the end of each boosting round</li></ul><p>e.g., AdaBoost</p><h2 id="Random-Forest-Algorithm">Random Forest Algorithm</h2><p>Construct an ensemble of decision trees by manipulating training set as well as features</p><ul><li>Use bootstrap sample to train every decision tree (similiar to Bagging)</li><li>Use the following tree induction algorithm:<ul><li>At every internal node of decision tree, randomly sample p attributes for selecting split criterion</li><li>Repeat this procedure until all leaves are pure (unpruned tree)</li></ul></li></ul><h4 id="Characterstics-of-Random-Forest">Characterstics of Random Forest</h4><ul><li>Base classifiers are unpruned trees and hence are unstable classifiers</li><li>Base classifers are decorrelated (due to randomization in training set as well as features)</li><li>Random forests reduce variance of unstable classifiers without negatively impacting the bias</li><li>Selection of hyper-parameter p<ul><li>Small value ensures lack of correlation</li><li>High value promotes strong base classifiers</li><li>Common default choice <img src="https://math.now.sh?inline=%5Csqrt%7Bd%7D%2C%20log_2%28d%2B1%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 5523) is being offered by &lt;a href=&quot;https://www-users.cs.umn.edu/~kumar001/&quot;&gt;Prof. Vipin K</summary>
      
    
    
    
    
    <category term="Data Mining" scheme="https://gaoxiangluo.github.io/tags/Data-Mining/"/>
    
    <category term="SVM" scheme="https://gaoxiangluo.github.io/tags/SVM/"/>
    
    <category term="Ensemble Learning" scheme="https://gaoxiangluo.github.io/tags/Ensemble-Learning/"/>
    
    <category term="Random Forest" scheme="https://gaoxiangluo.github.io/tags/Random-Forest/"/>
    
    <category term="Bagging" scheme="https://gaoxiangluo.github.io/tags/Bagging/"/>
    
    <category term="Boosting" scheme="https://gaoxiangluo.github.io/tags/Boosting/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining: Imbalanced class problem</title>
    <link href="https://gaoxiangluo.github.io/2020/10/06/Data-Mining-Imbalanced-class-problem/"/>
    <id>https://gaoxiangluo.github.io/2020/10/06/Data-Mining-Imbalanced-class-problem/</id>
    <published>2020-10-07T00:43:25.000Z</published>
    <updated>2020-10-14T02:02:40.683Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 5523) is being offered by <a href="https://www-users.cs.umn.edu/~kumar001/">Prof. Vipin Kumar</a> at the University of Minnesota in Fall 2020.</p></blockquote><p>Lots of classification problems where the classes are skewed (more records from one class than another)</p><ul><li>Credit card fraud</li><li>Intrusion detection</li><li>Defective products in manufacturing assembly line</li></ul><h2 id="Challenges">Challenges</h2><ul><li>Evaluation measures such as accuracy are not well-suited for imbalanced class.</li><li>Detecting the rare class is like finding a needle in a haystack (classic parable haha)</li></ul><h2 id="Confusion-Matrix">Confusion Matrix</h2><p><img src="figure1.png" alt="figure1"></p><ul><li>a: TP (true positive)</li><li>b: FN (fasle negative)</li><li>c: FP (false postiive)</li><li>d: TN (true negative)</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Accuracy%20%3D%20%5Cfrac%7BTP%2BTN%7D%7BTP%2BTN%2BFP%2BFN%7D%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=ErrorRate%3D1-Accuracy%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Precision%28p%29%3D%20Positive%20%5C%2C%20Predictie%20%5C%2C%20Value%20%3D%20%5Cfrac%7BTP%7D%7BTP%2BFP%7D%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Recall%28r%29%3DSensitivity%20%3D%20%5Cfrac%7BTP%7D%7BTP%2BFN%7D%0A" /></p><p>Recall is out of all the positive samples that are given, what fraction of them this classifier correctly classify. Sensitivity is often used in medical community.</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=F-measure%28F%29%20%3D%20%5Cfrac%7B2rp%7D%7Br%2Bp%7D%20%3D%20%5Cfrac%7B2TP%7D%7B2TP%2BFN%2BFP%7D%0A" /></p><p>Accuracy alone is not a sufficient way to evaluate models, so we introduce precision and recall, and then using F-measure to take both of them into account.</p><p><img src="figure2.png" alt="figure2"><br>Let’s look at the example above. There isn’t really a one measure is better than another, because it depends on your situation particularly. For example, the first one here might be a bad idea for COVID testing, because the precision is only 50%, then the second one is favored.</p><p>There are more measures of classification performance.</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Specificity%3DTN%20%5C%2C%20Rate%20%3D%20%5Cfrac%7BTN%7D%7BTN%2BFP%7D%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=FP%20%5C%2C%20Rate%20%3D%20%5Calpha%20%3D%20%5Cfrac%7BFN%7D%7BFN%2BTP%7D%20%3D%201%20-%20Specificity%0A" /></p><p><img src="https://math.now.sh?inline=%5Calpha" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the probibility that we reject the null hypothesis when it is true. This is a Type I error or a false positive (FP).</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=FN%20%5C%2C%20Rate%20%3D%20%5Cbeta%20%3D%20%5Cfrac%7BFN%7D%7BFN%2BTP%7D%20%3D%201-%20Sensitivity%0A" /></p><p><img src="https://math.now.sh?inline=%5Cbeta" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the probability that we accept the null hypothesis when it is false. This is a Type II error or a false negative (FN).</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Power%20%3D%20Sensitivity%20%3D%201%20-%20%5Cbeta%0A" /></p><p><img src="figure3.png" alt="figure3"><br>Let’s look at another example. These two cases are actually the same classifier. If we look at the actual class yes row, both case A and B give us 80% of right prediction. And if we look at actual class no row, both case A and B gives us 80% of right prediction too. In short, the <img src="https://math.now.sh?inline=%5Cfrac%7BTP%20Rate%7D%7BFP%20Rate%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the same for these two cases. The only difference here maybe the sample size, but they’re the same classifier. Notice that here the F-measures are different. This is an exmaple to show you that F-measeare can potentially fail us.</p><h2 id="ROC-Receiver-Operating-Characteristic">ROC (Receiver Operating Characteristic)</h2><p><img src="figure4.png" alt="figure4"><br>Every points sitting on the red curve, the TPR is higher than FRP.</p><p>The coordinates on the graph is (TPR,FPR).</p><ul><li>(0,0): declare everything to be negative class</li><li>(1,1): declare everything to be positive class</li><li>(1,0): ideal</li></ul><h3 id="Using-ROC-for-Model-Comparison">Using ROC for Model Comparison</h3><p><img src="figure5.png" alt="figure5"></p><h3 id="Hot-to-construct-an-ROC-curve">Hot to construct an ROC curve</h3><p><img src="figure6.png" alt="figure6"><br>In the image above, the score is the fraction of positive prediction and negative prediction in the leaf node (decision tree, for instance). This is a way to convert discrete ouput value to continuous value because we need continuous value to plot the ROC curve. The following is how it may look like:<br><img src="figure7.png" alt="figure7"></p><h3 id="How-to-build-classifier-with-imbalanced-training-set">How to build classifier with imbalanced training set</h3><p>Modify the distribution of training data so that rare class is well-resented in training set.</p><ul><li>Undersample the majority class</li><li>Oversample the rare class</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 5523) is being offered by &lt;a href=&quot;https://www-users.cs.umn.edu/~kumar001/&quot;&gt;Prof. Vipin K</summary>
      
    
    
    
    
    <category term="Data Mining" scheme="https://gaoxiangluo.github.io/tags/Data-Mining/"/>
    
    <category term="Confusion Matrix" scheme="https://gaoxiangluo.github.io/tags/Confusion-Matrix/"/>
    
    <category term="Receiver operating characteristic" scheme="https://gaoxiangluo.github.io/tags/Receiver-operating-characteristic/"/>
    
  </entry>
  
  <entry>
    <title>Overview of K Nearest Neighbor Classifiers and Rule-Based Classifiers</title>
    <link href="https://gaoxiangluo.github.io/2020/10/03/Overview-of-K-Nearest-Neighbor-Classifiers-and-Rule-Based-Classifiers/"/>
    <id>https://gaoxiangluo.github.io/2020/10/03/Overview-of-K-Nearest-Neighbor-Classifiers-and-Rule-Based-Classifiers/</id>
    <published>2020-10-03T05:17:12.000Z</published>
    <updated>2020-10-04T05:23:39.811Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 5523) is being offered by <a href="https://www-users.cs.umn.edu/~kumar001/">Prof. Vipin Kumar</a> at the University of Minnesota in Fall 2020.</p></blockquote><h1>K Nearest Neighbor (KNN) Classfiers (Overview)</h1><p>Basic idea: If it walks like a duck, quacks like a duck, then it’s probably a duck.</p><p>Require three things:</p><ul><li>The set of labeled records</li><li>Distance metric to compute distance between records</li><li>The value of <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, the number of nearest neighbors to retrieve</li></ul><p>To classify an unknown record:</p><ul><li>Compute distance to other training records</li><li>identify k nearest neighbors</li><li>Use class labels of nearest neighbors to determine the class label of unknown record</li></ul><p>Sometimes we call KNN the laziest because it doesn’t do anything until we have to make a decision. In other words, there is not much happening during the training period.</p><p>To recall: to compute proximity between two points, we need certain distance metric.<br>For example: Euclidean distance</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=d%28x%2Cy%29%20%3D%20%5Csqrt%7B%5Csum_i(x_i-y_i)%5E2%7D%0A" /></p><p>Next, determine the class from nearest neighbor list</p><ul><li>Take the majority vote of class labels among the KNNs</li><li>Weight the vote accoridng to distance. E.g. weight factor <img src="https://math.now.sh?inline=w%20%3D%20%5Cfrac%7B1%7D%7Bd%5E2%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul><h2 id="The-value-of-k-matters">The value of k matters</h2><ul><li>If k is too small, sensitive to noice points</li><li>If k is too large, neighborhood may include points from other classes</li></ul><h2 id="Choice-of-promixity-measure-matters">Choice of promixity measure matters</h2><p>For documents, cosine is better than correlation or Euclidean. E.g: in the image below, they have the same Euclidean distance but different cosine similarity.</p><p><img src="ex1.png" alt="ex1"></p><h2 id="Data-preprocessing-is-often-required">Data preprocessing is often required</h2><p>Arributes may have to be scaled to prevent distance measures from being dominated by one of the attributes. In addition, time series are often standardized to have a mean of 0 and a standard deviation of 1.</p><p>Is there a notion of complexity in KNN? Yes, the complexity may depend on the choice of <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. If the choice of <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is 1, then we will get many arbitrary shapes that include a single record respectively. Any classifiers we learn, we shall think about its complexity. Decision tree’s complexity can be the depth of the tree, or whether the tree is a binary tree.</p><h2 id="Issues-of-missing-values-in-training-and-test-sets">Issues of missing values in training and test sets</h2><ul><li>Proximity computations normally require the presence of all attributes</li><li>Some appoaches use the subset of attributes present in two instances</li></ul><h2 id="Issues-of-irrelevant-and-redundant-attributes">Issues of irrelevant and redundant attributes</h2><ul><li>Irrelevant attributes add noise to the proximity measure</li><li>Redundant attributes bias the proximity measure towards certain attributes</li><li>Can use variable selection or dimensionality reduction to address irrelevant and redundant attributes</li></ul><h2 id="Improving-KNN-Efficiency">Improving KNN Efficiency</h2><ul><li>Avoid having to compute distance to all objects in the training set<ul><li>Multi-dimensional access methods (k-d trees)</li><li>Fast approximate similarity search</li><li>Locality Sensetive Hashing (LSH)</li></ul></li><li>Condensing<ul><li>Determine a smaller set of objects that give the same performance</li></ul></li><li>Editing<ul><li>Remove objects to improve efficiency</li></ul></li></ul><h1>Rule-Based Classifiers</h1><p>Rule-based classifier: classify records by using a collection of “if…then…” rules.</p><p>Rule: (Condition) <img src="https://math.now.sh?inline=%5Crightarrow" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> <img src="https://math.now.sh?inline=y" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p><ul><li><p>Where</p><ul><li>Condition is a conjection of tests on attributes</li><li><img src="https://math.now.sh?inline=y" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the class label</li></ul></li><li><p>Exmaples of classification rules</p><ul><li>(Blood Type=Warm) <img src="https://math.now.sh?inline=%5Cbigcap" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (Lay Eggs=Yes) <img src="https://math.now.sh?inline=%5Crightarrow" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> Birds</li><li>(Taxable Income&lt;50K) <img src="https://math.now.sh?inline=%5Cbigcap" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (Refund=Yes) <img src="https://math.now.sh?inline=%5Crightarrow" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> Evade=No</li></ul></li></ul><p>An example:<br><img src="figure1.png" alt="figure1"><br>A rule <img src="https://math.now.sh?inline=R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> covers an instance <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> if the attributes of the instance satisfy the condition of the rule, just like <img src="https://math.now.sh?inline=R1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=R2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, etc above. <img src="https://math.now.sh?inline=R1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> covers a bird.</p><h2 id="Rule-Coverage-and-Accuracy">Rule Coverage and Accuracy</h2><ul><li>Coverage of a rule</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7B%5Ctext%7Brecords%20that%20satisfy%20the%20antecedent%20of%20a%20rule%7D%7D%7B%5Ctext%7Btotal%20records%7D%7D%0A" /></p><ul><li>Accuracy of a rule</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7B%5Ctext%7Brecords%20that%20satisfy%20the%20antecedent%20that%20also%20satisfy%20the%20consequent%20of%20a%20rule%7D%7D%7B%5Ctext%7Brecords%20that%20satisfy%20the%20antecedent%20of%20a%20rule%7D%7D%0A" /></p><h2 id="Characteristics-of-Rule-Sets-Strategy-1">Characteristics of Rule Sets: Strategy 1</h2><ul><li><p>Mutually exclusive rules</p><ul><li>Classifier contains mutually exclusive rules if the rules are independent of each other</li><li>Every record is covered by at most one rule</li></ul></li><li><p>Exhaustive Rules</p><ul><li>Classifier has exhaustive coverage if it accounts for every possible combination of attribute values</li><li>Each record is covered by at least one rule</li></ul></li></ul><h2 id="What-if-rules-are-not-mutually-exclusive">What if rules are not mutually exclusive?</h2><ul><li>We can solve this by ordering rule sets. Rules are rank ordered according to their priority. When a test record is presented to the classifier, it will be assigned to the class label of the higheset ranked rule it has triggered. If none of the rules fired, it will be assigned to the default class. There are two ways to order rules:<ul><li>Rule-based ordering: Individual rules are ranked based on their quality</li><li>Class-based ordering: Rules that belong to the same class appear together</li></ul></li></ul><h2 id="Direct-method-to-build-classification-rules">Direct method to build classification rules</h2><ol><li>Start from an empty rule</li><li>Grow a rule using the Learn-One-Rule function</li><li>Remove training records covered by the rule</li><li>Repeat Step 2 and 3 until stopping criterion is met</li></ol><p>Here is a visual example of the steps above:<br><img src="figure2.png" alt="figure2"><br><img src="figure3.png" alt="figure3"></p><p>Here are two common strategies to grow the rule:<br><img src="figure4.png" alt="figure4"></p><h2 id="Rule-evaluation">Rule evaluation</h2><ul><li>Foil’s Information Gain (FOIL: First Order Inductive Learner – an early rule-based learning algorithm)</li><li><img src="https://math.now.sh?inline=R_0%3A%20%5C%7B%5C%7D%20%5Crightarrow%20class%20%5Ctext%7B%28initial%20rule%29%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li><img src="https://math.now.sh?inline=R_1%3A%20%5C%7BA%5C%7D%20%5Crightarrow%20class%20%5Ctext%7B%28rule%20after%20adding%20conject%29%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gain%28R_0%2CR_1%29%20%3D%20p_1%20%5Ctimes%20%5B%20log_2(%5Cfrac%7Bp_1%7D%7Bp_1%2Bn_1%7D)-%20log_2(%5Cfrac%7Bp_0%7D%7Bp_0%2Bn_0%7D)%20%5D%0A" /></p><ul><li><img src="https://math.now.sh?inline=p_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of positive instances covered by R0</li><li><img src="https://math.now.sh?inline=n_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of negative instances covered by R0</li><li><img src="https://math.now.sh?inline=p_1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of positive instances covered by R1</li><li><img src="https://math.now.sh?inline=n_1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of negative instances covered by R1</li></ul><h2 id="Direct-Method-RIPPER">Direct Method: RIPPER</h2><ul><li>For 2-class problem, choose one of the classes as positive class, and the other as negative class.<ul><li>Learn rules for postiive class</li><li>Negative class will be default class</li></ul></li><li>For multi-class problem<ul><li>Order the classes according to increasing class prevelence (fraction of instances that belong to a particular class)</li><li>Learning the rule set for smallest class first, treat the rest as negative class</li><li>Repeat with next smallest class as positive class</li></ul></li></ul><p>How to grow a rule in RIPPER?</p><ul><li>Start from empty rule</li><li>Add conjuncts as long as they improve FOIL’s information gain</li><li>Stop when rule no longer covers negative examples</li><li>Prune the rule immediately using incremental reduced error pruning</li><li>Measure for pruning <img src="https://math.now.sh?inline=v%20%3D%20%5Cfrac%7Bp-n%7D%7Bp%2Bn%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><ul><li><img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of positive examples covered by the rule in the validation set</li><li><img src="https://math.now.sh?inline=n" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of negative examples covered by the rule in the validation set</li></ul></li><li>Pruning method: delete any final sequence of conditions that maximizes <img src="https://math.now.sh?inline=v" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul><p>Build a rule set:</p><ul><li>Use sequential covering algorithms<ul><li>Find the best rule that covers the current set of positive examples</li><li>Eliminate both positive and negative examples covered by the rule</li></ul></li><li>Each time a rule is added to the rule set compute the new description length<ul><li>Stop adding new rules when the new description length is a bit longer than the smallest description length obtained so far’</li></ul></li></ul><h2 id="Advantages-of-Rule-Based-Classifiers">Advantages of Rule-Based Classifiers</h2><ul><li>Has characterstics quite similiar to decision trees<ul><li>As highly expressive as decision trees</li><li>Easy to interpret (if rules are ordered by class)</li><li>Performance comparable to decision trees<ul><li>Can handle redundant and irrelevant attributes</li><li>Variable interaction can cause issues (e.g., X-OR problem)</li></ul></li></ul></li><li>Better suited for handling imbalanced classes</li><li>Harder to handle missing values in the test set</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 5523) is being offered by &lt;a href=&quot;https://www-users.cs.umn.edu/~kumar001/&quot;&gt;Prof. Vipin K</summary>
      
    
    
    
    
    <category term="Data Mining" scheme="https://gaoxiangluo.github.io/tags/Data-Mining/"/>
    
    <category term="Classifier" scheme="https://gaoxiangluo.github.io/tags/Classifier/"/>
    
    <category term="K Nearest Neighbor" scheme="https://gaoxiangluo.github.io/tags/K-Nearest-Neighbor/"/>
    
    <category term="Rule-Based Classifier" scheme="https://gaoxiangluo.github.io/tags/Rule-Based-Classifier/"/>
    
  </entry>
  
  <entry>
    <title>Math that you need to know for Deep Neural Networks</title>
    <link href="https://gaoxiangluo.github.io/2020/10/01/Math-that-you-need-to-know-for-Deep-Neural-Networks/"/>
    <id>https://gaoxiangluo.github.io/2020/10/01/Math-that-you-need-to-know-for-Deep-Neural-Networks/</id>
    <published>2020-10-01T06:37:58.000Z</published>
    <updated>2020-10-02T06:45:52.880Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 8980) is being offered by <a href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p></blockquote><h1>MATH for DNN</h1><h2 id="Our-notation">Our notation:</h2><p>The notation in machine learning is not standardized, but this course tends to use the most generally-used notation in machine learning.</p><ul><li>scalars: <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, vectors: <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, matrices: <img src="https://math.now.sh?inline=%5Cmathbb%20X" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, tensors: <img src="https://math.now.sh?inline=%5Cmathcal%20X" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, sets: <img src="https://math.now.sh?inline=S" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li>vectors are always <strong>column vectors</strong>, unless stated otherwise</li><li><img src="https://math.now.sh?inline=x_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>-th element of <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=x_%7Bij%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: <img src="https://math.now.sh?inline=%28i%2C%20j%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>-th element of <img src="https://math.now.sh?inline=%5Cmathbb%20X" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=%5Cmathbb%20x%5Ei" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>-th row of <img src="https://math.now.sh?inline=%5Cmathbb%20X" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> as a \textbf{row vector}, <img src="https://math.now.sh?inline=%5Cmathbb%20x_j" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: <img src="https://math.now.sh?inline=j" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>-th column of <img src="https://math.now.sh?inline=%5Cmathbb%20X" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> as a <strong>column vector</strong></li><li><img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: real numbers, <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D_%2B" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: positive reals, <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: space of <img src="https://math.now.sh?inline=n" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>-dimensional vectors, <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: space of <img src="https://math.now.sh?inline=m%20%5Ctimes%20n" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> matrices, <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%20%5Ctimes%20k%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: space of <img src="https://math.now.sh?inline=m%20%5Ctimes%20n%20%5Ctimes%20k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> tensors, etc</li><li><img src="https://math.now.sh?inline=%5Bn%5D%20%5Cdoteq%20%5C%7B1%2C%20%5Cdots%2C%20n%20%5C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul><h2 id="Differentiability-–-first-order">Differentiability – first order</h2><p>The definition of differentiability needs to be different in higher dimensional spaces than what is taught in calculus courses. A function is differentiable in higher dimensional space if, for a small perturbation in the input, the function’s value change is linear with respect to that perturbation with some lower-order term.</p><p>Consider <img src="https://math.now.sh?inline=f%28%5Cmathbb%20x%29%3A%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D%5Em" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p><ul><li>Definition: a function is <strong>first-order differentiable</strong> at a point <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> if there exists a matrix <img src="https://math.now.sh?inline=%5Cmathbb%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> such that</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cfrac%7Bf%28%5Cmathbb%20x%20%2B%20%5Cmathbb%20%5Cdelta%29%20-%20f(%5Cmathbb%20x)%20-%20%5Cmathbb%20B%20%5Cmathbb%20%5Cdelta%7D%7B%5C%7C%5Cmathbb%20%5Cdelta%5C%7C_%7B2%7D%7D%20%5Cto%20%5Cmathbb%200%20%5Cquad%20%5Cmathrm%7Bas%7D%20%5Cquad%20%5Cmathbb%20%5Cdelta%20%5Cto%20%5Cmathbb%200%0A" /></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=i.e.%2C%20%5Cquad%20f%28%5Cmathbb%20x%20%2B%20%5Cmathbb%20%5Cdelta%29%20%3D%20f(%5Cmathbb%20x)%20%2B%20%5Cmathbb%20B%20%5Cmathbb%20%5Cdelta%20%2B%20o(%5C%7C%5Cmathbb%20%5Cdelta%5C%7C_%7B2%7D)%20%5Cquad%20%5Cmathrm%7Bas%7D%20%5Cquad%20%5Cmathbb%20%5Cdelta%20%5Cto%20%5Cmathbb%200%0A" /></p><ul><li><img src="https://math.now.sh?inline=%5Cmathbb%20B" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is called the (Frechet) derivative.<br>When <img src="https://math.now.sh?inline=m%3D1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=%5Cmathbb%20b%5E%5Cintercal" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (i.e., <img src="https://math.now.sh?inline=%5Cmathbb%20B%5E%5Cintercal" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) called <strong>gradient</strong>, denoted as <img src="https://math.now.sh?inline=%5Cnabla%20f%28%5Cmathbb%20x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. For general <img src="https://math.now.sh?inline=m" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, also called <strong>Jacobian</strong> matrix, denoted as <img src="https://math.now.sh?inline=%5Cmathbb%20J_f%28%5Cmathbb%20x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</li><li>Calculation:  <img src="https://math.now.sh?inline=b_%7Bij%7D%20%3D%20%20%5Cfrac%7B%5Cpartial%20f_i%7D%7B%5Cpartial%20x_j%7D%28%5Cmathbb%20x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li><strong>Sufficient (but not necessary) condition for differentiability</strong>: if all partial derivatives exist and are <strong>continuous</strong> at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, then <img src="https://math.now.sh?inline=f%28%5Cmathbb%20x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</li></ul><h2 id="Calculus-rules">Calculus rules</h2><p>Many of the rules are similar to the lower-dimensional analogue. However, one rule to pay attention to is the Chain rule. Discussion of this will come after the definition of these rules.</p><p>Assume <img src="https://math.now.sh?inline=f%2C%20g%3A%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D%5Em" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> are differentiable at a point <img src="https://math.now.sh?inline=%5Cmathbb%20x%20%5Cin%20%5Cmathbb%7BR%7D%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</p><ul><li><p><strong>linearity</strong>: <img src="https://math.now.sh?inline=%5Clambda_1%20f%20%2B%20%5Clambda_2%20g" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=%5Cnabla%5C%7B%5Clambda_1%20f%20%2B%20%5Clambda_2%20g%5C%7D%28%5Cmathbb%20x%29%3D%20%5Clambda_1%20%5Cnabla%20f(%5Cmathbb%20x)%20%2B%20%5Clambda_2%20%5Cnabla%20g(%5Cmathbb%20x)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p></li><li><p><strong>product</strong>: assume <img src="https://math.now.sh?inline=m%3D1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=fg" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=%5Cnabla%20%5C%7Bfg%5C%7D%20%28%5Cmathbb%20x%29%20%3D%20f(%5Cmathbb%20x)%20%5Cnabla%20g(%5Cmathbb%20x)%20%2B%20%20g(%5Cmathbb%20x)%20%5Cnabla%20f(%5Cmathbb%20x)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p></li><li><p><strong>quotient</strong>: assume <img src="https://math.now.sh?inline=m%3D1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=g%28%5Cmathbb%20x%29%20%5Cne%200" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, <img src="https://math.now.sh?inline=%5Cfrac%7Bf%7D%7Bg%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cnabla%20%5C%7B%5Cfrac%7Bf%7D%7Bg%7D%5C%7D%28%5Cmathbb%20x%29)%20%3D%20%5Cfrac%7Bg(%5Cmathbb%20x)%20%5Cnabla%20f(%5Cmathbb%20x)%20-%20f(%5Cmathbb%20x)%20%5Cnabla%20g(%5Cmathbb%20x)%7D%7Bg%5E2%20(%5Cmathbb%20x)%7D%0A" /></p></li><li><p><strong>Chain rule</strong>: Let <img src="https://math.now.sh?inline=f%3A%20%5Cmathbb%7BR%7D%5Em%20%5Cto%20%5Cmathbb%7BR%7D%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=h%3A%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D%5Ek" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=%5Cmathbb%20y%20%3D%20f%28%5Cmathbb%20x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=h" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20y" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Then, <img src="https://math.now.sh?inline=h%5Ccirc%20f%3A%20%5Cmathbb%7BR%7D%5En%20%5Cto%20%5Cmathbb%7BR%7D%5Ek" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is differentiable at <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and</p></li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cmathbb%20J_%7B%5C%7Bh%20%5Ccirc%20f%5C%7D%7D%20%28%5Cmathbb%20x%29%20%3D%20%5Cmathbb%20J_%7Bh%7D%20(f(%5Cmathbb%20x))%20%5Cmathbb%20J_f(%5Cmathbb%20x).%0A" /></p><p>When <img src="https://math.now.sh?inline=k%3D1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>,</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cnabla%20%5C%7Bh%20%5Ccirc%20f%5C%7D%20%28%5Cmathbb%20x%29%20%3D%20%5Cmathbb%20J%5E%5Ctop_f(%5Cmathbb%20x)%20%5Cnabla%20h(f(%5Cmathbb%20x))%0A" /></p><p>The thing to note with the chain rule is that when you take the Jacobian of the composition of two matrices, you need to multiply the Jacobian matrices of each. However, when you take the gradient of the composition of two matrices, you need to reverse the order and appply the proper transpose. This is because the gradient is already the transposed form of the first derivative.</p><p><strong>First-order differentiable</strong> at a point <img src="https://math.now.sh?inline=%5Cmathbb%20x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> if there exists a matrix <img src="https://math.now.sh?inline=%5Cmathbb%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> such that</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=i.e.%2C%20%5Cquad%20f%28%5Cmathbb%20x%20%2B%20%5Cmathbb%20%5Cdelta%29%20%3D%20f(%5Cmathbb%20x)%20%2B%20%5Cmathbb%20B%20%5Cmathbb%20%5Cdelta%20%2B%20o(%5C%7C%5Cmathbb%20%5Cdelta%5C%7C_%7B2%7D)%20%5Cquad%20%5Cmathrm%7Bas%7D%20%5Cquad%20%5Cmathbb%20%5Cdelta%20%5Cto%20%5Cmathbb%200%0A" /></p><ul><li>to prove the chain rule for <img src="https://math.now.sh?inline=h%20%5Ccirc%20f%28x%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br><img src="figure1.png" alt="figure1"></li></ul><h2 id="Differentiability-–-second-order">Differentiability – second order</h2><p>Consider <img src="https://math.now.sh?inline=f%28x%29%20%3A%20%5Cmathbb%7BR%7D%5E2%20%5Crightarrow%20%5Cmathbb%7BR%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and assure <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is 1st-order differentiable in a small ball around around <img src="https://math.now.sh?inline=x" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br><img src="figure2.png" alt="figure2"></p><h2 id="Taylor’s-theorem">Taylor’s theorem</h2><p>Vector version: consider <img src="https://math.now.sh?inline=f%28x%29%20%3A%20%5Cmathbb%7BR%7D%5E2%20%5Crightarrow%20%5Cmathbb%7BR%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br><img src="figure3.png" alt="figure3"><br>Gradient always the same form as the variable. In matrix version, we replace L2 norm with frobenius form, which is a generlization of L2 norm in matrix space. The difference of second order and first order is that second-order has a extra Hessian.</p><p><img src="figure4.png" alt="figure4"></p><p>before: <strong>gradient, Hessian</strong> <img src="https://math.now.sh?inline=%5Crightarrow" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> <strong>Taylor Espansion</strong><br>now: <strong>Taylor Espansion</strong> <img src="https://math.now.sh?inline=%5Crightarrow" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> <strong>gradient, Hessian</strong></p><h2 id="Taylor-approximation-–-asymptotic-uniqueness">Taylor approximation – asymptotic uniqueness</h2><p><img src="figure5.png" alt="figure5"></p><h2 id="Directional-derivatives-and-curvatures">Directional derivatives and curvatures</h2><p>Consider <img src="https://math.now.sh?inline=f%28x%29%20%3A%20%5Cmathbb%7BR%7D%5En%20%5Crightarrow%20R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p><ul><li>directional derivative: <img src="https://math.now.sh?inline=D_vf%28x%29%3D%5Cfrac%7Bd%7D%7Bdt%7Df(x%2Btv)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li>When f is 1-st order differentiable at x</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=D_vf%28x%29%20%3D%20%5Clangle%20%5Cnabla%20f(x)%2C%20v%20%5Crangle%0A" /></p><ul><li>Now <img src="https://math.now.sh?inline=D_vf%28x%29%20%3A%20%5Cmathbb%7BR%7D%5En%20%5Crightarrow%20%5Cmathbb%7BR%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, what is <img src="https://math.now.sh?inline=D_u%28D_vf%29(x)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>?</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=D_u%28D_vf%29(x)%3D%5Clangle%20u%2C%20%5Cnabla%20%5E2%20f(x)%20v%20%5Crangle%0A" /></p><ul><li>When <img src="https://math.now.sh?inline=u%3Dv" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>,</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=D_u%28D_uf%29(x)%3D%5Clangle%20u%2C%20%5Cnabla%20%5E2%20f(x)%20u%20%5Crangle%20%3D%20%5Cfrac%7Bd%5E2%7D%7Bdt%5E2%7Df(x%2Btu)%0A" /></p><p><img src="figure6.png" alt="figure6"></p><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 8980) is being offered by &lt;a href=&quot;https://sunju.org/&quot;&gt;Prof. Ju Sun&lt;/a&gt; at the University</summary>
      
    
    
    
    
    <category term="Machine Learning" scheme="https://gaoxiangluo.github.io/tags/Machine-Learning/"/>
    
    <category term="Deep Learning" scheme="https://gaoxiangluo.github.io/tags/Deep-Learning/"/>
    
    <category term="Taylor&#39;s Theorem" scheme="https://gaoxiangluo.github.io/tags/Taylor-s-Theorem/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining: Bayesian Classifiers</title>
    <link href="https://gaoxiangluo.github.io/2020/09/29/Data-Mining-Bayesian-Classifiers/"/>
    <id>https://gaoxiangluo.github.io/2020/09/29/Data-Mining-Bayesian-Classifiers/</id>
    <published>2020-09-29T14:26:59.000Z</published>
    <updated>2020-10-02T06:39:55.862Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 5523) is being offered by <a href="https://www-users.cs.umn.edu/~kumar001/">Prof. Vipin Kumar</a> at the University of Minnesota in Fall 2020.</p></blockquote><h1>Bayesian Classifiers</h1><p>Let’s recall what bayes theorem is from Intro to Stat.</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=P%28Y%20%7C%20X%29%20%3D%20%5Cfrac%7BP(X%20%7C%20Y)P(Y)%7D%7BP(X)%7D%0A" /></p><h2 id="Using-bayes-theorem-for-classification">Using bayes theorem for classification</h2><p><img src="figure1.png" alt="figure1"></p><p>If each attribute <img src="https://math.now.sh?inline=X_d" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is independent while being given a class attribute <img src="https://math.now.sh?inline=Y_j" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, we can have:</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=P%28X_1%2CX_2%2C...%2CX_d%7CY_j%29%3DP(X_1%7CY_j)P(X_2%7CY_j)...P(X_d%7CY_j)%0A" /></p><p>For instance:<br><img src="figure2.png" alt="figure2"></p><h3 id="Estimate-probabilities-from-data">Estimate probabilities from data</h3><p><img src="figure4.png" alt="figure4"><br>While considering binary values are easy by counting, let’s look at what should we do with continues data.<br><img src="figure3.png" alt="figure3"></p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=P%28X_i%7CY_j%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%20%5Cpi%20%5Csigma_%7Bij%7D%5E2%7D%7De%5E-%5Cfrac%7B(X_i-%20%5Cmu_%7Bij%7D)%5E2%7D%7B2%20%5Csigma_%7Bij%7D%5E2%7D%0A" /></p><p>Still the sample dataset above, let’s apply this naive bayer classifier.</p><ul><li>given a test data point<br><img src="figure5.png" alt="figure5"></li><li>follow a decision tree<br><img src="figure6.png" alt="figure6"></li></ul><h3 id="Issue-with-naive-bayes-classifier">Issue with naive bayes classifier</h3><ol><li>If the test data point has missing attribute, we might have the possibility to identify one class to be 0.</li><li>If there is a missing data, we might not be able to classify as following:<br><img src="figure7.png" alt="figure7"></li></ol><p>Therefore, we want to use other estimates of conditional probabilities than simple fraction.</p><p>Original: <img src="https://math.now.sh?inline=P%28X_i%20%3D%20c%20%7C%20y%29%20%3D%20%5Cfrac%7Bn_c%7D%7Bn%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p><ul><li><img src="https://math.now.sh?inline=n" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the number of training instances belonging to class y</li><li><img src="https://math.now.sh?inline=n_c" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the number of instances with <img src="https://math.now.sh?inline=X_i%20%3D%20c" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and <img src="https://math.now.sh?inline=Y%20%3D%20y" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/><br>Laplace Estimate: <img src="https://math.now.sh?inline=P%28X_i%20%3D%20c%20%7C%20y%29%20%3D%20%5Cfrac%7Bn_c%20%2B%201%7D%7Bn%20%2B%20v%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li><img src="https://math.now.sh?inline=v" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the total number of attribute values that <img src="https://math.now.sh?inline=X_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> can take<br>m - estimate: <img src="https://math.now.sh?inline=P%28X_i%20%3D%20c%20%7C%20y%29%20%3D%20%5Cfrac%7Bn_c%20%2B%20mp%7D%7Bn%20%2B%20m%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li><li><img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the initial estimate of <img src="https://math.now.sh?inline=P%28X_i%20%3D%20c%20%7C%20y%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> known apriori</li><li><img src="https://math.now.sh?inline=m" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the hyper-parameter for our confidence in <img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li></ul><h2 id="Naive-Bayes-Summary">Naive Bayes (Summary)</h2><ul><li>Robust to isolated noise points</li><li>Handle missing values by ignoring the instance during probability estimate calculations</li><li>Robust to irrelevant attributes (because irrelevant attributes will have the similar weights to classified label e.g: 50%, 50%)</li><li><strong>Drawback</strong>: redundant and correlated attributes will violate class condition assumption<ul><li>Use other techniques such as Bayesian Belief Networks (BBN)</li></ul></li></ul><p>A intuitive bad example:<br><img src="figure8.png" alt="figure8"><br>No, we cannot use naive bayes because conditional independence of attributes is violated. For instance, if <img src="https://math.now.sh?inline=x%3D5" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, then <img src="https://math.now.sh?inline=y" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> can be either red or blue.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 5523) is being offered by &lt;a href=&quot;https://www-users.cs.umn.edu/~kumar001/&quot;&gt;Prof. Vipin K</summary>
      
    
    
    
    
    <category term="Data Mining" scheme="https://gaoxiangluo.github.io/tags/Data-Mining/"/>
    
    <category term="Bayesian Classifier" scheme="https://gaoxiangluo.github.io/tags/Bayesian-Classifier/"/>
    
  </entry>
  
  <entry>
    <title>Visual and Rigorous Proof of Universal Approximation Theorem (UAT)</title>
    <link href="https://gaoxiangluo.github.io/2020/09/27/Visual-and-Rigorous-Proof-of-Universal-Approximation-Theorem-UAT/"/>
    <id>https://gaoxiangluo.github.io/2020/09/27/Visual-and-Rigorous-Proof-of-Universal-Approximation-Theorem-UAT/</id>
    <published>2020-09-28T02:16:59.000Z</published>
    <updated>2020-10-02T06:05:58.040Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 8980) is being offered by <a href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p></blockquote><blockquote><p>Gaoxiang: This lecture is scribed by myself and Andrew Walker.</p></blockquote><h1>Why should we trust Neural Networks (NNs)?</h1><p>We will start by looking at the supervised learning. Although today’s NNs are not only for the supervised learning, we will use this embedded illustration from machine learning to give you some ideas.</p><p><img src="general_view_and_nn_view.png" alt="general_view_and_nn_view"><br>As shown above, the only difference between these two views is that we choose a NN architecture instead of selecting a family of functions. The idea behind both views is function approximation, and we want to emphasize more in function approximation to give you a more accurate description of supervised learning.</p><p><img src="figure1.png" alt="figure1"><br>Basically you can think of supervised learning in this way. First of all, we have an underlying true function <img src="https://math.now.sh?inline=f_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and our data can be generated by <img src="https://math.now.sh?inline=f_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> with dense sampling, which are the blue dots in above. Second, we choose a family of functions <img src="https://math.now.sh?inline=H" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. The purpose here is to learn from this family <img src="https://math.now.sh?inline=H" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> to find a function <img src="https://math.now.sh?inline=f%20%5Cin%20H" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> (orange curve) that is close to the ground truth function <img src="https://math.now.sh?inline=f_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. This is different with the views mentioned earlier. Those views are to fit the data, which is more about the training error. But here if you really find the ground truth function by learning, you will do perfectly well on eliminating test error.</p><p>There are two aspects of this more accurate description. Does our family of functions <img src="https://math.now.sh?inline=H" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> really have the power to find <img src="https://math.now.sh?inline=f_0" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>? This is what we called <strong>Approximation Capacity</strong>. Another important aspect to consider is <strong>Optimization &amp; Generalization</strong>, because sometimes even you choose a powerful function class, it does not guarantee that you will find the best <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Since optimization will be covered later in this class, now let us look at the capacity of our family functions.</p><p>Before we look at the capacity of NN, let us clarify some notations.</p><ul><li>k-layer NNs: with k layers of weights (along the deepest path)</li><li>k-hidden-layer NNs: with k hidden layers of nodes</li></ul><p><img src="figure2.png" alt="figure2"><br>Now let us think of a single-output (i.e., <img src="https://math.now.sh?inline=%5Cmathbb%20R%5En%20%5Cmapsto%20%5Cmathbb%20R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) problem, and we can start with one neuron. It’s basically summing up all input with weights, add a threshold/offset, and pass through a non-linear function <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> which is called the activation function <img src="https://math.now.sh?inline=%5Csigma" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. As <img src="https://math.now.sh?inline=%5Cleft%5C%7B%5Cmathbf%20x%20%5Cmapsto%20%5Csigma%28%5Cmathbf%20w%5E%5Cintercal%20%5Cmathbf%20x%20%2B%20b%29%20%5Cright%5C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> shown, the output that a single neuron can represent largely depends on what kind of activation function <img src="https://math.now.sh?inline=%5Csigma" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> we have. If we choose an identity function, it will turn out to be linear, which is not powerful. If we choose a sign function like perceptron, which is a 0/1 function with hyperplane threshold, it has some constrain as well.</p><p><img src="figure3.png" alt="figure3"><br>For instance, if we have 1s on 1-quadrant and 3-quadrant, and 0s on 2-quadrant and 4-quadrant, we cannot draw a line to classify these two classes. Even if we choose sigmoid function <img src="https://math.now.sh?inline=%5Cleft%5C%7B%5Cmathbb%20x%20%5Cmapsto%20%5Cfrac%7B1%7D%7B1%2B%20e%5E%7B-%5Cleft%28%20%5Cmathbb%20w%5E%5Cintercal%20%5Cmathbb%20x%20%2B%20b%20%5Cright%29%7D%7D%20%5Cright%5C%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and ReLU, we still cannot solve the quadrants’ example mentioned earlier, because all of the functions above are monotonic in a certain direction (as shown in above), when the ground truth function, that we try to get close to, is not monotonic.</p><p><img src="figure4.png" alt="figure4"><br>Instead of trying to think of a very complicated <img src="https://math.now.sh?inline=%5Csigma" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, what if we add more layers like <img src="https://math.now.sh?inline=%5Csigma%5Cleft%28%20%5Cmathbb%20w_L%5E%5Cintercal%20%5Cleft(%20%5Cmathbb%20W_%7BL-1%7D%20%20%20%5Cleft(%20%5Cdots%20%20%5Cleft(%20%5Cmathbb%20W_1%20%5Cmathbb%20x%20%2B%20%5Cmathbb%20b_1%20%5Cright%29%20%2B%20%5Cdots%20%5Cright)%20%20%5Cmathbb%20b_%7BL-1%7D%20%20%20%20%5Cright)%20%20%2B%20b_L%20%5Cright)" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>? Adding depth alone is not going to help because it’s multiplying weights repeatedly, and composition of linear operation will still be linear. Therefore, we want to try not only adding layers but also adding nonlinearity into NNs. Surpassingly, this turns out to be really powerful. This leads to the fundamental belief of DNNs – <strong>Universal Approximation Theorem (UAT)</strong>. This theorem states:<br><strong>The 2-layer network can approximate arbitrary continuous functions arbitrarily well, provided that the hidden layer is sufficiently wide.</strong></p><h1>Why should UAT hold?</h1><p><img src="1d.png" alt="1d"><br>The live demo is from <a href="http://neuralnetworksanddeeplearning.com/chap4.html">http://neuralnetworksanddeeplearning.com/chap4.html</a>.<br>Let us start with a single-input-single-output function <img src="https://math.now.sh?inline=%5Cmathbb%20R%20%5Cto%20%5Cmathbb%20R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> using sigmoid <img src="https://math.now.sh?inline=%5Csigma%20%3D%20%5Cfrac%7B1%7D%7B1%2B%20e%5E%7B-%5Cleft%28%20%5Cmathbb%20w%5E%5Cintercal%20%5Cmathbb%20x%20%2B%20b%20%5Cright%29%7D%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, whose graph is typically like (a). If we increase the value of <img src="https://math.now.sh?inline=w" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> significantly, we will get a function which is almost the same as step function, as illustrated in (b). Next, if we change the value of <img src="https://math.now.sh?inline=b" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, what we’re doing is moving the graph horizontally like ©. Now let us consider two neurons summing up in (d), which is similar to adding two step functions together. More interestingly, if we change the weight of <img src="https://math.now.sh?inline=w_2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> to the opposite of <img src="https://math.now.sh?inline=w_1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, we will get a bump function in (e). Once we take more neurons into account, we will end up with lots of consecutive bumps in (f).</p><p><img src="figure5.png" alt="figure5"><br>This is really similar to Riemann Sum while defining an integral, where you can draw a smooth curve passing through upper bound’s midpoint of the bump (Midpoint Rule) or the top-left corner of the bump (Trapezoidal Rule). Therefore, this 1-hidden layer NN is powerful enough to approximate many non-linear functions if we want to expend the number of neurons as many as we need. Now the question is how about high-dimensional?</p><p><img src="2d.png" alt="2d"><br>The idea is similar in high dimensions. This is a two-inputs-one-output function <img src="https://math.now.sh?inline=%5Cmathbb%20R%5E2%20%5Cto%20%5Cmathbb%20R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> using sigmoid. First of all, we turn off the weight in one direction, and the graph just looks like sigmoid in (1). If we again increase the weight sharply, we will get a step function in (2). Now let us turn on weight for another direction, and repeat the previous steps. What we get is is a bump function (two step functions in two orthogonal directions) in (3). Next, if we add more neurons to each direction, then there will be summing up of step functions in each direction, which will generally lead to summing up of bump functions in (4). We observe that if the height of a bump is <img src="https://math.now.sh?inline=h" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, then in (4) the highest bump is in height <img src="https://math.now.sh?inline=2h" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Having comparing to the 1D case mentioned earlier, we want to get only the highest bump, which we call a tower in (5), by applying some offsets/cut-off positions. At this point, if we double the neurons in the hidden layer corresponding to each input, we’re able to get two towers. I believe you will get the idea at this point. More neurons in the hidden layers, more towers we can construct.</p><p><img src="figure67.png" alt="figure67"><br>As shown in the left above, I believe you’re convinced that with 1 hidden layer, we can construct many towers (in this case, square towers as inputs are 2D), to approximate any 2D functions arbitrarily well. This is also what we called <strong>Shallow Network Networks</strong>, which is NN with 1 hidden layer. The example above in based on 2D input <img src="https://math.now.sh?inline=%5Cmathbb%20R%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, what if we want to have high-dimensional input <img src="https://math.now.sh?inline=%5Cmathbb%20R%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. Then we don’t have to be as rigid as we were in only x and y directions to construct many square towers. As the input space increases, we could have had more cuts on our square towers then it will be closer and closer to many circle towers. At this point, you might ask what if the output space is also high-dimensional such as <img src="https://math.now.sh?inline=%5Cmathbb%20R%5En%20%5Cto%20%5Cmathbb%20R%5Em" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> functions. The answer is we can approximate each <img src="https://math.now.sh?inline=%5Cmathbb%20R%5En%20%5Cto%20%5Cmathbb%20R" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> separately and then glue them together. I believe you’re convinced that a shallow NN or a 2-layer NN is already powerful enough, but one constrain obviously is that we need lots of neurons if we only have 1 hidden layer. Later we will introduce Deep Neural Networks (DNNs) and why we want to add one more hidden layer.</p><h1>UAT in rigorous form</h1><p>The Universal Approximation Theorem is a fundamental theorem underpinning the power behind the neural network’s prediction ability. The first formulation of the UAT (that has now developed into many variations) was developed in the late 1980’s. This section includes the mathematical discussion of the UAT.</p><p><img src="theorem01.png" alt="theorem01"><br>Breaking this theorem down, we see <img src="https://math.now.sh?inline=%5Csigma" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is an activation function. The variable <img src="https://math.now.sh?inline=%5Cvarepsilon" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the arbitrary level of precision we would like to reach between our target function <img src="https://math.now.sh?inline=f" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> and our neural network function <img src="https://math.now.sh?inline=F" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. This theorem states that within the constraints, a single layer neural network with a large enough number of nodes can fit with arbitrarily small error any target function. The visual proof in the last section was intuitive, but the rigorous proof requires functional analysis and quadratic theory. We will not cover the rigorous proof in this course.</p><p>Notice the key limitations in the original formulation of the UAT — the target function must lie on the hypercube, the activation function must be non-constant, bounded, and continuous, the target function must be continuous. These statements have been somewhat addressed since the original formulation. The target function may lie on a space other than the hypercube; it just has to be compact (bounded and closed). A commonly-used activation function (ReLU) is not bounded, although a composition of ReLUs is. The target function needs to be continuous, although the classification problem is by definition discontinuous. However, the discontinuous classification can be approximated arbitrarily well by a very steep sigmoid function, for example.</p><p><img src="diff.png" alt="diff"><br>In fact, the UAT has been found to apply for <img src="https://math.now.sh?inline=%5Csigma" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> as general as the set of all non-polynomial functions. [LLPS93]</p><h1>From shallow to deep Neural Networks</h1><p>The UAT says that a shallow, single-layer neural network can hit arbitrarily accurate levels of prediction. So why do we use multi-layer (or deep) neural networks? The UAT says the number of nodes is an integer N, but places no upper limit on N.</p><p>The value of N blows up very quickly for higher-dimensionality datasets. To show this, we return to our visual proof terminology. Assume our target function is 1-Lipschitz (essentially, it does not ever have a slope greater than 1 or -1. Formally: <img src="https://math.now.sh?inline=%5Cleft%7C%20f%28x%29%20-%20f(y)%20%5Cright%7C%20%5Cle%20%5Cleft%7C%20x%20-%20y%20%5Cright%7C%2C%20%5Cforall%5C%3B%20x%2C%20y%20%5Cin%20%5Cmathbb%7BR%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) and has a domain in <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D%5E1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. With this in mind, to achieve <img src="https://math.now.sh?inline=%5Cvarepsilon" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> accuracy, we need at most  <img src="https://math.now.sh?inline=%5Cfrac%7B1%7D%7B%5Cvarepsilon%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> bumps per unit length along the domain. For a 2D target function, we need <img src="https://math.now.sh?inline=%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-2%7D%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>. In fact, in general, for a target function with domain n-Dimensions, we need <img src="https://math.now.sh?inline=%5Cmathcal%7BO%7D%28%5Cvarepsilon%5E%7B-n%7D%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> bumps.</p><p>As you can see, this exponential growth makes single layer neural networks infeasible for higher dimension problems. The value of deep NNs is that they can provide much better computing power. In 2017, it was shown that DNNs can have the number of nodes <img src="https://math.now.sh?inline=%5Cmathcal%7BO%7D%28n%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> while 2-layer NNs need <img src="https://math.now.sh?inline=%5Cmathcal%7BO%7D%28a%5En%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> for Boolean functions in domain <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</p><p>To show this, we define two classes of functions —<br><img src="two_class_fun.png" alt="two_class_fun"></p><p>The 2017 theorems:<br><img src="thm1.png" alt="thm1"><br><img src="thm2.png" alt="thm2"><br>These theorems essentially show (within the constraints) that shallow networks are exponential with respect to domain dimension and that deep networks are linear with respect to domain dimension.</p><p>Since we have covered so many variations on the UAT, you may be wondering, what is the most general variation of the UAT so far?<br><img src="pro2.png" alt="pro2"></p><p>This activation function must be continuous and not a polynomial. Then, given a target continuous function, you can find a shallow neural network that approximates the target function arbitrarily well.</p><p>However, deeper is not always better. Theorems [LPW+17][KL19]  have shown that to maintain the UAT property of arbitrary approximation power, deep networks need around <img src="https://math.now.sh?inline=n" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> nodes per hidden layer in <img src="https://math.now.sh?inline=%5Cmathbb%7BR%7D%5En" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</p><p><img src="thm13.png" alt="thm13"><br>However, this is still an active area of research and in practice, the most optimal network is often forgone for a less theoretically optimal but still practically better option.</p><h1>Conclusion</h1><p>While designing your network, do not try to use these theorems and mathematical results to advise your design. Deep Learning is a very new and volatile field and it is often the case that researchers develop new architectures without knowing why they work, and later on mathematicians try to prove why a certain architecture is optimal.<br>The reason we covered the UAT is so we get an understanding of why we can just take a neural network to approximate some function. If we have no other understanding of the target function, maybe a neural network would be a good choice.</p><h1>Reference</h1><p><img src="reference.png" alt="reference"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 8980) is being offered by &lt;a href=&quot;https://sunju.org/&quot;&gt;Prof. Ju Sun&lt;/a&gt; at the University</summary>
      
    
    
    
    
    <category term="Study Note" scheme="https://gaoxiangluo.github.io/tags/Study-Note/"/>
    
    <category term="Deep Learning" scheme="https://gaoxiangluo.github.io/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://gaoxiangluo.github.io/tags/Neural-Network/"/>
    
    <category term="Theory" scheme="https://gaoxiangluo.github.io/tags/Theory/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining: Classification with Decision Tree</title>
    <link href="https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/"/>
    <id>https://gaoxiangluo.github.io/2020/09/25/Data-Mining-Classification-with-Decision-Tree/</id>
    <published>2020-09-25T16:16:41.000Z</published>
    <updated>2020-10-05T20:38:00.365Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 5523) is being offered by <a href="https://www-users.cs.umn.edu/~kumar001/">Prof. Vipin Kumar</a> at the University of Minnesota in Fall 2020.</p></blockquote><p>Base Classifiers</p><ul><li>Decision Tree based Methods</li><li>Rule-based Methods</li><li>Nearest-neighbor</li><li>Neural Networks, Deep Neural Nets</li><li>Naive Bayes and Bayesian Belief Networks</li><li>Support Vector Machines</li></ul><p>Ensemble Classifiers</p><ul><li>Boosting, Bagging, Random Forests</li></ul><h1>Decision Tree</h1><h2 id="Decision-Tree-Induction-Algorithms">Decision Tree Induction Algorithms</h2><ul><li>Hunt’s Algorithm (one of the earliest)</li><li>CART</li><li>ID3, C4.5</li><li>SLIQ,SPRINT</li></ul><h2 id="Hunt’s-Algorithm">Hunt’s Algorithm</h2><p><img src="figure1a.png" alt="figure1a"><br>As you may see in the picture, a Hunt’s Algorithm in decision tree is to define a condition to split data into two or more branches when a node is not pure (involving both labeled classes), until the leaf node has all the data with the same class. Now with this model, we might ask how do we determine the best spiltting. Therefore, some evaluation metrics may come into place.</p><h2 id="Measures-of-Node-Impurity">Measures of Node Impurity</h2><p>First, we have to know what is node impurity. If a node has 10 data points in total, and 5 of them are labeled class 1 and 5 of them are labeled class 0, then it has a high degree of impurity. But if a node has 9 data points labeled class 1 and only 1 data point labeled class 0, then we say it has a low degree of impurity. People have designed methods to measure node impurity. There are three most popular ones:</p><ul><li>Gini Index</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gini%20%5C%2C%20Index%3D1-%20%5Csum_%7Bi%3D0%7D%5E%7Bc-1%7Dp_i%28t%29%5E2%0A" /></p><p>where <img src="https://math.now.sh?inline=p%5Ei%28t%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the frequency of class <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> at node <img src="https://math.now.sh?inline=t" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>, and <img src="https://math.now.sh?inline=c" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the total number of classes</p><pre><code>- It has a minimum of 0 when all records belong to one class, which is the most beneficial situation for classification.- It has a maximum of $1 - \frac&#123;1&#125;&#123;c&#125;$ when records are equally distributed among all classes, which is the least beneficial situation.- It's used in decision tree algorithems such as CART, SLIQ, SPRINT </code></pre><ul><li>Entropy</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Entropy%3D%20-%20%5Csum_%7Bi%3D0%7D%5E%7Bc-1%7Dp_i%28t%29log_2p_i(t)%0A" /></p><pre><code>- It has a minimum of 0 and a maximum of $log_2c$.- Entropy based computations are quite similiar to the GINI index computations.</code></pre><ul><li>Misclassification Error</li></ul><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Classification%20%5C%2C%20Error%20%3D%201%20-%20max%5Bp_i%28t%29%5D%0A" /></p><h3 id="How-to-find-the-best-split">How to find the best split?</h3><ol><li>Compute Impurity Measure (<img src="https://math.now.sh?inline=P" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) before splitting</li><li>Compute Impurity Measure (<img src="https://math.now.sh?inline=M" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>) after splitting</li><li>Choose the attribute test condition that produces the highest gain</li></ol><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gain%20%3D%20P%20-%20M%0A" /></p><p>Particularlly, when we say information gain, it implies the Gain of Entropy method.</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Information%20%5C%2C%20Gain%20%3D%20Entropy%28parent%29%20-%20%5Csum_%7Bi%3D1%7Dk%20%5Cfrac%7Bn_i%7D%7Bn%7DEntropy(i)%0A" /></p><p>where Parent Node <img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is split into <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> partitions (children); <img src="https://math.now.sh?inline=n_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is number of records in child node <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>.</p><p>Let’s look at a simple example.<br><img src="figure2a.png" alt="figure2a"></p><p>I think this image is straighforward. At this point we may naturally think of the more partitions we have, the more likely we’ll end up with a lower degree of impurity, but does it mean the more partitions the better? No, it’s not always meaningful to do so. For instance, if we classify each customers by ID, it doesn’t do anything or give us any useful information, so we want to have a measure that will penalize too many successors.</p><h4 id="Gain-Ratio-for-Entropy">Gain Ratio (for Entropy)</h4><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=Gain%20%5C%2C%20Ratio%20%3D%20%5Cfrac%7BGain_split%7D%7BSplit%20%5C%2C%20Info%7D%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%20Split%20%5C%2C%20Info%20%3D%20-%5Csum_%7Bi%3D1%7D%5Ek%20%5Cfrac%7Bn_i%7D%7Bn%7Dlog_2%5Cfrac%7Bn_i%7D%7Bn%7D%0A" /></p><p>Where parent node <img src="https://math.now.sh?inline=p" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is split into <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> partitions, and <img src="https://math.now.sh?inline=n_i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> is the number of records in child node <img src="https://math.now.sh?inline=i" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p><ul><li>Split Info happends to be larget if too many successors, this is a way to prefer small number of successors.</li><li>This is degisned to overcome the disadvantage of information gain</li><li>Used in C4.5 algorithm</li></ul><p><img src="figure3a.png" alt="figure3a"><br>One of the senarios of applying split info may be the image above. We see that the left most split and the right most split has a similiar value of Gini Index, but the right most one has a much lower split info so that we would go with the lower successors split.</p><h2 id="The-Pros-and-Cons-of-Decision-Tree-Based-Classification">The Pros and Cons of Decision Tree Based Classification</h2><p>Advantages:</p><ul><li>Relativelty inexpensive to construct</li><li>Extremely fast at classifying unkown records</li><li>Easy to interpret for small-sized trees</li><li>Robust to noise (especially when methods to avoid overfitting are employed)</li><li>Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)</li></ul><p>Disadvantages:</p><ul><li>Due to the greedy nature of splitting criterion, interacting attributes (that can distinguish between classes together but not individually) may be passed over in favor of other attribtues that are less discriminating</li><li>Each decision boundary involves only a single attribute</li></ul><h2 id="Classification-Errors">Classification Errors</h2><ul><li>Training errors</li><li>Test errors</li><li>Generalization errors: expected error of a model over random selection of records from same distribution</li></ul><p><img src="figure1.png" alt="figure1"></p><p>Let’s look at this example above. The positive class is from a gaussian centered at (10,10) with 400 noisy instances added.</p><p><img src="figure2.png" alt="figure2"></p><p>If we only use 10% of dataset to train a decision tree, it will end up with a tree with 4 nodes as the picture above. While we increase the size of training dataset, the training error continues to goes down slowly, and we might have a decision tree with more nodes. But doesn’t it really improve our model’s performance?</p><p><img src="figure3.png" alt="figure3"></p><p>Actually not! As the model becomes more and more complex with larger training set, the test error starts to go up. This is what we called <strong>overfitting</strong>.</p><ul><li>Underfitting: when model is too simple, both training and test errors are large.</li><li>Overfitting: when model is too complex, training error is small but test error is large.</li></ul><p><img src="figure4.png" alt="figure4"></p><p>What if we increase the size of the model while we have a overfitting model? The anwser is that both the training errors and test error would go down as the image above.</p><p>At this point, let’s summurize the possible reasons for model overfitting.</p><ul><li>Limited training size</li><li>High model complexity: have something to do with multiple comparision procedure</li></ul><h3 id="Notes-on-overfitting">Notes on overfitting</h3><ul><li>Overfitting results in decision trees that are more complex than necessary</li><li>Training errors does not provide a good estimate of how well the tree will perform on previously unseen records</li><li>Need ways for estimating generalization errors</li></ul><h2 id="Model-Selection">Model Selection</h2><p>Purpose: to ensure that model is not overly complex (to avoid overfitting)<br>Goal: to estimate generalization error</p><ul><li>Using validation set</li><li>Incorporating model complexity</li></ul><h3 id="Using-validation-set">Using validation set</h3><ul><li>Diving training data into two parts<ul><li>Training set: use for model building</li><li>Validation set: use for estimating generalization error</li></ul></li><li>Drawback:<ul><li>Less data available for training</li></ul></li></ul><h3 id="Incorporating-model-complexity">Incorporating model complexity</h3><p>Purpose: use some model complexities while building a model with training set.<br>This is to 1) reduce training set error and 2) add penalty to increasing complexity simultaneously.</p><p>Gen.Error(Model) = Train.Error(Model,Train.Data) + <img src="https://math.now.sh?inline=%5Calpha" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> x Complexity(Model)</p><h3 id="Estimating-the-Complexity-of-Decision-Trees">Estimating the Complexity of Decision Trees</h3><p><strong>Pessimistic Error Estimate</strong> of decision tree <img src="https://math.now.sh?inline=T" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> with <img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> leaf nodes.</p><p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=err_%7Bgen%7D%28T%29%20%3D%20err(T)%20%2B%20%5COmega%20%5Ctimes%20%5Cfrac%7Bk%7D%7BN_%7Btrain%7D%7D%0A" /></p><ul><li><img src="https://math.now.sh?inline=err%28T%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: error rate on all training records (number of error divived by the total training examples)</li><li><img src="https://math.now.sh?inline=%5COmega" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: trade-off hyper-parameter (similar to <img src="https://math.now.sh?inline=%5Calpha" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>)<ul><li>relative cost of adding a leaf node</li></ul></li><li><img src="https://math.now.sh?inline=k" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: number of leaf nodes</li><li><img src="https://math.now.sh?inline=N_%7Btrain%7D" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>: total number of training records</li></ul><p>Let’s look at an example:<br><img src="figure5.png" alt="figure5"><br>How to calculate <img src="https://math.now.sh?inline=e%28T_L%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>?<br>For example, if a leaf node has 3 positive and 1 negative, then the negative case is an error case. Applying the same through to all the leaf nodes will give you the total number of error cases. The other parts of this formula is straightforward in the image above.</p><h2 id="Model-selection-for-decision-trees">Model selection for decision trees</h2><h3 id="Pre-Pruning-Early-Stopping-Rule">Pre-Pruning (Early Stopping Rule)</h3><ul><li>Stop the algorithm before it becomes a fully-grown tree</li><li>Typical stopping conditions for a node:<ul><li>stop if all instances belong to the same class</li><li>stop if all the attribute values are the same</li></ul></li><li>More restrictive conditions:<ul><li>stop if number of instances is less than some user-specified threshold</li><li>stop if class distribution of instances are independent of the available features (e.g., using <img src="https://math.now.sh?inline=x%5E2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> test)</li><li>stop if expanding the current node does not improve impurity measures (e.g., Gini or information gain)</li><li>stop if estimated generalization error falls below certain threshold</li></ul></li></ul><h3 id="Post-pruning">Post-pruning</h3><ul><li>Grow decision tree to its entirety</li><li>Subtree replacement<ul><li>Trim the nodes of the decision tree in a bottom-up fashion</li><li>If generalization error improves after trimming, replace sub-tree by a leaf node</li><li>Class label of leaf node is determined from majority class of instances in the sub-tree.</li></ul></li></ul><h2 id="Model-Evaluation">Model Evaluation</h2><p>Purpose:</p><ul><li>To estimate performance of classifier on previously unseen data (test set)</li></ul><p>Holdout</p><ul><li>Reserve k% for training and (100-k)% for testing</li><li>Random subsampling: repeated holdout</li></ul><p>Cross validation</p><ul><li>Partition data into k disjoint subsets</li><li>k-fold: train on k-1 partitions, test on the remaining one</li><li>Leave-one-out: k = n</li></ul><p>If I only have a small number of dataset, and use too much for test set, then I don’t have enough data for training. I don’t want my model with biases toward how I train my model. What should I do?</p><h3 id="Cross-validation-Example">Cross-validation Example</h3><p><img src="figure6.png" alt="figure6"></p><ul><li>Repeated cross-validation<ul><li>perform cross-validation a number of times</li><li>gives an estimate of the variance of the generalization error</li></ul></li><li>Stratified cross-validation<ul><li>Guarantee the same percentage of class labels in training and test</li><li>Important when classes are imbalanced and the sample is small</li></ul></li><li>Use nested cross-validation approach for model selection and evaluation</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 5523) is being offered by &lt;a href=&quot;https://www-users.cs.umn.edu/~kumar001/&quot;&gt;Prof. Vipin K</summary>
      
    
    
    
    
    <category term="Data Mining" scheme="https://gaoxiangluo.github.io/tags/Data-Mining/"/>
    
    <category term="Classifier" scheme="https://gaoxiangluo.github.io/tags/Classifier/"/>
    
    <category term="Decision Tree" scheme="https://gaoxiangluo.github.io/tags/Decision-Tree/"/>
    
    <category term="Gini Index" scheme="https://gaoxiangluo.github.io/tags/Gini-Index/"/>
    
    <category term="Entropy" scheme="https://gaoxiangluo.github.io/tags/Entropy/"/>
    
  </entry>
  
  <entry>
    <title>Deep Neural Networks(DNNs) Overview</title>
    <link href="https://gaoxiangluo.github.io/2020/09/20/Deep-Neural-Networks-DNNs-Overview/"/>
    <id>https://gaoxiangluo.github.io/2020/09/20/Deep-Neural-Networks-DNNs-Overview/</id>
    <published>2020-09-21T03:23:39.000Z</published>
    <updated>2020-10-02T06:13:20.539Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course (CSCI 8980) is being offered by <a href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p></blockquote><h1>Why Deep Learning</h1><h2 id="What-is-Deep-Learning-DL">What is Deep Learning (DL)?</h2><h3 id="DL-is-about…">DL is about…</h3><ul><li>Deep Neural Network (DNNs)</li><li><strong>Data</strong> or training DNNs (e.g. images, videos, text sequences)</li><li><strong>Methods</strong> for training DNNs (e.g. AdaGrad, ADAM, RMSProp, Dropout)</li><li><strong>Hardware</strong> platforms for training DNNs (e.g. GPUs, TPUs, FPGAs)</li><li><strong>Software</strong> platforms for training DNNs (e.g., Tensorflow, Pytorch, MXNet)</li><li><strong>Applications!</strong> (e.g. vision, speech, NLP, imaging, physics, mathematics finance)</li></ul><h3 id="DL-leads-to-many-things-…">DL leads to many things …</h3><blockquote><p>Oxford Dictionary </br><br>Revolution: a great change in conditions, ways of working, beliefs, etc. that affects large numbers of people.</p></blockquote><h3 id="Academic-breakthroughs">Academic breakthroughs</h3><ul><li>increasing successful rate in image classification (2012)</li><li>increasing successful rate in speech recognition (2012)</li><li>Go game (2017)</li><li>image generation</li></ul><p><img src="slide5.png" alt="slide5"></p><h2 id="Commercial-breakthroughs">Commercial breakthroughs</h2><ul><li>self-driving vehicles</li><li>smart-home devices</li><li>healthcare</li><li>robotics</li></ul><p>Paper are produced at an <strong>overwhelming</strong> rate. </br><br>Note: <a href="arvis.org">arvis</a> (where people post preprint papers)</p><h1>Why first principles?</h1><ul><li>Tuning and optimizing for a task require basic intuitions</li><li>Historial lesson: model structures in data</li><li>Current challenge: move toward trustworthiness</li><li>Future world: navigate uncertainties</li></ul><h2 id="Structures-are-crucial">Structures are crucial</h2><p><img src="slide14a.png" alt="slide14"></p><h2 id="Toward-trustworthy-AI">Toward trustworthy AI</h2><ul><li>we need to know first principles in order to improve and understand</li><li>Trustworthiness:<ul><li>robustness</li><li>fairness (<em>what if data is primarily from certain population group</em>)</li><li>explainability (<em>what if we make human-level robot but we don’t understand how to make decision</em>)</li><li>transparency</li></ul></li></ul><h2 id="Future-uncertainties">Future uncertainties</h2><ul><li>New types of data (e.g. 6-D tensors)</li><li>New hardware (e.g. better GPU memory)</li><li>New model pipelines (e.g. network of networks, differential programming)</li><li>New applications</li><li>New techniques replacing DL</li></ul><h1>Netruel Networks: Old and New</h1><h2 id="Start-from-Neurons">Start from Neurons</h2><h3 id="Model-of-biological-neurons">Model of biological neurons</h3><p><img src="slide4.png" alt="slide4"></p><h2 id="Shallow-to-DNNs">Shallow to DNNs</h2><h3 id="Artificial-Neurons">Artificial Neurons</h3><p><img src="slide7.png" alt="slide7"><br>ReLU is the most popular activation function nowadays.</p><p><img src="slide8.png" alt="slide8"><br>People play diverse forms of NN but only play with graph without cycles, because we cannot do auto differentiation once we have cycles.</p><h3 id="Supervised-Learning-in-ML">Supervised Learning in ML</h3><p><img src="slide9.png" alt="slide9"></p><h3 id="Supervised-Learning-in-DL">Supervised Learning in DL</h3><p><img src="slide10.png" alt="slide10"><br>Instead of finding a function from a family of functions, we try to find a group of weights (w) and offsets(b) to minimize the average loss.</p><h3 id="Perceptron">Perceptron</h3><p>Def: Perceptron is a single artificial neuron for binary classification.<br><img src="slide12.png" alt="slide12"><br><img src="slide13.png" alt="slide13"><br><img src="slide14.png" alt="slide14"><br><img src="slide16.png" alt="slide16"></p><h3 id="They-are-all-shallow-NNs">They are all (shallow) NNs</h3><ul><li>Linear Regression</li><li>Perceptron and Logistic Regression</li><li>Softmax Regression</li><li>Multilayer Perceptron (feedforward NNs)</li><li>Support Vector Machine (SVM)</li><li>PCA (autoencoder)</li><li>Matrix Factorization</li></ul><h2 id="Some-Background-Knowledge">Some Background Knowledge</h2><h3 id="Turing-Test">Turing Test</h3><p>How we measure whether we achieve human-level AI?<br><img src="slide20.png" alt="slide20"></p><h3 id="Key-ingredients-of-DL">Key ingredients of DL</h3><p><img src="slide26.png" alt="slide26"><br>Suprisingly, we’re still using those computational methods that were invented 25-30 years ago. Nowadays the progress of DL largely depends on improvement of hardware (GPUs), but seldom on new computational methods.</p><p>Question to myself: Are we approaching the third AI winter? Should we focus more on foundamental research instead of picking new applications?</p><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course (CSCI 8980) is being offered by &lt;a href=&quot;https://sunju.org/&quot;&gt;Prof. Ju Sun&lt;/a&gt; at the University</summary>
      
    
    
    
    
    <category term="Deep Learning" scheme="https://gaoxiangluo.github.io/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://gaoxiangluo.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>For UMN: How to setup remote enviroment with CSE lab machine via VSCode</title>
    <link href="https://gaoxiangluo.github.io/2020/09/12/For-UMN-How-to-setup-remote-enviroment-with-CSE-lab-machine-via-VSCode/"/>
    <id>https://gaoxiangluo.github.io/2020/09/12/For-UMN-How-to-setup-remote-enviroment-with-CSE-lab-machine-via-VSCode/</id>
    <published>2020-09-12T14:10:03.000Z</published>
    <updated>2020-09-28T05:16:41.185Z</updated>
    
    <content type="html"><![CDATA[<h1>VSCode Setup with CSE Lab Machines on Your Own Computer</h1><h2 id="Demo">Demo</h2><p><img src="Demo.png" alt="Demo"></p><h3 id="Why-should-you-consider-this-setup">Why should you consider this setup?</h3><ul><li>feel like working on CSE Lab Machines literally</li><li>don’t have to install <code>OCaml</code> on your computer because you’re using lab’s <code>OCaml</code></li><li>graphical user interface with <code>OCaml</code> Syntax Highlighting</li><li>faster than <code>vole</code> since it talks to server directly rather than transmitting through web browser</li><li>can be used on <code>Linux</code>, <code>MacOS</code> and <code>Windows</code> because they all have <code>VSCode</code></li><li>file transfer: drag you local file into CSE lab machine</li></ul><h2 id="Tutorial">Tutorial</h2><p>This tutorial can help you to set up your remote enviroment graphically with CSE lab machines on your own computer.</p><hr><ol><li><p>Open <code>VSCode</code>, go to extension on the left nevigation bar and search <code>ssh</code><br><img src="step1.png" alt="step1"></p></li><li><p>Install <code>REMOTE - SSH</code>, click <code>SSH extension</code> icon on the left nevigation bar and click <code>connect</code><br><img src="step2.png" alt="step2"></p></li><li><p>On the pop-up command window, type in <code>ssh &lt;x500&gt;@&lt;cse lab machines&gt; -A</code>.<br>For instance, I will use:</p><ul><li><code>ssh luo00042@atlas.cselabs.umn.edu -A</code> ATLAS is a server machine which has 256G RAM! But sometimes if a student is running heavy-duty task on ATLAS, you may want to use another machine.</li><li><code>ssh luo00042@csel-kh4250-03.cselabs.umn.edu -A</code></li></ul></li></ol><p><img src="step3.png" alt="step3"></p><p><strong>Note:</strong> A list of CSE lab UNIX machines that you can remotely work on, you can find them <a href="https://cseit.umn.edu/computer-classrooms/cse-labs-unix-machine-listings">here</a>.</p><p><strong>Remember:</strong> Replace the <code>x500</code> with yours, not mine; and make sure to include the <code>-A</code> flag, otherwise you have to use VPN or be under university wifi to connect.</p><ol start="4"><li><p>Click to update ssh configuration so you don’t have to type the long ssh command every time.<br><img src="step4.png" alt="step4"></p></li><li><p>When it’s done, there will be a SSH target on the left. Click <code>connect</code><br><img src="step5.png" alt="step5"></p></li><li><p>A new window will pop up and there will be prompt that you need to enter your <code>x500</code> password.<br><img src="step6.png" alt="step6"></p></li><li><p>You are in CSE lab machine via SSH connection! Click <code>Open Folder</code>. Doesn’t it look familiar to you? That’s your CSE lab folder! Navigate to your own repo and click <code>OK</code><br><img src="step7.png" alt="step7"></p></li><li><p>That’s it! You can open a terminal within <code>VSCode</code>, and it will be under SSH connection as well!<br><img src="step8.png" alt="step8"></p></li></ol><h3 id="OCaml-Syntax-Highlighting-Enable">OCaml Syntax Highlighting Enable</h3><ol start="9"><li><p>This is how it looks before enabling <code>OCaml</code> syntax highlighting<br><img src="step9.png" alt="step9"></p></li><li><p>Go to extension on the left nevigation bar again and search <code>OCaml</code>. Install it, and reload the window.<br><img src="step10.png" alt="step10"></p></li><li><p>Now you <code>OCaml</code> code should be highlighted accoridng to the syntax.<br><img src="step11.png" alt="step11"></p></li></ol><p>Now you can edit your code <strong>on lab machine</strong>, save your code <strong>on lab machine</strong>, and test your code with terminal <strong>on lab machine</strong>!</p><p><strong>Editor Note:</strong> While it’s easier to edit your code with a nice-looking IDE, it’s still good to pick up tools like emacs and vim. I hope you find this tutorial helpful to you!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;VSCode Setup with CSE Lab Machines on Your Own Computer&lt;/h1&gt;
&lt;h2 id=&quot;Demo&quot;&gt;Demo&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;Demo.png&quot; alt=&quot;Demo&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Why-</summary>
      
    
    
    
    
    <category term="How-to" scheme="https://gaoxiangluo.github.io/tags/How-to/"/>
    
  </entry>
  
  <entry>
    <title>Robotics: Perception study note week 1</title>
    <link href="https://gaoxiangluo.github.io/2020/09/06/Robotics-Perception-study-note-week-1/"/>
    <id>https://gaoxiangluo.github.io/2020/09/06/Robotics-Perception-study-note-week-1/</id>
    <published>2020-09-07T03:47:32.000Z</published>
    <updated>2020-09-28T03:34:12.512Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Acknowledgement: This course is being offered on <a href="https://www.coursera.org/learn/robotics-perception/">Coursera</a> for free to audit.</p></blockquote><blockquote><p>Gaoxiang: Due to the characteristic of this course, there maybe lots of pictures.</p></blockquote><h1>Overview</h1><ul><li>Week 1: Geometry of Projection</li><li>Week 2: Augmented Reality &amp; Visual Metrology</li><li>Week 3 &amp; 4: Where am I?</li></ul><hr><h1>Week 1</h1><p><img src="camera_types.png" alt="camera_type"><br>There are panoromic cameras, stereos camera, laser scanner and Kinect.</p><h1>How does a thin lens work?</h1><p><img src="thin_lens.png" alt="thin_lens"></p><ul><li>Rays parallet to the optical axis meet the focus after leaving the lens.</li><li>Rays through center of the lens do not change direction.</li></ul><p><img src="thin_lens_equation.png" alt="equation"></p><h1>What happens when we move the image plane?</h1><p>moving image plane = focusing (practically)<br><img src="move_image_plane.png" alt="plane_move"><br>The read line segment is what makes image blur.</p><h1>Perspective projection: size of object image</h1><p><img src="perspective_projection.png" alt="perspective_projection"></p><ul><li>This is easily proved by similarity of triangle.</li><li>A point object of the same size coming closer results on a larger image.</li><li>A point moving on the same ray does not change its image.</li></ul><hr><h1>Single View Geometry</h1><h2 id="Two-facts">Two facts:</h2><ul><li>When we take a pictuer, the 3D location has become a 2D plane. We have lost the third dimension in this process.</li><li>It matters how are oritented to the world when we take a picture.</li></ul><h2 id="Ideas-I-Measurements-on-planes">Ideas I: Measurements on planes</h2><p>We can unwarp then measutre, which means to make parallel lines stay parallel and perpendicular angle stay perpendicular.<br><img src="measure_planes.png" alt="measure_planes"></p><h2 id="Ideas-II-Vanishing-points">Ideas II: Vanishing points</h2><p><img src="vanishing_point.png" alt="vanishing_point"><br>The roof and ground are parellel in real world (blue lines), and they converge to a point if we draw them. That’s the vanishing point.</p><p><img src="imaging_vanishing_point.png" alt="imaging_vanishing_point"><br>As the blue dot moves further away, the projection point on the image plane will be closer and closer to the vanishing point.</p><hr><h1>More on Perspective Projection</h1><h2 id="Bi-perspectograph">Bi-perspectograph</h2><p><img src="prove_bi_perspectograph.png" alt="prove_bi_perspectograph"><br>Take some time to understand this proof. As the point P* on image plane moves, we’re able to draw the projection curve on the ground plane with point P’.</p><h2 id="Two-parameters">Two parameters:</h2><ul><li>Height of camera (line O*S)<ul><li>If we project a circle from image plane, the lower the camera is, the more squeezed is the ellipse.</li></ul></li><li>Distance of projection center from image plane (distance between two parallel lines LM and OS); also known as focal length<ul><li>When it moves, it <strong>only change the size not the shape</strong>.</li></ul></li></ul><hr><h1>Glimpse on Vanishing Points</h1><p><img src="properties_vanishing_point.png" alt="properties_vanishing_point"><br>The three properties of vanishing points are on the image above.</p><h2 id="Vanishing-Lines">Vanishing Lines</h2><p><img src="vanishing_lines.png" alt="vanishing_lines"><br>Horizon is a set of all directions to the infinity.</p><p>In other words, vanishing lines (horizon) is the intersection of image plane and the ground plane that is lifted up.<br><img src="compute_vanishing_lines.png" alt="compute_vanishing_lines"></p><h2 id="How-to-measure-height">How to measure height?</h2><p>If we draw a line of a walking person’s feet (ground plane), and lift up this line to the person’s head, these two lines are parellel in the third-person perspective. But in my perspective, these two lines will intersect to a vanishing point on horizon, which is the same height of camera.<br><img src="comparing_heights.png" alt="comparsing_heights"><br>According to this rule, we need a referenc2e object with known length then we can measure the heights in the scene.<br><img src="measuring_height.png" alt="measuring_height"><br>Connect the bottom of the object and the reference on the groud plane, it will intersect with horizon on a vanishing point. Then connect the vanishing point and the head of the object back to the reference. Now you have a ratio between object and reference so that you can measure the height.</p><hr><h1>Homogeneous Coordinates</h1><p>Before we move on to next section, I think it’s neccesary to know what is homogeneous coordinates and why we use it. This part is from the course video but my understandings.</p><h2 id="What-is-homogeneous-coordinates">What is homogeneous coordinates?</h2><p>It represents coordinates in 2 dimensions with a 3-vector.</p><ul><li>From euclidean to homogeneous<ul><li>add third coordinate as 1<br>(2,3)’ – (2,3,1)’</li><li>add third coordinate as 0 to express infinity<br>(2,3)’ – (2,3,0)’</li></ul></li><li>From homogeneous to euclidean<ul><li>divived by value of third coordinate<br>(4,5,1)’ – (4,5)<br>(8,6,3)’ – (8/3,6/3)</li><li>divived by 0 also proves that the point is in infinity</li></ul></li></ul><h2 id="Why-we-use-homogeneous-coordinates">Why we use homogeneous coordinates?</h2><ul><li>x = cx (x!=0)<br>e.g: (2,4,1)’ == (4,8,2)</li><li>it can represent point at infinity by the form (x,y,0)</li></ul><hr><h1>Perspective Projection</h1><h2 id="How-to-represent-a-point">How to represent a point?</h2><p><img src="projective_plane.png" alt="projective_plane"><br>The point in the image plane can be considered as a ray pointing to infinity.<br><img src="point.png" alt="point"><br>Since we consider a point on place as a ray going through the point and penetrating to the space, we can represent it as a vector using homogeneous coordinates.</p><h2 id="How-to-represent-a-line">How to represent a line?</h2><p><img src="lines.png" alt="lines"><br><strong>Important:</strong> a line is a plane of rays through origin.<br>(a,b,c) is a normal vector to the plane.<br><img src="line_representation.png" alt="line_representation"><br>We can also represent the line with polar coordinate representation.</p><h2 id="Line-passing-through-two-points">Line passing through two points</h2><p><img src="line_two_points.png" alt="line_two_points"><br>For every two points on the plane, there are two rays passing through them respectively from origin. It’s known that two lines define a plane, and sometimes we describe this plane by using it’s normal line.</p><p><img src="calculate_line.png" alt="calculate_line"><br>This is a sample MATLAB code to calculate the line.</p><pre class=" language-language-matlab"><code class="language-language-matlab">function I = get_line_by_two_points(x,y)x1 = [x(1), y(1), 1]';x2 = [x(2), y(2), 1]';I = cross(x1,x2);I = I / sqrt(I(1)*I(1) + I(2)*I(2));</code></pre><h2 id="How-to-find-the-intersection-of-two-lines">How to find the intersection of two lines?</h2><p><img src="intersection_of_lines.png" alt="intersection_of_lines"></p><pre class=" language-language-matlab"><code class="language-language-matlab">function x0 = get_point_by_two_line(I, II)x0 = cross(I,II);x0 = [x0(1)/x0(3); x0(2)/x0(3)];</code></pre><hr><h1>Point-Line Duality</h1><p><img src="duality.png" alt="duality"><br>This slide is to realize the duality of point and line in projective space. If given any formula, we can switch the meanings of poitns and lines to get another formula.</p><p><strong>Good to think:</strong> Now we know that take the cross product of two lines we get a point(ray) on the image plane, but what if the point we get has the form of (x,y,0). If we convert from 3D to 2D, either x or y divived by 0 we will get a point in infinity.<br><img src="point_at_infinity.png" alt="point_at_infinity"></p><p>That’s the point at infinity, what is line at infinity. We have to find a line that every points with form (x,y,0) will pass through it. So, algebraically:<br><img src="line_at_infinity.png" alt="line_at_infinity"></p><h2 id="Ideal-Points-and-Lines">Ideal Points and Lines</h2><p><img src="ideal_point_and_line.png" alt="ideal_point_and_line"></p><hr><h1>Rotations and Translations</h1><h2 id="Camera-Coordinates-World-Coordinates">Camera Coordinates &amp; World Coordinates</h2><p><img src="rgb_xyz.png" alt="rgb_xyz"><br>Convention: rgb to xyz</p><h2 id="What-is-the-geometric-meaning-of-translation">What is the geometric meaning of translation?</h2><p><img src="translation.png" alt="translation"><br>Mathematically, if the world coordinates of point P is (0,0,0), then the camera coordinates of point P is the vector from camera origin to world origin as shown on picture above.</p><h2 id="What-is-the-geometric-meaning-of-rotation">What is the geometric meaning of rotation?</h2><p><img src="rotation.png" alt="rotation"></p><ul><li>r1 is the x-axis of the world with repect to the camera as red</li><li>r2 is the y-axis of the world with repect to the camera as green</li><li>r3 is the z-axis of the world with repect to the camera as blue</li></ul><h2 id="An-example-of-how-to-read-rotation-matrix-and-tranlation-vector">An example of how to read rotation matrix and tranlation vector</h2><p><img src="rotation_matrix.png" alt="rotation_matrix"></p><ul><li>x is parellel to r1 but opposite direction, so r1 is (-1,0,0)’</li><li>y is parellel to r3 but opposite direction, so r2 is (0,0,-1)’</li><li>z is parellel to r2 but opposite direction, so r3 is (0,-1,0)’</li></ul><p><img src="read_translation.png" alt="read_translation"><br>Reading translation is simply read the origin of world to origin of camera. In this case, origin goes toward y direction for 5, and z direction for 10.<br><strong>Important:</strong> the vectors of rotation matrix have to be orthogonal to each other and determinant of rotation matrix has to equal one.</p><h2 id="body-coordinate-system">body coordinate system</h2><p>What if we have one more more coordinate system which is the body coordinates?<br><img src="transform_coordinate.png" alt="transform_coordinate"><br>It will be the same idea of using rotation and translation, but we have a simpler expression which is to use a 4x4 matrix.</p><h2 id="inverse-transformation">inverse transformation</h2><p><img src="inverse_transformation.png" alt="inverse_transformation"><br>Since we know the translation is the vector from camera’s origin to world’s origin, then inverse transformation is from world’s origin back to camera’s origin when keeping the rotation unchanged.</p><h2 id="Alternative-way-to-find-coordinates-after-transformation">Alternative way to find coordinates after transformation</h2><p><img src="golden_fule.png" alt="golden_fule"><br>The approach previously is more intuition-based I think. If you prefer mathematical computation, the formula can be applied to find the coordinates after transformation. Remember to put translation in front of rotations.</p><p><img src="rotation_rule.png" alt="rotation_rule"><br>This is a picture I found from wikipedia if you’re not familiar with rotation with repect to a certain axis.</p><h1>Pinhole Camera Model</h1><p><img src="camera_model.png" alt="camera_model"></p><ul><li>Pink: camera body (camera oritentation and position in the world)</li><li>Blue: sensor (transform optical measurement into pixel)</li><li>Red: focal length</li></ul><p><img src="pinhole_camera.png" alt="pinhole_camera"><br>In daily life, the camera is not a canvas but a reverse canvas. The image plane is behind us rather than in front of us, but we can imagine there is a virtual image in front of us.</p><h2 id="1st-Person-Camera-World">1st Person Camera World</h2><p><img src="1st_person.png" alt="1st_person"></p><p>How to define is that x-axis is the horizontal line in front of you, and y-axis is a vertical line in front of you, then z-axis is pointing toward/from you due to right-hand rule.</p><p>The point c in the image in you, as the center of the universe (0,0,0), and there is a xy-plane in front of you. The item we see through the image plane can be shrinked to the image plane through the equations above.</p><p>Since the image plane is fixed-size. If I take image plane further out, away from object, then object image will be smaller.</p><p>At this point, it’s good to think about:</p><ul><li>Where is the center of projection?</li><li>What is the focal length?</li></ul><h2 id="Let’s-do-a-experiement">Let’s do a experiement</h2><ol><li><p>Draw a set of rediating line on a piece of paper.</p></li><li><p>Place your phone camera as the picture shown.<br><img src="locating_center.png" alt="locating_center"></p></li><li><p>Look at the image in your camera, and move your camera back and forth until you see every line is parellel to each other.<br><img src="parellel_lines.gif" alt="parellel_lines"><br>(from my iPhone)</p></li><li><p>Draw a line to record the camera position.</p></li></ol><p>This process can be illustrated in this diagram.<br><img src="locate_center.png" alt="locate_center"></p><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Acknowledgement: This course is being offered on &lt;a href=&quot;https://www.coursera.org/learn/robotics-perception/&quot;&gt;Coursera&lt;/a&gt; </summary>
      
    
    
    
    
    <category term="Study Note" scheme="https://gaoxiangluo.github.io/tags/Study-Note/"/>
    
    <category term="Coursera" scheme="https://gaoxiangluo.github.io/tags/Coursera/"/>
    
    <category term="Robotics" scheme="https://gaoxiangluo.github.io/tags/Robotics/"/>
    
    <category term="Perception" scheme="https://gaoxiangluo.github.io/tags/Perception/"/>
    
  </entry>
  
  <entry>
    <title>Work on multiple computers for your schoolwork? Use git!</title>
    <link href="https://gaoxiangluo.github.io/2020/09/04/Work-on-multiple-computers-for-your-schoolwork-Use-git/"/>
    <id>https://gaoxiangluo.github.io/2020/09/04/Work-on-multiple-computers-for-your-schoolwork-Use-git/</id>
    <published>2020-09-05T02:16:57.000Z</published>
    <updated>2020-09-12T14:24:32.473Z</updated>
    
    <content type="html"><![CDATA[<h1>Sync Computers</h1><p>How do I use git to sync my Projects/HWs/Labs if I need to code them on laptop/desktop/lab machine?</p><pre class=" language-language-bash"><code class="language-language-bash"># On the first device$ git init$ git add .$ git commit -m "First release"$ git push </code></pre><pre class=" language-language-bash"><code class="language-language-bash"># On other devices$ git clone URL # end with .git</code></pre><pre class=" language-language-bash"><code class="language-language-bash"># From now on, every time one of devices is ready to release$ git commit -a -m "Next release"$ git push</code></pre><pre class=" language-language-bash"><code class="language-language-bash"># Any devices that want to sync to latest release$ git pull # a pull is simply a fetch then merge</code></pre><p>What if you want to continue edit on your file that wasn’t finished on other devices? In other words, without pushing.</p><pre class=" language-language-bash"><code class="language-language-bash">$ git pull other.computer:/path/to/files HEAD # HEAD tag is like a cursor that normally points at the latest commit</code></pre><p>If you’re using MacOS or Windows, there is a offcial GUI called <strong>Github Destop</strong> that will do the job for you. Also, <strong>Visual Studio Code</strong> has build-in plugin for git.</p><hr><h1>Branch is Your Friend</h1><p>What if you just want to try out some new ideas or maybe make a hotfix without interrupting the main repo from being used?</p><pre class=" language-language-bash"><code class="language-language-bash"># Try new ideas$ git commit -m "Initial commit" # always remember to commit before switching around$ git checkout -b new_branch # create a new branch and switch to it#### try some new ideas on this branch ####$ git commit -a -m "Another commit" # good habbit to commit$ git checkout master # switch back to master$ git branch -m new_branch # if you want to remove the branch that was created</code></pre><pre class=" language-language-bash"><code class="language-language-bash"># Quick Fixes$ git commit -a$ git checkout -b fixes###### fix goes here #######$ git commit -a -m "Bug fixed"$ git checkout master# You can even merge in the freshly baked bugfix$ git merge fixes</code></pre><hr><h1>Commit Message is Important</h1><p>Sometimes it’s good to be descriptive in your commit, so that other users will understand what changes you’ve made. If you use two <code>-m</code> flag, the first one will act like a title and the second will act like a descriptive body.</p><pre class=" language-language-bash"><code class="language-language-bash">$ git commit -m "title" -m "more detailed info"</code></pre><p>Did you just commit, but wish you had typed a different message? This can help you to change the last message.</p><pre class=" language-language-bash"><code class="language-language-bash">$ git commit --amend -m "an updated commit message"</code></pre><hr><h1>Acknowledgement</h1><p>Thanks <a href="http://www-cs-students.stanford.edu/~blynn/">Ben Lynn</a> for publishing the fun and informative git guide <a href="http://www-cs-students.stanford.edu/~blynn/gitmagic/">git magic</a>.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Sync Computers&lt;/h1&gt;
&lt;p&gt;How do I use git to sync my Projects/HWs/Labs if I need to code them on laptop/desktop/lab machine?&lt;/p&gt;
&lt;pre clas</summary>
      
    
    
    
    
    <category term="How-to" scheme="https://gaoxiangluo.github.io/tags/How-to/"/>
    
    <category term="Git" scheme="https://gaoxiangluo.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Build a blog with Hexo</title>
    <link href="https://gaoxiangluo.github.io/2020/08/29/Build-a-blog-with-Hexo/"/>
    <id>https://gaoxiangluo.github.io/2020/08/29/Build-a-blog-with-Hexo/</id>
    <published>2020-08-30T01:15:25.000Z</published>
    <updated>2020-09-12T14:24:30.987Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Setup">Setup</h2><ol><li><p>Install <code>Node.js</code>.</p><ul><li>MacOS: Download <code>Node.js</code> from <code>https://nodejs.org/</code>. When you install <code>Node.js</code>, <code>npm</code> will be installed automatically.</li><li>Ubuntu 20.04: Download them from command line.<pre class=" language-language-bash"><code class="language-language-bash">$ sudo apt update$ sudo apt install nodejs$ sudo apt install npm</code></pre></li></ul></li><li><p>When you install <code>Node.js</code>, <code>npm</code> will be installed automatically.</p></li><li><p>Make sure you have <code>git</code> installed.</p></li><li><p>Download Hexo’s framework using <code>npm</code>.</p><pre class=" language-language-bash"><code class="language-language-bash">$ sudo npm install -g hexo-cli</code></pre></li></ol><hr><h2 id="Initialization">Initialization</h2><ol><li><p>Make a new folder and navigate to it through terminal.</p></li><li><p>Initialize a blog with Hexo command line under the folder.</p><pre class=" language-language-bash"><code class="language-language-bash">$ hexo init</code></pre></li><li><p>Start a local server to host your blog.</p><pre class=" language-language-bash"><code class="language-language-bash">$ hexo server</code></pre></li><li><p>Now open a browser, and type <code>localhost:4000</code> in address bar. You will see the blog that you just created!</p></li></ol><hr><h2 id="Host-the-blog-on-Github">Host the blog on Github</h2><ol><li><p>In order to have others see your blog, you have to find a place to host your blog. Here I suggest using GitHub page, because each Github account can have a free domain ending with <code>github.io</code>.<br>npm install hexo-deployer-git --save</p></li><li><p>Go to Github, create a new repository and name it <code>web-name.github.io</code> (<strong>replace <code>web-name</code> with the name that you want</strong>). Make sure the repository is <strong>public</strong>.<br><img src="init-github-io.png" alt="Make a new repo to host blogs"></p></li><li><p>Open <code>_config.yml</code> with your favorite editor, and configure it as follows:</p><pre class=" language-language-yml"><code class="language-language-yml">deploy:  type: git  repo: https://github.com/GaoxiangLuo/web-name.github.io.git  branch: master</code></pre></li><li><p>Deploy your blog onto Github, login with username and password if being prompted</p><pre class=" language-language-bash"><code class="language-language-bash">$ hexo deploy</code></pre></li><li><p>Congratulation! You blog has been published at <code>web-name.github.io</code>!</p></li></ol><hr><div style="text-align: right"> To be continued... </div>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Setup&quot;&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;code&gt;Node.js&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MacOS: Download &lt;code&gt;Node.js&lt;/code&gt; from &lt;code&gt;https://no</summary>
      
    
    
    
    
    <category term="How-to" scheme="https://gaoxiangluo.github.io/tags/How-to/"/>
    
    <category term="Web Dev" scheme="https://gaoxiangluo.github.io/tags/Web-Dev/"/>
    
    <category term="Hexo" scheme="https://gaoxiangluo.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>I started my blog</title>
    <link href="https://gaoxiangluo.github.io/2020/08/29/I-started-my-blog/"/>
    <id>https://gaoxiangluo.github.io/2020/08/29/I-started-my-blog/</id>
    <published>2020-08-29T19:13:36.000Z</published>
    <updated>2020-09-07T03:49:56.219Z</updated>
    
    <content type="html"><![CDATA[<p>Hey all! I finally started my own blog, since I’ve been captured by how elegant the markdown language is. There are some reasons for me to start blogging.  Firstly, it motivates me to summarize the cool stuff that I’ve learned, which helps me to review and gain deeper insights of those knowledge. Secondly, I hope my blog will be a place where people can know me more; meanwhile, I would like to contribute my effort to the overall ComSci society, in publishing blogs that help people resolve technical issues. I hope it’s not too late to start as a rising junior undergrad, and I would like to see my blog grows as I grow!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hey all! I finally started my own blog, since I’ve been captured by how elegant the markdown language is. There are some reasons for me t</summary>
      
    
    
    
    
    <category term="Log" scheme="https://gaoxiangluo.github.io/tags/Log/"/>
    
  </entry>
  
</feed>
