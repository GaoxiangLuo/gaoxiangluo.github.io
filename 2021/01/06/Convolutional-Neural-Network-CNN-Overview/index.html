<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Acknowledgement: This course (CSCI 8980) was offered by Prof. Ju Sun at the University of Minnesota in Fall 2020. Pictures of slides are from the course.  From Fully Connected to Convolutional Neural">
<meta property="og:type" content="article">
<meta property="og:title" content="Convolutional Neural Network (CNN): Overview">
<meta property="og:url" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/index.html">
<meta property="og:site_name" content="Leon">
<meta property="og:description" content="Acknowledgement: This course (CSCI 8980) was offered by Prof. Ju Sun at the University of Minnesota in Fall 2020. Pictures of slides are from the course.  From Fully Connected to Convolutional Neural">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure1.png">
<meta property="og:image" content="https://math.now.sh?inline=%28%5Cfrac%7Bimage%7D%7B2%5En-1%7D%29">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure2.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure3.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure4.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure5.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure6.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure7.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure8.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure9.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure10.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Cast">
<meta property="og:image" content="https://math.now.sh?inline=%5Cstar">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure11.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure12.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure13.png">
<meta property="og:image" content="https://math.now.sh?inline=C_1">
<meta property="og:image" content="https://math.now.sh?inline=C_2">
<meta property="og:image" content="https://math.now.sh?inline=H%20%5Ctimes%20W">
<meta property="og:image" content="https://math.now.sh?inline=0%28C_1C_2H%5E2W%5E2%29">
<meta property="og:image" content="https://math.now.sh?inline=h%20%5Ctimes%20w">
<meta property="og:image" content="https://math.now.sh?inline=O%28C_1C_2hw%29">
<meta property="og:image" content="https://math.now.sh?inline=h%2Cw">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure14.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure15.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure16.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure17.png">
<meta property="og:image" content="https://math.now.sh?inline=%5Cgeq">
<meta property="og:image" content="https://math.now.sh?inline=%5Capprox">
<meta property="og:image" content="https://math.now.sh?inline=%5Cimplies">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure18.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure19.png">
<meta property="og:image" content="https://math.now.sh?from=%5Cmathcal%20F%20%28w%20%5Cast%20x%29%3D%5Cmathcal%20F(w)%20%5Codot%20%5Cmathcal%20%20F(x)%0A">
<meta property="og:image" content="https://math.now.sh?inline=%5Cnabla_x%20max%28x_1%2C%5Cdots%2Cx_n%29">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure20.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure21.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure22.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure23.png">
<meta property="og:image" content="https://math.now.sh?from=a%20%5Cast%20%28b%20%5Cast%20c%29%20%3D%20(a%20%5Cast%20b)%20%5Cast%20c%0A">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure24.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure25.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure26.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure27.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure28.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure29.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure31.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure32.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure33.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure34.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure35.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure36.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure37.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure38.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure39.png">
<meta property="og:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure40.png">
<meta property="article:published_time" content="2021-01-07T05:09:17.000Z">
<meta property="article:modified_time" content="2021-02-15T16:52:17.050Z">
<meta property="article:author" content="Gaoxiang Luo">
<meta property="article:tag" content="Neural Network">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="Convolution">
<meta property="article:tag" content="Pooling">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/figure1.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Convolutional Neural Network (CNN): Overview</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="Leon" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2021/01/07/A-Survey-of-Deep-Semantic-Segmentation-on-Computed-Tomography/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/12/15/I-received-Undergraduate-Research-Scholarship/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&text=Convolutional Neural Network (CNN): Overview"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&is_video=false&description=Convolutional Neural Network (CNN): Overview"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Convolutional Neural Network (CNN): Overview&body=Check out this article: https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&name=Convolutional Neural Network (CNN): Overview&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&t=Convolutional Neural Network (CNN): Overview"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">From Fully Connected to Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Find-patterns-in-an-image"><span class="toc-number">1.1.</span> <span class="toc-text">Find patterns in an image</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Digital-Images"><span class="toc-number">1.1.1.</span> <span class="toc-text">Digital Images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-find-a-pattern-in-images"><span class="toc-number">1.1.2.</span> <span class="toc-text">How to find a pattern in images?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Template-matching-previals-in-classic-image-processing"><span class="toc-number">1.1.3.</span> <span class="toc-text">Template matching previals in classic image processing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Problem-with-template-matching"><span class="toc-number">1.1.4.</span> <span class="toc-text">Problem with template matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-based-approach"><span class="toc-number">1.1.5.</span> <span class="toc-text">Feature-based approach</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Problems-with-fully-connected-networks"><span class="toc-number">1.2.</span> <span class="toc-text">Problems with fully connected networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Complexity"><span class="toc-number">1.2.1.</span> <span class="toc-text">Complexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Locality-and-ordering"><span class="toc-number">1.2.2.</span> <span class="toc-text">Locality and ordering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Invariance"><span class="toc-number">1.2.3.</span> <span class="toc-text">Invariance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ideal-neural-networks-for-spatial-data"><span class="toc-number">1.2.4.</span> <span class="toc-text">Ideal neural networks for spatial data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-quick-preview-of-convolutional-neural-network-CNN"><span class="toc-number">1.2.5.</span> <span class="toc-text">A quick preview of convolutional neural network (CNN)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Components-of-CNNs"><span class="toc-number">1.3.</span> <span class="toc-text">Components of CNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution-Layer"><span class="toc-number">1.3.1.</span> <span class="toc-text">Convolution Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Convolution-is-misnomer"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Convolution is misnomer!</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#More-on-conlution-correlation"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">More on conlution&#x2F;correlation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Connection-to-fully-connected-neural-network"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Connection to fully-connected neural network</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multiple-filters-each-layer"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">Multiple filters each layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Do-we-reduce-the-complexity"><span class="toc-number">1.3.1.5.</span> <span class="toc-text">Do we reduce the complexity?</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pooling-Layer"><span class="toc-number">1.3.2.</span> <span class="toc-text">Pooling Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-pooling"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Why pooling?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Combine-covolution-and-pooling-%E2%80%93-convolution-with-strides"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Combine covolution and pooling – convolution with strides</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-multilayers"><span class="toc-number">1.3.3.</span> <span class="toc-text">Why multilayers?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-feature-learning"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Hierarchical feature learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Computation"><span class="toc-number">1.3.4.</span> <span class="toc-text">Computation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-compute-convolution"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">How to compute convolution?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#More-on-computation"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">More on computation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architectures-for-classification"><span class="toc-number">1.4.</span> <span class="toc-text">Architectures for classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Typical-design-patterns"><span class="toc-number">1.4.1.</span> <span class="toc-text">Typical design patterns</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LeNet-5-1998"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">LeNet-5 (1998)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AlexNet-2012"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">AlexNet (2012)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VGG-Net-2014"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">VGG-Net (2014)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet-2015"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">ResNet (2015)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dense-2016"><span class="toc-number">1.4.1.5.</span> <span class="toc-text">Dense (2016)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Other-models-to-look-at"><span class="toc-number">1.4.1.6.</span> <span class="toc-text">Other models to look at</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Practicle-tips"><span class="toc-number">1.5.</span> <span class="toc-text">Practicle tips</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transfer-Learning"><span class="toc-number">1.5.1.</span> <span class="toc-text">Transfer Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Are-CNNs-only-for-images"><span class="toc-number">1.5.2.</span> <span class="toc-text">Are CNNs only for images?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transposed-Convolution"><span class="toc-number">1.5.3.</span> <span class="toc-text">Transposed Convolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalization"><span class="toc-number">1.5.4.</span> <span class="toc-text">Normalization</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Applications of CNNs in Computer Vision</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Object-Detection"><span class="toc-number">2.1.</span> <span class="toc-text">Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-object-detection"><span class="toc-number">2.1.1.</span> <span class="toc-text">What is object detection?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object-Detection-Network"><span class="toc-number">2.1.2.</span> <span class="toc-text">Object Detection Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Multiple-Objects"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Multiple Objects</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-step-object-detection-framwork"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">4-step object-detection framwork</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Region-Proposal-indentify-regions-of-interest-RoI-for-potential-locations-of-objects"><span class="toc-number">2.1.2.2.1.</span> <span class="toc-text">1. Region Proposal: indentify regions of interest (RoI) for potential locations of objects</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Selective-Search"><span class="toc-number">2.1.2.2.1.1.</span> <span class="toc-text">Selective Search</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Feature-Extraction-extract-visial-features-within-each-RoI-for-classification"><span class="toc-number">2.1.2.2.2.</span> <span class="toc-text">2. Feature Extraction: extract visial features within each RoI for classification</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-Non-maximum-Suppression-NMS-avoid-repeated-detections"><span class="toc-number">2.1.2.2.3.</span> <span class="toc-text">3. Non-maximum Suppression (NMS): avoid repeated detections</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-Evaluateion-metrics-evaluate-performance-of-model"><span class="toc-number">2.1.2.2.4.</span> <span class="toc-text">4. Evaluateion metrics: evaluate performance of model</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#State-of-the-Art-Object-Detection-CNNS"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">State of the Art Object Detection CNNS</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#R-CNNs-Region-based-CNNs"><span class="toc-number">2.1.2.3.1.</span> <span class="toc-text">R-CNNs: Region-based CNNs</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Convolutional Neural Network (CNN): Overview
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Leon</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-01-07T05:09:17.000Z" itemprop="datePublished">2021-01-06</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/CNN/" rel="tag">CNN</a>, <a class="tag-link-link" href="/tags/Convolution/" rel="tag">Convolution</a>, <a class="tag-link-link" href="/tags/Neural-Network/" rel="tag">Neural Network</a>, <a class="tag-link-link" href="/tags/Pooling/" rel="tag">Pooling</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>Acknowledgement: This course (CSCI 8980) was offered by <a target="_blank" rel="noopener" href="https://sunju.org/">Prof. Ju Sun</a> at the University of Minnesota in Fall 2020. Pictures of slides are from the course.</p>
</blockquote>
<h1>From Fully Connected to Convolutional Neural Networks</h1>
<h2 id="Find-patterns-in-an-image">Find patterns in an image</h2>
<h3 id="Digital-Images">Digital Images</h3>
<p><img src="figure1.png" alt="figure1"><br>
Digit images can be simply looked as matrices, and each entry of matrices (i.e., pixels) is an integer. We call the traditional MNIST images gray scale level images. Color images are 3-dimensional tensors. R,G,B are respectively treated as a gray scale image. We usually normalize the data to alleviate the effect of noise, by</p>
<ul>
<li>dividing the data by its “pixel-depth - 1” <img src="https://math.now.sh?inline=%28%5Cfrac%7Bimage%7D%7B2%5En-1%7D%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li>
<li>zero-mean unit-variance</li>
<li>min-max.</li>
</ul>
<p>This little change can effect performance to an extent. JPEG by default usually throws some details out from the images. If you’re dealing with questions that every pixel is important, maybe you want to use PNG rather than JPEG since PNG is lossless.</p>
<h3 id="How-to-find-a-pattern-in-images">How to find a pattern in images?</h3>
<p><img src="figure2.png" alt="figure2"><br>
Let’s divide it by two questions. We firstly ask whether this object is in this image. Secondly we ask where is the object in the image if this object is in this image. Scan-window method is to take a sample and scan through the entire image to find similarity (distance). Inner product is a measure of similarity, and each time inner product of the original (red) and overlapped (green) patches are taken. If they’re similiar, then the inner product is larger. The output matrix is called the correlation. Now let’s look at the largest magnitude because that will be the canidate match of detection.<br>
Is correlation always reliable? No, a counterexample in this case is that the background is plain white.</p>
<h3 id="Template-matching-previals-in-classic-image-processing">Template matching previals in classic image processing</h3>
<p><img src="figure3.png" alt="figure3"><br>
How do you detect the edge? A typical edge looks like left(white) right(black) middle(gray) as the picture above.</p>
<h3 id="Problem-with-template-matching">Problem with template matching</h3>
<p><img src="figure4.png" alt="figure4"><br>
What are some potential problems? Would scan-window method work here? In practices, there are many variation. This object can be smaller, larger, scaling, rotation, deformation, etc.</p>
<h3 id="Feature-based-approach">Feature-based approach</h3>
<p><img src="figure5.png" alt="figure5"><br>
We may be convinced that the classic template matching slicing window scheme doesn’t work well if there are data augmentation (rotation, scaling, translation, etc). So, is it possible to extract the features first that are invariant to scaling, rotation, deformation? Yes! An era of feature-based methods has begun…</p>
<h2 id="Problems-with-fully-connected-networks">Problems with fully connected networks</h2>
<h3 id="Complexity">Complexity</h3>
<p><img src="figure6.png" alt="figure6"><br>
Fully connected networks are not feasible just looking at the dimension. Images with high-resolution have many pixels, which are the input of neural networks. It’s computational impossible. This is from the complexity and storage point of view.</p>
<h3 id="Locality-and-ordering">Locality and ordering</h3>
<p><img src="figure7.png" alt="figure7"><br>
No many people talk about this argument but it’s very important. An image tends to have some local structure, because adjacent pixels of natural images are highly correlated. Natual images are pixel-wise smooth. Fully connected neural networks (FCNNs) treats the input as vector, and it’s even insensitive to any universal permutation of the coordinates of all inputs.</p>
<h3 id="Invariance">Invariance</h3>
<p>It’s not sensitive to where the object is in the image. You always want to do certain levels of invariance in recognition. For intance, if I move the digit 8 around in the image, the neural network should be invariant to translation. Think about moving a digit from one place to another place in the image, in the image pixel matric what we see is some irregualr movements/changes. In short, fully connected network is not invariant to scaling, rotation, translation, etc.</p>
<h3 id="Ideal-neural-networks-for-spatial-data">Ideal neural networks for spatial data</h3>
<p><img src="figure8.png" alt="figure8"><br>
We hope to acheive learning features locally. For instance, learn the low-level features such as lines, edges, curves, etc, or a bit high-level featuers such as eyes, ears, etc, which are invariant to translation, rotation, local deformation, etc.</p>
<h3 id="A-quick-preview-of-convolutional-neural-network-CNN">A quick preview of convolutional neural network (CNN)</h3>
<p><img src="figure9.png" alt="figure9"><br>
Input -&gt; Convolution Layers (Feature Extraction) -&gt; Fully Connected Layers (Classification) -&gt; Output</p>
<h2 id="Components-of-CNNs">Components of CNNs</h2>
<h3 id="Convolution-Layer">Convolution Layer</h3>
<h4 id="Convolution-is-misnomer">Convolution is misnomer!</h4>
<p><img src="figure10.png" alt="figure10"><br>
Convolution is actually mathematically a wrong name. It’s pretty much correlation. The only difference here is if you flip or not as shown in the image above. People actually implement correlation and call it convolution… In math notation, <img src="https://math.now.sh?inline=%5Cast" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> for convolution and <img src="https://math.now.sh?inline=%5Cstar" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> for (cross)-correlation.</p>
<h4 id="More-on-conlution-correlation">More on conlution/correlation</h4>
<p><img src="figure11.png" alt="figure11"><br>
In image, people usually call the template in between as filter/kernel. Also, people call the area the filter cover each time in the image as receptive field. Further, people call the output from kernel as feature map.<br>
There are also two concepts, which are padding and stride. Padding is to put 0s outside of the image. People do this in order to catch edge information. Stride is basically meaning step size here. Sometimes if people want to skip one pixel, they will usually set the stride as 2. More information and GIF can be found <a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic">here</a>. Typically, people only set stride to at most 2.</p>
<h4 id="Connection-to-fully-connected-neural-network">Connection to fully-connected neural network</h4>
<p><img src="figure12.png" alt="figure12"><br>
Abstractly, you can think of convolution imports local pattern. Think of it in the fully-connected neural network, then convolution makes each neuron connects only to its receptive field, and all neurons share the same weight pattern.<br>
Convolution enforeces local pattern, because each time I only look at local area. By weight sharing, we cut down the number of parameters that we need to learn.</p>
<h4 id="Multiple-filters-each-layer">Multiple filters each layer</h4>
<p><img src="figure13.png" alt="figure13"><br>
For one filter, as image above from input volomn to output volomn there is an important summation, which acts just like flatten the matrix. Btw, 2D covolution is moving in 2D. It’s not 3D convolution because it’s not moving in depth but moving 3D tensor in 2D.</p>
<h4 id="Do-we-reduce-the-complexity">Do we reduce the complexity?</h4>
<p>Suppose <img src="https://math.now.sh?inline=C_1" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> input channels and <img src="https://math.now.sh?inline=C_2" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> output channels of size <img src="https://math.now.sh?inline=H%20%5Ctimes%20W" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></p>
<ul>
<li>number of parameters if implementing fully connected layer: <img src="https://math.now.sh?inline=0%28C_1C_2H%5E2W%5E2%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li>
<li>number of parameters if implementing convolution of <img src="https://math.now.sh?inline=h%20%5Ctimes%20w" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/>? <img src="https://math.now.sh?inline=O%28C_1C_2hw%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> where <img src="https://math.now.sh?inline=h%2Cw" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> are usually small constants, e.g., 3 in practice</li>
</ul>
<h3 id="Pooling-Layer">Pooling Layer</h3>
<p>Convolution helps to achieve locality, and reduced complexity, what about invariance?<br>
<img src="figure14.png" alt="figure14"><br>
Pooling is not a difficult operation to understand. In the image above, there is a 4x4 image. If the size of pooling receptive field size is 2x2 with a stride of 2. Then I just slide the pooling receptive field over the original image. For max pooling, I select the maximum pixel value of within my pooling receptive field. Other than max pooling, there are also average pooling, i.e., weighed average within the receptive field.</p>
<h4 id="Why-pooling">Why pooling?</h4>
<p><img src="figure15.png" alt="figure15"></p>
<ul>
<li>deep layer: more filters, which makes us end up with many channels (thicker), so we have to subsample to avoid explosion in computation</li>
<li>subsampling keep important features; for imaging processing, most of the time people are doing recognition, so what matters is the general shape but not low-level details. So, subsampling is ok.<br>
<img src="figure16.png" alt="figure16"><br>
Do we really achieve invariance?<br>
<img src="figure17.png" alt="figure17"><br>
Let’s look at the image above as top view and bottom view, where the bottem view can be think of the top view’s detector stage move to the left by 1. What we observe is that even though there are movement in detector stage, but the pooling stage is roughly the same, and that’s roughly what we mean by saying pooling can achieve a certain degree of invariance.</li>
</ul>
<h4 id="Combine-covolution-and-pooling-–-convolution-with-strides">Combine covolution and pooling – convolution with strides</h4>
<p>idea: convolution with stride <img src="https://math.now.sh?inline=%5Cgeq" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> 2 <img src="https://math.now.sh?inline=%5Capprox" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> convolution + subsampling</p>
<p>One can either do a stride 2 pooling after convolution, or simply a convolution with stride 2. The idea of getting rid of pooling layers is more and more popular within Generative Adversial Networks (GANs).</p>
<h3 id="Why-multilayers">Why multilayers?</h3>
<ul>
<li>For efficiency: each object can have different variations, simply using one kernel for each variation of the object is impossible.</li>
<li>For goodness: Kernels can be shared across digits or all object catogories; low-level features likely sharable <img src="https://math.now.sh?inline=%5Cimplies" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/> form hierarchy</li>
</ul>
<h4 id="Hierarchical-feature-learning">Hierarchical feature learning</h4>
<p><img src="figure18.png" alt="figure18"><br>
The above are the real output of certain layers of CNNs. Really we see that low-level feature is somewhat similiar.</p>
<h3 id="Computation">Computation</h3>
<h4 id="How-to-compute-convolution">How to compute convolution?</h4>
<p>Convolution layer is locally connected, weight-sharing fully connected layer. If we vetorize both input and output, the operation can be represented as a matrix multiplication.<br>
<img src="figure19.png" alt="figure19"></p>
<h4 id="More-on-computation">More on computation</h4>
<p>To compute the convolution</p>
<ul>
<li>use (sparse) matrix-vector multiplication (early version of cuDNN)</li>
<li>ise fast Fourier transform (introduced in later version of cuDNN)</li>
</ul>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=%5Cmathcal%20F%20%28w%20%5Cast%20x%29%3D%5Cmathcal%20F(w)%20%5Codot%20%5Cmathcal%20%20F(x)%0A" /></p><p>To compute the max-pooling</p>
<ul>
<li>forward: just try to pick up the maximum</li>
<li>backward? what’s <img src="https://math.now.sh?inline=%5Cnabla_x%20max%28x_1%2C%5Cdots%2Cx_n%29" style="filter: opacity(100%);transform:scale(1.00);text-align:center;display:inline-block;margin: 0;"/></li>
</ul>
<h2 id="Architectures-for-classification">Architectures for classification</h2>
<h3 id="Typical-design-patterns">Typical design patterns</h3>
<p>Feature Extraction (Conv) + Classification (Fully connected)</p>
<ul>
<li>Conv: depth increases (more filters), dimension decreases (subsampling) when moving deeper<br>
<img src="figure20.png" alt="figure20"></li>
<li>one or two fully connected layers for classifaction</li>
</ul>
<h4 id="LeNet-5-1998">LeNet-5 (1998)</h4>
<p><img src="figure21.png" alt="figure21"><br>
At that time, people were stilling using hypertangent, and 5x5 filter, which now has been replaced by ReLU and 3x3 filter.</p>
<h4 id="AlexNet-2012">AlexNet (2012)</h4>
<p><img src="figure22.png" alt="figure22"><br>
Naivly, this can be thought of as a deeper version of LeNet with more convolutional layers and more fully connected layers. This brings a breakthrough on ImageNet competition in 2012, and impressed the computer vision community.</p>
<ul>
<li>ReLU as activation</li>
<li>larger filters: 11x11, 5x5, 3x3</li>
<li>dropout used for regularization (dropout wasn’t proposed by this paper but made popular by this paper)</li>
<li>weight decay/regularization</li>
</ul>
<h4 id="VGG-Net-2014">VGG-Net (2014)</h4>
<p><img src="figure23.png" alt="figure23"><br>
This is a further deeper version of AlexNet. (At that time, people really was trying to go deeper after they see the befinit of more layers.) They have some novel modification, which they perform several convolution in a row and followed by a pooling layer. The intuition comes from the associative law of convolution, which is</p>
<p style="filter: opacity(100%);transform:scale(1.00);text-align:center;"><img src="https://math.now.sh?from=a%20%5Cast%20%28b%20%5Cast%20c%29%20%3D%20(a%20%5Cast%20b)%20%5Cast%20c%0A" /></p><p>This is to say that I can have serveral smaller filter (several convolution layers) and gain a roughly (<strong>not exactly</strong>) the same effect of a single large filter. And you will found the number of parameters reduce significantly.</p>
<h4 id="ResNet-2015">ResNet (2015)</h4>
<p>Sometimes performance get worse when the network is really deep. What’s going wrong? The performance somewhat get degragated during going deeper.<br>
<img src="figure24.png" alt="figure24"></p>
<ul>
<li>skip connection</li>
<li>batch normalization<br>
Intuition for skip connection: If I have a task which has 50 layers to attain optimal performance already, what if I add up to 100 layers? That will bring performance degragation, but if I have skip connection after the 50th layer because I know it doesn’t need 100 layers. If I training algorithm is smart enough, it will make the weights of all the layers after 50th layer to be zeros. Then, without lossing the previous information from eailier layers, even though the latter layers’s weight are zeros, but I can still reach to output layer with skip connection. In addition, skip connection in auto differention package also acts like a skip conenction. Moreoever, skip connection also alleviates vanishing-gradient problem to some extents.</li>
</ul>
<h4 id="Dense-2016">Dense (2016)</h4>
<p>No soon after ResNet, Dense Net was proposed to take full adventage of skip connections.<br>
<img src="figure25.png" alt="figure25"></p>
<h4 id="Other-models-to-look-at">Other models to look at</h4>
<p>on accuracy:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.11946">EfficientNet</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.05431">ResNeXt</a></li>
</ul>
<p>on compact model:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07360">SqueezeNet</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.11164">ShuffleNet</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.04381">MobileNet</a></li>
</ul>
<h2 id="Practicle-tips">Practicle tips</h2>
<h3 id="Transfer-Learning">Transfer Learning</h3>
<p>We recall that CNNS learn increasingly complex and sementically meaningful features, while they have similiar low-level features. So, lower-level tend to learn features that are generic.<br>
<img src="figure26.png" alt="figure26"><br>
The image above you can see that different objects can share similiar low-level features. So, people can take a pretrain model (take both network architecture and <strong>weights</strong>) which has been trained on a very large and diverse dataset, as a starting point. And based on the size of dataset of you task, to select a scheme to perform fine tuning as the image below.<br>
<img src="figure27.png" alt="figure27"></p>
<h3 id="Are-CNNs-only-for-images">Are CNNs only for images?</h3>
<p>No, it’s popular in imaging, but also can be used in speed recognition, text classification, video analysis, and time series analysis.</p>
<h3 id="Transposed-Convolution">Transposed Convolution</h3>
<p>convolution with strides: downsampling<br>
transposed convolution: unsampling<br>
Transposed convolution is often used for segmentation (U-Net), generation, or other regression. The ouputs are structured object such as images, videos, time series, speech, etc.<br>
For unsampling, there are definitely classical way to perform it, which is nearest neighbor/bilinear/bicubic intepolation. In deep learning, people want to learn interpolation with learnable filter, that’s why the transposed convolution come into play. Actually, transposed convolution is also called <strong>fractionally strided convolutions</strong> or deconvolution.<br>
<img src="figure28.png" alt="figure28"><br>
The graph and more details are <a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic">here</a>.</p>
<h3 id="Normalization">Normalization</h3>
<p><img src="figure29.png" alt="figure29"><br>
normalization in different directions/groups of the data tensors</p>
<ul>
<li>N is the batch size</li>
<li>C is the channel size</li>
<li>WH is the per output dimension (1D for fully connected, but 2D for CNNs)</li>
</ul>
<p>batch normalization is popular, but with layer/group normalization you can have small N (batch size), reach simplicity (training/test normalizations are consistent).</p>
<h1>Applications of CNNs in Computer Vision</h1>
<blockquote>
<p>Acknowledgement: The content of the following slides is from one of the grad students in the class, Andrea Walker, who were giving a presentation.</p>
</blockquote>
<h2 id="Object-Detection">Object Detection</h2>
<h3 id="What-is-object-detection">What is object detection?</h3>
<p>There are 2 main tasks:</p>
<ul>
<li>Localizing one or more objects in the image</li>
<li>Classifying each object in the image<br>
<img src="figure31.png" alt="figure1"><br>
Credit: The image above is from <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.06849">Islam el al., 2019</a>.</li>
</ul>
<h3 id="Object-Detection-Network">Object Detection Network</h3>
<p><img src="figure32.png" alt="figure2"><br>
The image above is from <a target="_blank" rel="noopener" href="http://vision.stanford.edu/teaching/cs231n/slides/2020/lecture_12.pdf">Stanford CS231N</a>. The inputs here are a image and a bounding box which contains the object, and the outputs here are a classification score of multiclass and a bounding box of the classified object. Since after the CNN conponent, there are two fully connected layers to output two different outcomes respectively. Here the obejctive loss we use is simply add the loss of both tasks together. I believe you’re familair with the softmax loss as it’s used in handwritten digit recognition in MNIST, and notably the localization here can be treated as a regression problem so that we can use the L2 loss.</p>
<h4 id="Multiple-Objects">Multiple Objects</h4>
<p>In the example above, we only have one object (a cat) in the image. What if we have multiple objects in the image? If we have one more object in the images, in the input we will have one more class label with four values to indicate another bounding box, here we see that the program starts to scale once we have multiple obejcts in a image.</p>
<p>A simple solution to multiple obejcts could be a slicing window over the entire image, which means we apply a CNN to many different crops of the image and then CNN classifies each crop as object or background. However, this is very computationally expensive! Most of the object detection application require this to be happened in the real time. So, what can be a good solution to multiple obejcts?</p>
<h4 id="4-step-object-detection-framwork">4-step object-detection framwork</h4>
<h5 id="1-Region-Proposal-indentify-regions-of-interest-RoI-for-potential-locations-of-objects">1. Region Proposal: indentify regions of interest (RoI) for potential locations of objects</h5>
<ul>
<li>General procesures for region proposal:
<ul>
<li>Generate thousands of bouding boxes</li>
<li>Classify them as forground or background based on ‘objectness score’</li>
<li>Pass only foreground through rest of the network<br>
People commonly use <strong>selective search</strong>, which is a fast algorithm, ~200 region proposals in a few seconds on CPU<br>
<img src="figure33.png" alt="figure3"><br>
The images under this entire section are from <a target="_blank" rel="noopener" href="https://www.manning.com/books/deep-learning-for-vision-systems">Elendy, 2020</a>.</li>
</ul>
</li>
</ul>
<h6 id="Selective-Search">Selective Search</h6>
<p>Selective search is a greedy search algorithm. The following are the steps of selective search.</p>
<ol>
<li>Segmentation. It basically defines the ‘blobs’ within the image in the segmentations step that can potentially be objects.<br>
<img src="figure34.png" alt="figure4"></li>
<li>Take these proposed regions that may contain objects as input, and the output will be as the image shown below. Those blue boxes are the regions that may contain a object, and the algorithm combines the two similiar regions into one region. So, after many iteration we’ll find we have less blue boxes now. This step will continue until the entire object is in a bounding box.<br>
<img src="figure35.png" alt="figure5"></li>
</ol>
<h5 id="2-Feature-Extraction-extract-visial-features-within-each-RoI-for-classification">2. Feature Extraction: extract visial features within each RoI for classification</h5>
<ul>
<li>Use a pretrained CNN network, and extract features</li>
<li>Make 2 predictions using additional layers:
<ul>
<li>Bounding box prediction (x, y, width, height) <em>You may be curious why we have to make a prediction of bounding boxes here, since we’ve already identify a region of interest in the previous step. It’s important for the network to predict a bounding box, because we’re interested in having a bounding box that predicts the optimal bounding box for the obejct that we’re detecting. The region of interest may not include the entire obejct, so we don’t want to be limited by that.</em></li>
<li>Class prediction (softmax function predicting the class probability for each object)</li>
</ul>
</li>
</ul>
<h5 id="3-Non-maximum-Suppression-NMS-avoid-repeated-detections">3. Non-maximum Suppression (NMS): avoid repeated detections</h5>
<p>There is a 4-step technique for eliminating duplicate detections of objects.</p>
<ol>
<li>Discard bounding boxes with prediction below a <strong>confident threshold</strong>.</li>
<li>Select the bounding boxes with the highest probability</li>
<li>Calculate the overlap of all remaining boxes with the same class prediction</li>
<li>Supress any box with an IoU smaller than a threshold (NMS threshold, usually 0.5).<br>
<img src="figure36.png" alt="figure6"></li>
</ol>
<h5 id="4-Evaluateion-metrics-evaluate-performance-of-model">4. Evaluateion metrics: evaluate performance of model</h5>
<p>Once an object detector has been developed, it’s typically evaluated using two main metrics:</p>
<ul>
<li>Frames per second (FPS) - detection speed</li>
<li>Mean Average Precision (mAP) - network precision
<ul>
<li>mAP calculated from a bounding box’s object score and the precision-recall curve</li>
</ul>
</li>
</ul>
<h4 id="State-of-the-Art-Object-Detection-CNNS">State of the Art Object Detection CNNS</h4>
<h5 id="R-CNNs-Region-based-CNNs">R-CNNs: Region-based CNNs</h5>
<p>R-CNN family networks:</p>
<ul>
<li>R-CNN</li>
<li>Fast-RCNN</li>
<li>Faster-RCNN (SOTA)<br>
<img src="figure37.png" alt="figure7"><br>
R-CNN starts with a selective search to extract regions of interest (RoI) and wrapped them, then pass through a pretrain CNN to extract featrues. Once the feature extraction has been completed, classification and regression are performed to identify the class within each RoI as well as the bounding box. In order to understand this pipeline better, let’s look at R-CNN in a different angle.<br>
<img src="figure38.png" alt="figure8"><br>
Each region that pass through selective search will be sent to CNN to extract the featuers, and then the features will be passed to two modules – classification module, which is a support vector machine (SVM) and a regression module, which is a regerssor that estimate the bounding box. This is the base level of the family of R-CNNs.</li>
</ul>
<p><img src="figure39.png" alt="figure9"><br>
The image above is Fast-RCNN. The major change is where the CNN appears in the architecture. This time the input image will be directly sent into the CNN, and the region extractor happens after the CNN. This is important, because it speed up the network by only running one CNN instead of runnign ~2000 CNNs on each RoI.</p>
<p>CNN here performs both classification and feature extraction, and the SVM classification module is replaced with a softmax layer.</p>
<p><img src="figure40.png" alt="figure10"><br>
The image above is Faster-RCNN, and it reaches the SOTA performance. Its overal architectural design is similiar with Fast-RCNN, with the exceptionthat a different algorithm is used for region proposal. I used the same base CNN to do the feature extraction, the main change is that they created a region proposal network (PRN), this plays the role of the selective search algorithm. So, it’s really taking out the selective search algorithm from Fast-RCNN and replace it with PRN.</p>
<hr>
<div style="text-align: right"> To be continued... </div>
  </div>
</article>


  <!-- Gitalk start  -->

  <!-- Link Gitalk files  -->
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"/></script> 
  <div id="gitalk-container"></div>     
  <script type="text/javascript">
      var gitalk = new Gitalk({

        // gitalk's main params
        clientID: `3ce407f9c6ed6aaffdf8`,
        clientSecret: `2c6bc0465d806a6dc4ee20c9b1a0043d7a6101fe`,
        repo: `gaoxiangluo.github.io`,
        owner: 'GaoxiangLuo',
        admin: ['GaoxiangLuo'],
        id: location.pathname.slice(0, 50),
        distractionFreeMode: true,
        enableHotKey: true,
        language: `en`
      });
      gitalk.render('gitalk-container');
  </script> 
  <!-- Gitalk end -->




        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">From Fully Connected to Convolutional Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Find-patterns-in-an-image"><span class="toc-number">1.1.</span> <span class="toc-text">Find patterns in an image</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Digital-Images"><span class="toc-number">1.1.1.</span> <span class="toc-text">Digital Images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-find-a-pattern-in-images"><span class="toc-number">1.1.2.</span> <span class="toc-text">How to find a pattern in images?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Template-matching-previals-in-classic-image-processing"><span class="toc-number">1.1.3.</span> <span class="toc-text">Template matching previals in classic image processing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Problem-with-template-matching"><span class="toc-number">1.1.4.</span> <span class="toc-text">Problem with template matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-based-approach"><span class="toc-number">1.1.5.</span> <span class="toc-text">Feature-based approach</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Problems-with-fully-connected-networks"><span class="toc-number">1.2.</span> <span class="toc-text">Problems with fully connected networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Complexity"><span class="toc-number">1.2.1.</span> <span class="toc-text">Complexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Locality-and-ordering"><span class="toc-number">1.2.2.</span> <span class="toc-text">Locality and ordering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Invariance"><span class="toc-number">1.2.3.</span> <span class="toc-text">Invariance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ideal-neural-networks-for-spatial-data"><span class="toc-number">1.2.4.</span> <span class="toc-text">Ideal neural networks for spatial data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#A-quick-preview-of-convolutional-neural-network-CNN"><span class="toc-number">1.2.5.</span> <span class="toc-text">A quick preview of convolutional neural network (CNN)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Components-of-CNNs"><span class="toc-number">1.3.</span> <span class="toc-text">Components of CNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution-Layer"><span class="toc-number">1.3.1.</span> <span class="toc-text">Convolution Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Convolution-is-misnomer"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Convolution is misnomer!</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#More-on-conlution-correlation"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">More on conlution&#x2F;correlation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Connection-to-fully-connected-neural-network"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Connection to fully-connected neural network</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multiple-filters-each-layer"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">Multiple filters each layer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Do-we-reduce-the-complexity"><span class="toc-number">1.3.1.5.</span> <span class="toc-text">Do we reduce the complexity?</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pooling-Layer"><span class="toc-number">1.3.2.</span> <span class="toc-text">Pooling Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-pooling"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Why pooling?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Combine-covolution-and-pooling-%E2%80%93-convolution-with-strides"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Combine covolution and pooling – convolution with strides</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-multilayers"><span class="toc-number">1.3.3.</span> <span class="toc-text">Why multilayers?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-feature-learning"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Hierarchical feature learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Computation"><span class="toc-number">1.3.4.</span> <span class="toc-text">Computation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-compute-convolution"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">How to compute convolution?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#More-on-computation"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">More on computation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architectures-for-classification"><span class="toc-number">1.4.</span> <span class="toc-text">Architectures for classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Typical-design-patterns"><span class="toc-number">1.4.1.</span> <span class="toc-text">Typical design patterns</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LeNet-5-1998"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">LeNet-5 (1998)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AlexNet-2012"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">AlexNet (2012)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VGG-Net-2014"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">VGG-Net (2014)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet-2015"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">ResNet (2015)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dense-2016"><span class="toc-number">1.4.1.5.</span> <span class="toc-text">Dense (2016)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Other-models-to-look-at"><span class="toc-number">1.4.1.6.</span> <span class="toc-text">Other models to look at</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Practicle-tips"><span class="toc-number">1.5.</span> <span class="toc-text">Practicle tips</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transfer-Learning"><span class="toc-number">1.5.1.</span> <span class="toc-text">Transfer Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Are-CNNs-only-for-images"><span class="toc-number">1.5.2.</span> <span class="toc-text">Are CNNs only for images?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transposed-Convolution"><span class="toc-number">1.5.3.</span> <span class="toc-text">Transposed Convolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalization"><span class="toc-number">1.5.4.</span> <span class="toc-text">Normalization</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Applications of CNNs in Computer Vision</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Object-Detection"><span class="toc-number">2.1.</span> <span class="toc-text">Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-object-detection"><span class="toc-number">2.1.1.</span> <span class="toc-text">What is object detection?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object-Detection-Network"><span class="toc-number">2.1.2.</span> <span class="toc-text">Object Detection Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Multiple-Objects"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Multiple Objects</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-step-object-detection-framwork"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">4-step object-detection framwork</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Region-Proposal-indentify-regions-of-interest-RoI-for-potential-locations-of-objects"><span class="toc-number">2.1.2.2.1.</span> <span class="toc-text">1. Region Proposal: indentify regions of interest (RoI) for potential locations of objects</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Selective-Search"><span class="toc-number">2.1.2.2.1.1.</span> <span class="toc-text">Selective Search</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Feature-Extraction-extract-visial-features-within-each-RoI-for-classification"><span class="toc-number">2.1.2.2.2.</span> <span class="toc-text">2. Feature Extraction: extract visial features within each RoI for classification</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-Non-maximum-Suppression-NMS-avoid-repeated-detections"><span class="toc-number">2.1.2.2.3.</span> <span class="toc-text">3. Non-maximum Suppression (NMS): avoid repeated detections</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-Evaluateion-metrics-evaluate-performance-of-model"><span class="toc-number">2.1.2.2.4.</span> <span class="toc-text">4. Evaluateion metrics: evaluate performance of model</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#State-of-the-Art-Object-Detection-CNNS"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">State of the Art Object Detection CNNS</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#R-CNNs-Region-based-CNNs"><span class="toc-number">2.1.2.3.1.</span> <span class="toc-text">R-CNNs: Region-based CNNs</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&text=Convolutional Neural Network (CNN): Overview"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&is_video=false&description=Convolutional Neural Network (CNN): Overview"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Convolutional Neural Network (CNN): Overview&body=Check out this article: https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&title=Convolutional Neural Network (CNN): Overview"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&name=Convolutional Neural Network (CNN): Overview&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://gaoxiangluo.github.io/2021/01/06/Convolutional-Neural-Network-CNN-Overview/&t=Convolutional Neural Network (CNN): Overview"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2021
    Gaoxiang Luo
  </div>
  <!-- <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </nav>
  </div> -->
  <div class="busuanzi-count">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span class="site-pv">
      Page View <i class="fa fa-users"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
    <!-- <span class="site-uv">
      Number of Visitors <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuansi_value_site_uv"></span>
    </span> -->
  </div>
  <div class="Word-count">
    <i class="fas fa-chart-area"></i> Total count: </i><span class="post-count">27.4k</span>
  </div>
</footer>

    </div>
    <!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


    <!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="/lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-176745887-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":5,"vOffset":-38},"mobile":{"show":false,"scale":0.2},"react":{"opacityDefault":0.8,"opacityOnHover":0.2},"log":false});</script></body>
</html>
